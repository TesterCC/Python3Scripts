1. 什么是分布式锁？为什么用分布式锁？, page url: https://www.mianshi.icu/question/detail?id=297
1-297-答案：
简单题，在校招和初级工程师中有可能遇到。

在这个场景之下，可以举例自己用分布式锁解决过的问题，并且将话题引导过去自己的项目经历上。
分布式锁是一种在分布式系统中用于控制多个进程或线程对共享资源进行互斥访问的机制。简单来说，它确保在分布式环境下，同一时间只有一个进程或线程能够操作某个共享资源，从而避免数据不一致或资源冲突的问题。
分布式锁是一种在分布式系统中用于控制多个进程或线程对共享资源进行互斥访问的机制。简单来说，它确保在分布式环境下，同一时间只有一个进程或线程能够操作某个共享资源，从而避免数据不一致或资源冲突的问题。

其实就是一个运作在分布式环境下的锁。
不过在实践中，也不一定是非得使用分布式锁，某些情况下可以通过优化处理逻辑来规避使用分布式锁。

第一种方式就是通过引入顺序消息。假设说现在 A 和 B 要操作同一个资源，可以让 A 和 B 丢一个消息到消息队列，但是这个消息需要是顺序消息。而后有一个消费者消费这些顺序消息。

------------------------------

2. 常见的分布式锁有哪些解决方案？, page url: https://www.mianshi.icu/question/detail?id=298
2-298-答案：
简单题，分布式锁算是难题，但是这个问题只是泛泛而谈倒也还好。在校招中不常见，但是在社招，尤其是目标公司是分布式架构或者微服务架构，那么这个部分就会问的很多。

你要通过讨论一般的分布式锁的实现要点来刷亮点。
这个问题会有点歧义，有可能面试官问的是有什么开源的分布式锁实现，也有可能问的是如果要手搓一个分布式锁，应该怎么写。

这里我给出几个常见的方案：

因为这里有很多种实现方式，所以你在回答的时候，重点是回答出来 Redis 的实现方式，剩下的你随便提一下就可以。
实现分布式锁的方式有很多种。

第一种是基于Redis的分布式锁。简单来说，就是用SETNX命令来尝试获取锁，如果键不存在就设置键并返回1，表示锁拿到了；如果键存在就返回0，表示锁被别人占了。为了防止死锁，还得用 EXPIRE 命令给锁设个过期时间。另外，Redisson这个框架提供了更高级的锁，像可重入锁、公平锁啥的，用起来很方便。优点很明显，性能高，适合高并发场景，实现也简单。缺点就是依赖Redis的稳定性，尤其是在 Redis 宕机之后锁有可能被别人拿走。
从分布式锁的设计和实现上来看，任何具备任何支持排他性设置值的中间件都可以用来实现分布式锁，比如Redis、Zookeeper、etcd等，甚至还包括nacos等类似工具。而分布式锁的性能与可用性，也基本上取决于所选中间件。

需要注意的是，所有分布式锁都必须考虑中间件崩溃后的锁安全性问题，例如Redis分布式锁在Redis宕机时会消失。

此外，设置锁的过期时间和续约问题也是关键，因为无论过期时间多长，都可能出现业务未完成而锁已过期的情况。虽然分布式锁可以支持公平锁和可重入锁，但并非所有中间件都有能力支持这些特性。
我利用 Go 语言基于 Redis 写过一个分布式锁。它提供了加锁重试、解锁重试、TryLock，手动续约和自动续约功能。整体来说性能很不错，可用性则是完全取决于 Redis。

------------------------------

3. 实现分布式锁的最基本的三个要素是什么？, page url: https://www.mianshi.icu/question/detail?id=299
3-299-答案：
简单题，很少问，因为这个东西是一些人总结出来的私货，还不是业界公认的点。

在回答这个问题的时候，你可以深入讨论续约的问题，赢得竞争优势。
首先我得喷一下，这个问题，也就是所谓的三要素，不知道是哪个王八蛋总结出来的，看上去有点水平，实际上水平不高。但是怕你出去面试真的遇到这一类的面试官，所以我还是收录了这个问题。

但是我写过多次分布式锁，也没搞什么三要素。

这三个要素非常简单：
分布式锁的三个基本要素是加锁、释放锁和设置过期时间。

其中加锁，要考虑是否是可重入锁的问题。如果是可重入锁，那么加锁可能对应的是计数增加 1。
而在实践中，过期时间其实不是什么大问题，真正的大问题是续约机制。

------------------------------

4. TCP和UDP有什么区别？, page url: https://www.mianshi.icu/question/detail?id=410
4-410-答案：
简单题，在校招和初级工程师面试里面，简直不要太容易遇到。

在这个问题之下，装逼还是那句话：可靠 UDP 传输，并且进一步比较可靠 UDP 传输和 TCP 的差异。

这些内容实在太多，正常在面试中，你能记住几个点就差不多了。
TCP和UDP的主要区别在于连接性、可靠性和传输方式。

TCP是面向连接的，提供可靠的数据传输，适用于需要高可靠性的应用；而UDP是无连接的，传输速度快但不可靠，适用于实时性要求高的应用。
另外一个值得一提的就是可靠 UDP 传输，它可以看做是一个 UDP 版本的 TCP 了。

可靠UDP传输和TCP的主要区别在于实现机制、连接性、性能特点和适用场景。

------------------------------

5. UDP 和 TCP 对应的应用场景是什么？, page url: https://www.mianshi.icu/question/detail?id=411
5-411-答案：
简单题，在校招和初级工程师中比较常见。

你只需要抓住可靠传输用 TCP，别的 UDP 就可以了。
选择依据：
UDP和TCP各有其独特的特点和适用场景。

UDP是无连接的，传输速度快、延迟低，但不可靠，适用于对实时性要求高且可容忍少量丢包的应用，如视频会议、在线游戏、DNS查询和VoIP。

------------------------------

6. 详细介绍一下 TCP 的三次握手机制？, page url: https://www.mianshi.icu/question/detail?id=412
6-412-答案：
简单题，在校招和初级工程面试中，概率极大。尤其是如果你是一个没啥面试项目的应届生，那么差不多就是必问题了。

在这个问题之下，装逼的最佳方式有两个，一个是讨论三次握手机制意味着 TCP 创建连接非常麻烦，所以需要池化技术；另外一个是讨论为啥恰好是三次握手，而不是两次握手，也不是四次握手。
三次握手机制 是 TCP 协议在建立连接时采用的一种机制，确保双方能够可靠地建立通信。具体步骤如下：





这里稍微解释一下：
TCP三次握手是指建立一个可靠的TCP连接的过程。它的主要目的是让通信双方（客户端和服务器）确认彼此的接收和发送能力，并协商一些关键参数，确保数据传输的可靠性。

最开始，服务端要启动之后，要监听某个端口。

当客户端准备连接服务端的时候，发送　SYN　报文，并且带上自己的初始化序列号（ISN），而后进入　SYN－SENT　状态。这是第一次握手。
从这个过程上也可以看到，三次握手这个过程虽然可靠性很强，但是性能很差。一方面三次握手过程增加了连接建立的延迟，尤其是在网络状况不佳的情况下；另外一方面是每次建立连接都需要消耗系统资源（如文件描述符、内存等）。
当然，我们也可以从一个更加本质的角度去理解三次握手的过程。对于 TCP 连接来说，初始化的时候有两个关键的点：确认网络是连通，同步初始化序列号，即 ISN。

------------------------------

7. 为什么需要三次握手，而不是两次？, page url: https://www.mianshi.icu/question/detail?id=413
7-413-答案：
简单题，在校招和初级工程师面试中很常见。

在这个问题之下，你可以引入 TFO，以及如果客户端一直不 ACK 最终服务端会发生什么来赢得竞争优势。
参考这里：详细介绍一下 TCP 的三次握手机制？ (meoying.com)

记住这个图：

，还是服务端到客户端，都需要协商一个起始的报文序列号（ISN）；
从理论上来说，TCP 是一个全双工通信，也就是说要同时确认客户端可以发送数据到服务端，以及服务端能够发送数据到客户端。而在这个过程中，很重要的是要协商好客户端报文的初始序列号，以及服务端报文的初始序列号。
但是这里的问题是，在两次握手之后，客户端究竟能不能发送数据到服务端，服务端会不会正常处理这些报文。

这里就要先提及一种机制：TFO，TCP Fast Open。这个机制就是，客户端在第三次握手的时候，可以顺便带上数据。也就是第三次握手不是一个纯粹的 ACK 报文，这个报文里面还有客户端发送的数据。

当然，这要客户端和服务端都支持 TFO 机制。

------------------------------

8. 为什么要三次握手，而不是四次？, page url: https://www.mianshi.icu/question/detail?id=414
8-414-答案：
简单题，在校招和初级工程面试中很常见。

回答这个问题，我有一个独特的装逼技巧：三次握手可以看做是理论上四次握手的一个优化，这可以从四次挥手中得到解释。
参考这里：详细介绍一下 TCP 的三次握手机制？ (mianshi.icu)

记住这个图：
因为三次握手已经可以确认双方的发送接收能力正常，双方都知道彼此已经准备好，而且也可以完成对双方初始序号值得确认，也就无需再第四次握手了。

这个问题要从 TCP 的全双工通信角度开始说，全双工通信也就是意味着要做到客户端能把数据发到服务端，服务端也能把数据发到客户端。
这一点其实可以从四次挥手的角度也能看出来。四次挥手里面，两次挥手解决的是关闭客户端到服务端的信道，而另外两次挥手解决的就是服务端到客户端的信道。

------------------------------

9. 什么是 SYN洪泛攻击？如何防范？, page url: https://www.mianshi.icu/question/detail?id=415
9-415-答案：
略难的题，一般的后端研发没有什么机会接触到安全类的议题，这个问题基本上只会出现在社招中。
SYN洪泛攻击（SYN Flood Attack）是一种常见的拒绝服务（DoS）攻击方式，利用TCP/IP协议中的三次握手机制进行攻击。你可以先看这个问题详细介绍一下 TCP 的三次握手机制？ (mianshi.icu)。

这个 SYN 洪泛就是纯纯垃圾翻译，导致你看名字都想不到会是什么，你理解成是 SYN 洪水攻击，更加直观。


TCP三次握手：
SYN 洪泛攻击是一种 DDoS 攻击，攻击者发送大量的 SYN 包，但是并不响应服务端返回来的 SYN-ACK 包。

这样造成的后果是服务端需要消耗资源维护这些半开连接，并且因为没有收到客户端的 ACK 包，所以会不断重试，进一步加重了资源消耗。

SYN 洪泛攻击抵御起来，有多种手段。

------------------------------

10. 三次握手连接阶段，最后一次ACK包丢失，会发生什么？, page url: https://www.mianshi.icu/question/detail?id=416
10-416-答案：
简单题，在校招和初级工程师面试中比较常见。

其实这个问题说白了就是要是客户端没 ACK，会发生什么。一句话就能说清楚，服务端会重试，直到重试超出最大次数，则会关闭连接。
你把服务端返回的 SYN-ACK 看做普通的报文就可以了，也就是服务端如果没有收到 ACK 报文，会重新发送 SYN-ACK 报文。如下图，是重试之后成功了：


对于服务端来说，服务端没有收到 ACK 之后，就会重新发送 SYN-ACK 报文，并且继续等待客户端响应 ACK。如果达到重试上限，都没收到 ACK，服务端就会关闭连接。

------------------------------

11. 详细介绍一下 TCP 的四次挥手过程？, page url: https://www.mianshi.icu/question/detail?id=417
11-417-答案：
简单题，在校招和初级工程师面试中比较常见。

在这个问题之下，你可以引出为什么不能三次挥手，以及 TIME_WAIT 的问题来刷亮点。
可以参考三次握手的问题：

其实四次挥手很好理解。你还是要从全双工通信这个角度去理解。全双工通信就意味着客户端能发送数据到服务端，而服务端也能发送数据到客户端。

因此你关闭 TCP 连接的时候，是要关闭客户端到服务端，以及服务端到客户端这两个方向的数据传输。所以：
TCP 的四次挥手是指在断开连接时，通信双方（客户端和服务器）通过四个步骤来安全地终止连接的过程。它的目的是确保双方都能确认数据传输已经完成，并释放资源。

第一次挥手：客户端发送一个FIN包，表示客户端没有数据发送了，希望关闭连接。这个包的序列号seq = x，FIN标志位为1。而后客户端进入 FIN_WAIT_1 状态。

第二次挥手：服务端收到FIN包后，发送一个ACK（确认）包，确认收到客户端的FIN包。这个包的确认号ack = x + 1，表示期望收到客户端的下一个序列号。这个时候服务端会进入 CLOSE_WAIT 状态。客户端收到服务端的 ACK 报文，进入了 FIN_WAIT_2 状态。
TCP连接是全双工的，意味着数据可以在两个方向上独立传输。因此，每个方向的连接都需要单独关闭。第一次和第二次挥手关闭了客户端到服务端的连接，第三次和第四次挥手关闭了服务端到客户端的连接。如果只进行三次挥手，可能会导致其中一个方向的数据未完全传输完毕就被关闭。

------------------------------

12. 为什么连接的时候是三次握手，关闭的时候却是四次握手？, page url: https://www.mianshi.icu/question/detail?id=418
12-418-答案：
简单题，在校招和初中级工程师岗位面试中比较常见。

还是那句话，TCP 的很多问题你抓住全双工通信这个关键点就可以了。

你可以综合对比三次握手和四次挥手得出它们其实都是全双工模型引出的，只不过三次握手比较特殊，是一种优化之后的形态而已。
你可以看这个问题：为什么要三次握手，而不是四次？ (meoying.com)这里提出来了，本质上三次握手可以看做是一个优化过程。

四次挥手的具体内容可以参考这个部分：详细介绍一下 TCP 的四次挥手过程？ (meoying.com)

看这个图：
从理论上来说，因为 TCP 是全双工通信，那么不管是握手还是挥手，都应该是四次才对。

但是，TCP 在握手阶段，服务端可以在响应客户端的 SYN 报文的时候，顺便把自己的 SYN 报文也发送过去，也就是一个混合的 ACK + SYN 报文，因此省了一次握手。

而在挥手阶段，服务器在收到客户端的 FIN 报文段后，可能还有一些数据要传输，所以不能马上关闭连接。但是会做出应答，返回 ACK 报文段。

------------------------------

13. 为什么客户端的 TIME-WAIT 状态必须等待 2MSL ？, page url: https://www.mianshi.icu/question/detail?id=419
13-419-答案：
简单题，在校招和初级工程师面试中比较常见。

在这个问题之下，你要进一步解释为什么恰好是 2MSL，1MSL 行不行，3MSL行不行。而后可以利用 TIME_WAIT 调优进一步刷亮点，赢得竞争优势。
你先要看看四次挥手的过程：详细介绍一下 TCP 的四次挥手过程？ (meoying.com)。



MSL：Maximum Segment Lifetime，最大段生命周期，是指一个TCP报文段在网络中存在的最长时间。这个时间通常由网络设备和协议栈配置决定。

在最后阶段，客户端收到了服务端的 FIN 报文，并且返回了 ACK 之后，进入了 TIME_WAIT 状态，它会保持在这个状态 2MSL。
客户端在TCP连接断开过程中进入TIME-WAIT状态，并等待2MSL时间，主要是出于以下两个重要原因：
从这两点也能看出来为什么恰好是 2MSL。

如果是 1MSL，那么这两点都不能保证。

一方面是无法确保ACK的可靠传输。1MSL时间不足以确保服务端收到客户端的最后一个ACK报文。如果ACK在网络中延迟，服务端可能会重发FIN包，而客户端在1MSL后可能已经认为连接关闭，导致服务端无法正确关闭连接。
虽然 2MSL 是一个很好的值，但是我们有时会调小TIME_WAIT值以提高资源利用率和系统性能，尤其在高并发环境下。

------------------------------

14. 如果已经建立了连接，但是客户端出现故障了怎么办？, page url: https://www.mianshi.icu/question/detail?id=420
14-420-答案：
简单题，在校招和初级工程师面试中比较常见。

题目只是说客户端出现了故障，但是没说是什么故障，这就有点难办。那么我这里整理的回答都是针对客户端已经寄了，或者长时间没响应这种情况。
这要分成两种情况。

第一种情况是服务端发数据给客户端，因为客户端此时已经崩溃了，所以没办法 ACK 服务端的报文。那么会触发服务端的重试功能，在超过重试上限之后，服务端会判定连接不可用，直接关闭连接。

------------------------------

15. 处于 TIME-WAIT 状态的 TCP 连接过多会产生什么后果？怎样处理？, page url: https://www.mianshi.icu/question/detail?id=421
15-421-答案：
简单题，不过校招一般比较少问，因为校招生没经验。在社招的时候问得比较多。

如果你有具体的例子，可以在讲完理论怎么处理之后，具体讲讲自己的案例。
会带来三个问题：

而解决办法主要有三个：
处于TIME-WAIT状态的TCP连接过多主要会导致三个问题。

第一个问题是资源占用过多的问题，包括端口号占用，内存占用等。

第二个问题是性能下降，包括系统负担加重，响应时间变长等。
而具体到调整 TIME_WAIT 有关的参数，主要有四个。

第一个是 tcp_fin_timeout，用来设置TIME-WAIT状态的超时时间，减少此值可以缩短TIME-WAIT状态持续的时间，但可能会增加丢失数据包的风险。

第二个是 tcp_tw_reuse，表示是否允许重用TIME-WAIT sockets。设置为1可以允许在安全的情况下重用TIME-WAIT sockets，但需要确保不会与旧的数据包混淆。

------------------------------

16. TIME_WAIT 是服务器端的状态?还是客户端的状态?, page url: https://www.mianshi.icu/question/detail?id=422
16-422-答案：
简单题，在校招和初级工程师面试中有可能会遇到。

这个问题的背景是在高并发场景下，会遇到 TIME_WAIT 过多的问题，所以你可以从如何规避 TIME_WAIT 的角度来刷亮点。
你可以先看这两个问题：
严格来说，谁先发起关闭连接谁就会进入TIME_WAIT状态。大多数时候，在客户端-服务端的模式中，服务端都不会主动关闭连接，所以 TIMW_WAIT 一般出现在客户端。
而我们也知道，TIME_WAIT 状态的连接如果很多的话，会严重影响性能。所以可以通过限制服务端，让服务端不会先发起关闭连接来避免服务端遇到大量 TIME_WAIT 的问题。

------------------------------

17. TCP协议如何保证可靠性？, page url: https://www.mianshi.icu/question/detail?id=423
17-423-答案：
略难的题，因为要背的内容非常多。在各个层级的面试中都有可能遇到，但是相对来说，校招最容易遇到。

在这个问题之下，你只需要泛泛而谈各种可靠性保障措施，而后等面试官来追问。同时，你也可以考虑提及基于 UDP 的可靠传输和QUIC协议，作为引导。
可以说是，非常多措施了，是分布式系统网络传输协议设计的典范了。
TCP协议通过多种机制来保证数据的可靠传输。

首先，它使用确认应答机制，确保每个数据包都被接收方确认。

其次，序列号用于数据的有序传输，接收方可以根据序列号来检测丢失或重复的数据包。
最近流行的所谓的基于 UDP 的可靠传输，差不多也是靠这些机制。比如说 QUIC 协议也同样有序列号、确认应答、重传机制、流量控制、拥塞控制、快重传和快恢复这些机制。

------------------------------

18. 详细讲一下TCP的滑动窗口？, page url: https://www.mianshi.icu/question/detail?id=424
18-424-答案：
略难的题，难在要记忆的东西比较多，在校招和初中级岗位面试中比较常见。

你在回答这个问题的时候，要记得提及拥塞控制的四大金刚，以及超时重传机制，等待面试官进一步追问细节。
TCP的滑动窗口机制是用于控制数据传输流量和保证数据可靠性的重要机制。它通过动态调整窗口大小，来管理发送方和接收方之间的数据传输。

首先，在 TCP 的发送方和接收方各有一个窗口。并且因为 TCP 是一个全双工通信的协议，所以大多数时候，客户端和服务端都是既有接收窗口，又有发送窗口。



TCP 依靠发送方窗口和接收方窗口来完成流量控制和拥塞控制。并且因为 TCP 协议是全双工通信，这意味着客户端和服务端大多数时候都是同时有发送方窗口和接收方窗口。

（发送方窗口）从构成上来说，发送方窗口把全部报文分成了三个部分：窗口左边代表全部已发送并且收到了 ACK 的报文，窗口内部的报文，窗口右边还没发送的报文。

而窗口内部的报文也可以分成三类：已发送并且 ACK 的报文，已发送但是还没收到 ACK 的报文，还没发送的报文。
这里的难点就是动态调整的策略。

就接收方来说，它主要是考虑自己的缓冲区。如果要是还有很多空闲缓冲区，那么接收方就可以增大自己的窗口，并且在发给发送方的报文里面带上新的窗口大小。反之则是减小窗口。最极端的情况下，接收方会返回零缓冲区，也就是提醒发送方不要再发了，自己已经没有空闲缓冲区可用了。

------------------------------

19. TCP 的拥塞控制是如何运作的？, page url: https://www.mianshi.icu/question/detail?id=425
19-425-答案：
略难的题，主要是这部分的内容比较多，不容易记忆，在校招和初级工程师面试中很常见。

TCP 的拥塞控制的思路在很多地方都可以使用，你可以引用类似的设计来刷亮点，或者使用自己的真实案例来刷亮点。
TCP 一共使用了四种算法来实现拥塞控制：


慢启动是TCP连接初始阶段使用的算法，旨在逐步增加网络中的数据流量，避免突然大量发送数据导致网络拥塞。

初始时，拥塞窗口（cwnd）设置为一个最大段大小（MSS），每次收到一个ACK确认后，cwnd翻倍增长。这种指数增长方式使得cwnd迅速增大，但一旦达到慢启动阈值（ssthresh），增长速率会放缓，此时就步入了拥塞避免。
TCP的拥塞控制主要通过慢启动、拥塞避免、快速重传和快速恢复等机制来运作。

初始阶段，TCP使用慢启动算法，每次收到一个 ACK，窗口大小就翻倍，这个阶段窗口是指数增长。

当拥塞窗口达到慢启动阈值后，进入拥塞避免阶段，此时窗口是线性增长。
我在很多地方用过类似的思路。

------------------------------

20. HTTP常见的状态码有哪些？, page url: https://www.mianshi.icu/question/detail?id=426
20-426-答案：
简单题，在校招和初级工程师面试中很常见。

在这个问题之下，你可以将话题延伸到HTTP 状态码设计对业务错误码设计的影响，从而刷出亮点。
HTTP 的状态码非常多，而题目问的是常见的有哪些，所以你大概记住三五个就可以了：

还有别的错误码，你面试就是哪个熟悉用哪个。
常见的状态吗有很多，我列举几个：

200 代表 OK，服务器处理了请求。201 代表已接收，一般是异步接口返回，表达请求已经收到了，但是不一定处理好了。

301 代表永久重定向，302 代表临时重定向
HTTP 的状态码对业务错误码设计有很深远的影响，甚至于也影响到了别的协议的错误码设计。

举个例子来说，我在我们公司里面就推行过一个错误码规范。它分成三段，第一段是错误码的性质，2 代表没什么问题，4 代表客户端错误，5 代表服务端错误；第二段是模块码，第三段是具体错误。

------------------------------

21. 状态码301和302的区别是什么？, page url: https://www.mianshi.icu/question/detail?id=427
21-427-答案：
简单题，在校招和初级工程师中可能遇到，但是因为过于简单所以可能性不高。

在这个问题之下，你可以稍微讨论一下浏览器对这两种状态码的不同处理方式，赢得竞争优势。
在HTTP常见的状态码有哪些？ (meoying.com)中列举了 301 和 302。
301 代表的是永久重定向，而 302 代表的是临时重定向。正常来说，如果要是资源已经彻底移走了，那么就应该用 301，如果只是临时挪一下，那么就应该用 302。
浏览器和搜索引擎会对这两个状态码有不同的处理方式。

------------------------------

22. HTTP 常用的请求方式/请求方法有哪些？, page url: https://www.mianshi.icu/question/detail?id=428
22-428-答案：
简单题，在校招和初级岗位中常见，但是因为过于简单以至于可能性不太高。

你在这个问题下可以深入讨论 RESTful 风格的 Web API 是如何利用不同的请求方法来表达对资源的不同操作，从而刷出亮点。
HTTP 的方法有非常多，你在回答的时候尽量回答，越多越好，但是漏掉那么几个也是可以的。
GET 和 POST 是用得最多的两个方法。

GET 主要用来访问服务器上的资源，并不改变资源的状态。而 POST 则是主要用于向服务端提交数据，更新或者创建资源，但是 POST 可以不是幂等的。

除了这些，PUT 可以用来表达更新或者创建资源，但是操作应该是幂等的，大多数情况下是整体更新。PATCH 也是更新资源，但是一般是代表更新资源的一部分属性。DELETE 则是删除资源。
在 RESTful API 里面，就是充分利用好了这些方法。基本的思路就是，不同的 HTTP 方法代表了对资源的不同操作。例如说 GET 是获得资源，POST 是非幂等更新或者创建资源，PUT 是幂等地更新或者创建资源，PATCH 则是幂等地更新资源的部分属性。

------------------------------

23. GET请求和POST请求的区别？, page url: https://www.mianshi.icu/question/detail?id=429
23-429-答案：
简单题，在校招和初级工程师面试中有可能遇到，但是因为特别简单，所以可能性不大。
GET 和 POST 的区别，基础中的基础，入门中的入门。
GET 和 POST 的区别还是很大的。

首先从请求目的上来说，GET 是为了获取资源，而 POST 用于更新资源或者创新资源；
从幂等性来说，GET 是幂等的，但是 POST 不是幂等的；

------------------------------

24. 解释一下HTTP长连接和短连接？, page url: https://www.mianshi.icu/question/detail?id=430
24-430-答案：
简单题，在校招和社招的初中级岗位中有可能遇到。

你可以在回答基本的定义之后，讨论 HTTP 的长连接和 TCP 长连接的区别，从而赢得竞争优势。
HTTP 长连接：

HTTP 短连接：

注意到：HTTP 的长连接和 TCP 的长连接不是一回事，HTTP 长连接本质是客户端和服务端在应用层上的协商，底层 TCP 协议对此一无所知。
HTTP长连接和短连接主要区别在于TCP连接的持续时间和资源占用。长连接在一个TCP连接上可以发送多个HTTP请求和响应，通过Connection: keep-alive头部来维持连接，适用于频繁请求的场景，如加载多个资源的网页，减少了TCP握手的开销，提高了性能。
HTTP 长连接跟 TCP 长连接是两回事，它应该看做是客户端和服务端在应用层上的一种协商，通过 Connection 和 Keep-Alive 两个参数来控制。

------------------------------

25. HTTP 请求报文和响应报文的格式是什么？, page url: https://www.mianshi.icu/question/detail?id=431
25-431-答案：
简单题，在校招和初级工程师岗位中有可能遇到，再网上就很难遇到了。

在实践中这个问题其实没太大用处，毕竟我们现在已经没啥机会直接操作 HTTP 请求响应的字节流了，也就是在浏览器中通过控制台查看请求响应的时候有一点用处而已。

而后有一个非常高端的装逼方式，即讨论 HTTP 报文设计对后续很多应用层协议设计的影响，比如说各种 RPC 协议设计的影响。
主要分成：
HTTP请求报文由四部分组成：

第一部分是请求行，包含请求方法、URI和HTTP版本，如GET /index.html HTTP/1.1；

第二部分是请求头部，它包含一系列键值对，传递附加信息，如Host: example.com；
HTTP 的这种报文格式对后续很多应用层协议协议产生了深远的影响。

------------------------------

26. HTTP1.0 和 HTTP1.1 有什么区别?, page url: https://www.mianshi.icu/question/detail?id=432
26-432-答案：
简单题，在校招和社招初级工程师中有可能遇到。

在这个问题之下，你装逼就要深入讨论 HTTP1.1 的缺点，并且揭示在 HTTP2.0 里面的改进。

你记住最重要的两个：缓存控制和长连接。
HTTP/1.1相比HTTP/1.0带来了多项改进：

首先，HTTP/1.1默认启用长连接，减少了TCP握手的开销，这对于频繁交互的应用尤其重要。

其次，在缓存控制方面，HTTP/1.1提供了更精细的Cache-Control头部，使得缓存管理更加灵活。
但是 HTTP 1.1 虽然有了很多改进，但是这并不是终点，因为 HTTP 1.1 依旧有很多缺点。

最显著的缺点就是请求阻塞，也就是当一个请求在等待服务器响应时，后续请求必须等待，即使它们与前面的请求无关。这导致了队头阻塞，降低了性能。

------------------------------

27. HTTP1.1和 HTTP2.0 有什么区别？, page url: https://www.mianshi.icu/question/detail?id=433
27-433-答案：
简单题，在校招和初级岗位面试中有可能遇到。

回答这个问题，就要进一步讨论 HTTP 2.0 的弊端，以及 HTTP 3.0
HTTP/2.0相比HTTP/1.1带来了多项性能优化和效率提升。

首先，HTTP/2.0支持多路复用，允许在单个连接中同时发送多个请求和响应，解决了HTTP/1.1中的队头阻塞问题。

其次，HTTP/2.0使用HPACK算法压缩请求和响应的头部，减少了冗余数据，提高了传输效率。
HTTP/2.0虽大幅提升了性能，但仍存在依赖TCP导致的建连延迟和队头阻塞问题，且头部压缩复杂、明文传输风险未根除。

------------------------------

28. HTTP 与 HTTPS 有什么区别？, page url: https://www.mianshi.icu/question/detail?id=434
28-434-答案：
简单题，在校招和社招初级岗位中比较常见。

回答这个问题，你可以通过略微阐述 HTTPS 协议的 TLS 建立过程，将话题引导过去 HTTPS 通信过程，并且赢得竞争优势。

其它你都可以不用在意，只需要借助安全性这一个点。
HTTP与HTTPS的主要区别在于安全性和数据传输的可靠性。

HTTP使用明文传输数据，容易受到中间人攻击，数据可能被窃听或篡改。而HTTPS在HTTP基础上增加了TLS协议，对数据进行加密传输，确保了数据的安全性和完整性，需要使用数字证书来验证服务器身份

虽然HTTPS的加密和解密过程会增加一定的延迟，但现代硬件和优化技术已大幅减少了这种影响。此外，HTTPS有助于提高搜索引擎排名，浏览器显示安全锁标志，增强用户信任。
HTTPS相比HTTP，在通信步骤中显著增加了TLS握手、证书验证和密钥交换等关键步骤。

具体而言，HTTPS在建立TCP连接后，首先进行TLS握手，客户端发送支持的TLS版本、加密方法和随机数，服务器回应并附上服务器证书；

接着，客户端验证证书合法性，确保通信对方可信；

------------------------------

29. HTTPS 的优缺点是什么？, page url: https://www.mianshi.icu/question/detail?id=435
29-435-答案：
简单题，在校招和社招初级工程师面试中有可能遇到。


优点：
缺点：

上面这些都是很细碎的描述，粗略版如下：
HTTPS通过TLS协议对HTTP进行加密传输，主要优点包括数据加密、身份验证、数据完整性保护，提升了用户信任度和SEO排名，有效防止了流量劫持。然而，HTTPS也存在性能开销、成本高昂、部署复杂、缓存限制等缺点，且依赖于证书信任链，存在一定的安全风险。

------------------------------

30. HTTPS 的原理是什么？, page url: https://www.mianshi.icu/question/detail?id=436
30-436-答案：
略难的题，

这个题目难在很多人记不住 HTTPS 的通信步骤，也就是在 TCP 连接建立之后的 TLS、证书验证、密钥交换过程。因此如果你能把 HTTPS 的通信步骤讲清楚就能赢得竞争优势了，如果要是还能根据 TLS 握手过程讨论为什么安全，效果更好。
HTTPS 原理中最重要的是理解 HTTPS 的通信步骤，即一个 HTTPS 请求是怎么发出去的，涉及到 TCP 三次握手，TLS 建立过程，证书验证和密钥交换过程。

粗略来说，分成四个步骤：

而难点 TLS 握手的过程就要复杂很多，如图：
整个 HTTPS 的通信过程还是比较复杂的，粗略的来说，可以分成四步。

第一步是 DNS 解析，将域名转换成 IP。

第二步是建立 TCP 连接，也就是三次握手过程。
从上面的过程也能看出来，TLS握手的安全性源于其多重保障机制：

首先，通过密钥交换和随机数生成确保数据加密，防止窃听；

其次，利用数字证书和证书链验证服务器身份，杜绝中间人攻击；

------------------------------

31. 在浏览器中输入www.baidu.com后执行的全部过程？, page url: https://www.mianshi.icu/question/detail?id=437
31-437-答案：
略难的题。

回答这个问题有两个要点：第一个是提及尽可能多的知识点，等待面试官追问；第二个是深入讨论大规模分布式系统是如何处理一个 http 请求的。
这个问题大部分内容和HTTPS 的原理是什么？差不多，在回答的时候你也不是很需要说什么细节，主要是你不知道面试官对哪个细节感兴趣。比如说很多人在回答这个问题的时候，会反馈说面试官打断他的回答，其实就是你说的细节他不感兴趣。

具体来说，粗略的步骤可以看做是：

如图：
从输入 www.baidu.com 到看到页面的过程主要有几个步骤。

第一步，根据域名解析得到 IP。

第二步，根据 IP 地址发起 TCP 连接，完成三次握手。
当然，这只是一个简单的说法，实际上类似百度这种大体量的公司，必然会有很多额外的措施，只是说百度也没发布相关的文章，所以我们无从得知。

不过可以猜到一些。

第一个是 DNS 解析，百度要服务全国，所以肯定能做到不同地区的人解析到的结果不同。例如说我在广州发请求，那么解析得到的 IP 应该是靠近广州的；

------------------------------

32. 什么是 Cookie 和 Session ?, page url: https://www.mianshi.icu/question/detail?id=438
32-438-答案：
简单题，在校招和初级工程师面试中可能见到。

在这个问题之下，装逼要落到两个地方：一个是 Cookie 的局限性；另外一个是 JWT，Cookie 和 Session 的混合技术。
Cookie：
Session：

以上都是大路货色，无法帮你赢得竞争优势。真正赢得竞争优势的是你需要聊点别的。
Cookie和Session是Web应用中常用的会话管理技术。Cookie是存储在用户浏览器中的小型文本文件，由服务器发送并由浏览器保存，用于记录用户信息、跟踪用户行为。它存储在客户端，容量有限，可以设置过期时间，但易受客户端篡改和禁用。

Session则是服务器端用于存储用户会话信息的机制，通过Session ID识别用户，保持用户的会话状态。Session存储在服务器端，安全性较高，生命周期通常与浏览器会话一致。
此外，JWT作为一种新兴的会话管理技术，也在某些场景下替代了传统的Cookie和Session，提供了更高的安全性和灵活性。比如说很多老外喜欢禁用 cookie，在这种情况下，一般是通过 JWT 和Session 组合来维持会话。

------------------------------

33. Cookie 和 Session 是如何配合的呢？, page url: https://www.mianshi.icu/question/detail?id=439
33-439-答案：
简单题，在校招和初级工程师的岗位面试中有可能遇到。

在这个问题之下，你可以简单提及在 Cookie 被禁用的情况下，可以采用 JWT 来和 Session 配合。
一句话就能说清楚：Cookie 存储的是 SessionID。
简单来说，Cookie 里面一般存的是 Session ID。

在登录成功的时候，服务器将 Session ID 写入到 Cookie 中；
此外，在使用 Cookie 装 Session ID 的时候还有一个问题，即如果要是浏览器禁用了 Cookie，那么这个方案就寄了。在这种情况下，一般会考虑使用 JWT。

------------------------------

34. Cookie和Session 有什么区别？, page url: https://www.mianshi.icu/question/detail?id=440
34-440-答案：
简单题，在校招和初级工程师面试中有可能遇到。


可以参考什么是 Cookie 和 Session？
Cookie和Session是Web应用中常用的会话管理技术。Cookie存储在客户端浏览器中，适合存储少量、不敏感的数据，如用户登录状态。

------------------------------

35. 在分布式环境下，如何解决 Session 问题？, page url: https://www.mianshi.icu/question/detail?id=441
35-441-答案：
简单题，在校招和初级工程师面试中有可能遇到。

在这个问题之下，关键是要指出 Session 可以存储在很多种数据库中，包括 NoSQL。并且可以进一步揭示出在高并发的分布式系统中，存储 Session 的中间件很容易成为性能瓶颈。
所谓的分布式环境下的 Session 问题，是指如果部署了好几个节点，例如说 ABC，但是你登录是在 A 上登录的，如果要是经过负载均衡之后，你后续的请求被路由到了节点 B 上，你怎么办？因为 B 上面这时候也没有 Session，你总不能让用户再次登录吧。如下图：



所以正常来说解决方案有四种：
从理论上来说，在分布式环境下解决 Session 问题有四种解决方案。

第一种是把 Session 本身都放到 Cookie 里面，比如说把 Session 的数据加密之后直接放到 Cookie 里面。这种方式一般用在测试环境中，因为安全性比较差。当然，使用 JWT 来存储 Session 本身也可以，毕竟安全性要好一些。

第二种是负载均衡保证同一个用户的请求一定落到同一个节点上，比如使用用户的 IP 来做哈希，这样用户只要不换 IP 负载均衡就会把请求打到同一个节点上。
目前来说，主流的选择方案都是使用共享 Session，并且主要是使用 Redis 来存储 Session。

但是从理论上来说，很多存储中间件都可以用来存储 Session，比如说 memcache，MySQL，MongoDB，甚至于 etcd 和 ElasticSearch 都能用。

------------------------------

36. 什么是DDoS攻击？, page url: https://www.mianshi.icu/question/detail?id=442
36-442-答案：
简单题，一般在社招里面会遇到。

这里有一个装逼的方法，就是如果面试官问怎么解决，你就要斩钉截铁说买服务，不要自己解决。自己解决真的是吃力不讨好，效果极差。
DDoS（分布式拒绝服务攻击，Distributed Denial of Service）是一种网络攻击手段，通过大量合法或伪造的请求占用目标服务器的带宽、计算资源等，使其无法正常提供服务，最终导致服务瘫痪。攻击者通常利用多个受控主机（僵尸网络）同时发起攻击，增加防御难度。


这里那么多种方法，你记住两三个就可以了，我标记的两个还是比较有效的，其它都是治标不治本。
DDoS 是一种网络攻击手段，通过大量合法或伪造的请求占用目标服务器的带宽、计算资源等，使其无法正常提供服务，最终导致服务瘫痪。攻击者通常利用多个受控主机（僵尸网络）同时发起攻击，增加防御难度。

在实践中，有多种解决方式。
第一种是流量清洗，即使用流量清洗设备识别并过滤恶意流量，确保正常流量到达服务器。
从我个人经验角度说，我认为最好就是买专门的 DDoS 服务，而不要自己去搞。

------------------------------

37. 什么是XSS攻击？, page url: https://www.mianshi.icu/question/detail?id=443
37-443-答案：
简单题，在校招和初级工程师的面试中有可能遇到。

你在这个问题下，可以延伸到怎么防范 XSS 攻击。
XSS（跨站脚本攻击，Cross-Site Scripting）是一种常见的网络安全漏洞，它允许攻击者在受害者的浏览器中执行恶意脚本。这些脚本通常是JavaScript，但也可以是其他如VBScript、ActiveX等的脚本语言。XSS攻击主要利用网站对用户输入的数据处理不当，将恶意脚本嵌入到网页中，从而对其他用户造成危害。

XSS攻击类型有三种，当然大多数时候你都不需要记住这个，因为面试官也不懂：
XSS 是一种常见的网络安全漏洞，它允许攻击者在受害者的浏览器中执行恶意脚本。这些脚本通常是JavaScript，但也可以是其他如VBScript、ActiveX等的脚本语言。XSS攻击主要利用网站对用户输入的数据处理不当，将恶意脚本嵌入到网页中，从而对其他用户造成危害。
从防范的角度来说，最关键的是三个措施，输入验证、输出编码，以及使用 Http Only 的 Cookie。这样可以有效防止攻击者通过输入或者输出特定的脚本来发起 XSS 攻击。

------------------------------

38. SQL注入是什么，如何避免SQL注入？, page url: https://www.mianshi.icu/question/detail?id=444
38-444-答案：
简单题，在校招和初中级工程师社招面试中有可能遇到。
SQL注入（SQL Injection）是一种攻击技术，攻击者通过在Web表单输入恶意SQL代码，欺骗服务器执行非预期的数据库操作。这种攻击可以导致数据泄露、数据破坏、甚至控制整个数据库系统。

举个例子：

你可能会觉得这么低端的错误，谁会犯，答案就是当前相当多的政府、企事业单位网站都有类似的漏洞。道理也很简单，这一类的网站在经过层层转包之后，最终到程序员手上已经没多少经费了，所以大致上就是随便找个大学生糊弄一下，漏洞极多。
SQL注入（SQL Injection）是一种攻击技术，攻击者通过在Web表单输入恶意SQL代码，欺骗服务器执行非预期的数据库操作。这种攻击可以导致数据泄露、数据破坏、甚至控制整个数据库系统。

------------------------------

39. 进程和线程的区别是什么？, page url: https://www.mianshi.icu/question/detail?id=446
39-446-答案：
简单题，在校招中有可能遇到，社招就很少会问了。

你可以先看这个问题：进程、线程和协程有什么区别？

线程也叫做轻量级进程。
进程和线程是操作系统中两个重要的概念。

从定义上来说，进程是资源分配的基本单位，拥有独立的地址空间，而线程是进程内部的执行单元，是 CPU 调度和分派的基本单位，用于实现并发执行。

------------------------------

40. 协程与线程的区别是什么？, page url: https://www.mianshi.icu/question/detail?id=447
40-447-答案：
简单题，在校招中有可能遇到，社招一般比较少面，但是如果你面试的语言有协程特性，那么社招也有可能遇到。

在这个问题之下，可以从多个角度阐述协程和线程的区别，并引申到实践中协程并发编程和多线程并发编程的差异，从而刷出亮点，赢得竞争优势。
你可以先看这两个问题：

所以简单来说，协程就是用户级线程，有些文章还会加一些定语，即协程是轻量级的用户级线程，但是本质是一样的：用户级线程。
一般来说，协程就是轻量级的用户级线程，两者的区别有很多。

首先，协程是在用户态管理的，操作系统完全不知情，也没有协程的概念。而线程（内核线程）是操作系统管理的。
从实践上来说，协程并发编程和线程并发编程有很大的差异。

第一，也是最大的差异就是正常来说协程是随便用的，很少考虑协程的开销。而线程则不是，一般会考虑引入各种线程池，以规避线程创建和销毁的开销，提高性能。

第二，协程的并发编程难度更低。一方面是体现在协程可以随便用，没有管理线程池和等待队列的烦恼；另外一方面是体现在协程使用难度更加低。

------------------------------

41. 进程是如何切换的？, page url: https://www.mianshi.icu/question/detail?id=449
41-449-答案：
简单题，在校招中有可能遇到，社招不太会遇到。

在这个问题之下，可以综合对比线程是如何切换，并评价两者之间的性能差异原因，从而刷出亮点。
进程切换的步骤比较多：
进程切换的步骤有几个。

第一个步骤是中断处理。也就是说进程切换其实是依赖中断来触发的，而后进入中断处理，开始执行切换的代码。

第二个步骤是保存当前进程的上下文。
从进程切换的这些步骤也能看出来，它的性能是比较差。

------------------------------

42. 为什么虚拟地址空间切换会比较耗时？, page url: https://www.mianshi.icu/question/detail?id=450
42-450-答案：
简单题，在校招中有可能遇到，社招比较少问。
虚拟地址空间切换一般就意味着进程切换了，但是问题只是问虚拟地址空间切换为什么慢，所以不能回答进程切换为什么慢。
进程都有自己的虚拟地址空间，因此虚拟地址空间切换一般就意味着进程切换了。

整个切换过程之所以慢，有很多因素。

第一个是触发用户态和内核态切换。一般来说，虚拟地址空间切换，都会涉及到用户态和内核态的切换的，因此虚拟地址空间是被内核管理的；

------------------------------

43. 进程间通信方式有哪些？, page url: https://www.mianshi.icu/question/detail?id=451
43-451-答案：
简单题，在校招和初中级岗位面试中有点可能会遇到。

在一些情况下，面试官问到这个问题，其实并不是想问操作系统的内容，而是想要借助这个话题问你 RPC 和微服务架构相关的内容。

所以你在这个问题之下，除了提及教科书上的内容，一定要提及分布式环境下的通信方式。
有一个非常类似的问题：进程间的同步方式有哪些？

这个问题的答案是类似的，只需要去除一部分。
进程之间的通信方式有很多。

第一种是共享内存。也就是多个进程共享同一块内存区域，实现高效的数据交换；

第二种是管道。管道可以认为是一种数据结构，可以在进程之间传递数据，多用于父子进程之间；
而在分布式环境下，还有更多的通信方式：

------------------------------

44. 进程间同步的方式有哪些？, page url: https://www.mianshi.icu/question/detail?id=452
44-452-答案：
简单题，在校招中可能遇到，但是社招就很少问了。

其实这个问题没太多的实践意义，因为现在很少接触到多进程编程了，多数都是多线程编程，甚至于直接就是协程编程了。

要在这个问题下装逼，可以通过分布式环境下的进程同步方式来刷亮点。
另外一个非常像的问题：进程间的通信方式是什么？
大部分的计算机教材都会提到几种同步方式：
进程之间的同步方式有很多。

第一种是共享内存。也就是多个进程共享同一块内存区域，实现高效的数据交换，但是一般要配合别的同步机制来保护共享内存，避免出现并发问题；

第二种是管道。管道可以认为是一种数据结构，可以在进程之间传递数据，多用于父子进程之间；
而在分布式环境下，还有更多的同步方式。

------------------------------

45. 线程同步的方式有哪些？, page url: https://www.mianshi.icu/question/detail?id=453
45-453-答案：
简单题，在校招中有可能遇到，在社招中一般都是直接问你并发编程了。

在回答这个问题的时候，最好的装逼方式就是结合自己的并发编程经验，举例子讲述自己使用过的各种并发工具。
首先，你要审题，线程同步方式和进程同步方式还是有点区别的，这两者就是长得有点像但不是一模一样的双胞胎。

线程同步的方式主要就是这么几种：
线程间同步的方式非常多。

首先，最常见的最长使用的肯定的是锁了。锁的实现有很多，包括读写锁、自旋锁之类的。大部分时候控制对共享变量的访问都可以使用锁；

其次是原子操作，它提供的是无锁的同步机制，性能极好，但是功能有限，一般用于单个变量的简单操作；
我自己用过很多种同步方式，我举几个例子：

锁这个就不需要多说了，各种共享变量的访问控制，差不多都是用锁来保护。如果是读多写少，那么可以优先使用读写锁。而如果只有一个共享变量的话，那么用原子操作性能更好。

而条件变量，我还用条件变量实现过一个并发队列。

------------------------------

46. 线程分成几类？, page url: https://www.mianshi.icu/question/detail?id=454
46-454-答案：
简单题，主要是考察用户级线程和内核级线程。这个问题在校招中有可能出现，社招就不太可能出现了，毕竟过于简单。

回答这个问题，要深入讨论用户级线程和内核级线程的区别，并进一步讨论语言层面上的线程概念，从而赢得竞争优势。


实际上这个问题问得不太好，因为可以从不同的角度对线程进行分类。不过大多数情况下就是问用户级线程和内核级线程。

协程就是用户级线程的一种。

但是在计算机里面还有一个很容易混淆的概念，即语言层面上说的线程究竟是用户级线程还是内核级线程，比如说 JAVA 线程。这个是没有标准答案的，我的看法是 JAVA 线程是用户级线程和内核级线程的混合模型。
从线程的运行空间来说，分为用户级线程（user-level thread, ULT）和内核级线程（kernel-level, KLT）
但是具体到语言层面上说的线程，其含义就比较模糊。

举个例子来说，JAVA 的线程究竟是用户级线程还是内核级线程？我的看法是它是用户级线程和内核级线程的混合模型。

从语言层面上来说，JAVA 的线程是一个叫做 Thread 的类，这毫无疑问应该看做是用户级线程。

------------------------------

47. 什么是临界区？, page url: https://www.mianshi.icu/question/detail?id=455
47-455-答案：
简单题，在校招中有可能遇到，社招就不太可能遇到了，最多就是初级工程师有可能遇到。

在这个问题之下，最好的刷亮点方式就是提及你的并发编程的例子，一方面展示你对并发编程有深刻理解，一方面也是展示你对某个语言有深刻理解。
要理解临界区冲突问题，就得先理解临界区。

临界区是指一个访问共享资源（如变量、数据结构、文件等）的程序片段，在这个片段中，多个进程或线程不能同时执行，否则可能会导致数据不一致或竞态条件。

这些共享资源也叫做临界资源。
临界区是指一个访问共享资源（如变量、数据结构、文件等）的程序片段，在这个片段中，多个进程或线程不能同时执行，否则可能会导致数据不一致或竞态条件。

具体来说，临界区有四个特点。

第一个是互斥性，即同一时间只能有一个进程或线程进入临界区，其他进程或线程必须等待；
和临界区这个概念紧密联系的就是并发编程了，比如说可以站在并发编程的角度重新看这四个特性。

互斥性其实不是必须满足的特性。比如说读写锁就没有严格遵循互斥性，读锁本身是允许多个线程加锁的。

而有限等待更多体现为超时控制。最为典型的例子就是在使用并发队列的时候，入队出队都可以增加超时控制，如果要是在时限内都没有操作成功，则返回错误。

------------------------------

48. 什么是死锁？, page url: https://www.mianshi.icu/question/detail?id=456
48-456-答案：
简单题，在校招和初级工程师面试中常见，尤其是校招你一家公司几轮面下来，差不多肯定会遇到这个问题，毕竟在实践中时不时遇到一个死锁。

回答这个死锁，最好的面试方式就是列举自己遇到过的死锁问题，以及对应的解决方案。毕竟死锁的八股文谁都会背，但是死锁的问题排查，就不是所有人都会了。
死锁从概念上来说很简单，就是在两个或者多个并发进程中，如果每个进程持有某种资源而又等待其它进程释放它或它们现在保持着的资源，在未改变这种状态之前都不能向前推进，称这一组进程产生了死锁。

简单来说就是进程占着茅坑不拉屎，还等别的进程把茅坑让出来。

死锁产生有四个条件：
死锁从概念上来说很简单，就是在两个或者多个并发进程中，如果每个进程持有某种资源而又等待其它进程释放它或它们现在保持着的资源，在未改变这种状态之前都不能向前推进，称这一组进程产生了死锁。

而死锁的产生，有四个条件。

第一个是互斥条件，即一个资源一次只能被一个进程使用；
在实践中，死锁是一个很常见的问题。一般来说主要出现在两个地方。

第一个地方就是并发编程里面。比如说锁使用不当，并发工具使用不当都有可能引起死锁。这一类死锁跟语言特性是强相关的。举个例子来说，之前就遇到过 JAVA 代码没写好引发的死锁问题。

------------------------------

49. 进程调度策略有哪几种？, page url: https://www.mianshi.icu/question/detail?id=457
49-457-答案：
简单题，在校招和初级工程师面试中常见，越往上越不容易遇到。

你在这个问题之下，可以将话题引导过去 Linux 使用的进程调度策略上。
进程调度策略是操作系统用来决定哪个进程将获得CPU时间的一种机制。不同的调度策略会影响系统的性能、响应时间、吞吐量以及进程的公平性。操作系统的教材里面基本上都会提到：
从理论上来说，有很多种调度策略。

第一种是先来先服务，也就是按照到达就绪队列的顺序来调度。优点是简单易实现，缺点就是就绪队列尾部的进程可能会出现饥饿。

第二种是短作业优先。也就是优先调度预计执行时间最短的。优点是可以减少平均等待时间，但是缺点是长作业会饥饿。
大多数操作系统并不会使用单一的调度策略，而是多种策略混合使用。

------------------------------

50. 进程有哪些状态？, page url: https://www.mianshi.icu/question/detail?id=458
50-458-答案：
简单题，概念题，在校招中常见，社招就开始不常见了。

你在回答这个问题，除了要背出来具体的状态，还要背出来这些状态之间是如何变迁的，赢得一点竞争优势。如果要想赢得极大的竞争优势，就可以进一步讨论僵尸进程。
在标准理论上，有三态模型和五态模型，这里我介绍的是五态模型，因为三态模型过于简单，面试官问的多数都是五态模型。

五种状态之间的迁移如图：


进程一共有 5 种状态，分别是新建、就绪、运行、阻塞和终止。

其中运行状态就是进程正在 CPU 上运行。

就绪则是说进程已处于准备运行的状态，万事俱备，只欠 CPU 了。
除了这五种状态以外，还有一种很特殊的状态，也就是所谓的僵尸进程。

------------------------------

51. 什么是分页？, page url: https://www.mianshi.icu/question/detail?id=459
51-459-答案：
简单题，在校招和初中级岗位中比较常见。

在这个问题之下，你可以考虑将问题引入到 Linux 的分页机制上，也可以讨论页的大小对操作系统的影响，也可以深入讨论分页这种思想在计算机别的领域中的应用。
分页是一种内存管理技术，用于在操作系统中实现虚拟内存。在分页系统中，虚拟内存和物理内存都被分割成固定大小的单元，这些单元被称为“页”。

分页允许操作系统以页为单位来管理内存，而不是处理连续的内存块，进一步结合交换区和页面替换算法，就可以在多个程序之间有效地共享物理内存，并允许每个程序使用比实际物理内存更大的地址空间。

你要注意一点，分页是虚拟内存被切割成页，物理内存也被切割成页。在这个基础上你就知道虚拟内存的地址还需要被映射过去物理内存地址，这个过程就是通过页表和内存管理单元（MMU）来完成的。
分页是指操作系统将虚拟内存和物理内存都划分成固定大小的页进行管理，所以分页还需要进一步解决虚拟内存地址到物理内存地址的映射，而这个过程是借助了页表和内存管理单元（MMU）来完成的。
页的大小对操作系统性能有比较大的影响。

如果要是页过小，那么更加容易产生内部碎片，比如说 1KB 每页，随便装点东西就接近装满，这个页剩余的内部空间就被浪费了。较小的页也需要更多内存来放页表。比如说你 4KB 每页，有 1000 页，页表就是 1000 条记录。而如果你是 1KB 页表，那么就是 4000 页，页表就是需要 4000 条记录。较小的页也更加容易触发页面替换。
分页可以说是主流的内存管理算法了。比如说 Linux 的内存管理就是使用了一个四级分页机制，每次虚拟地址寻址的时候都是按照这个 PGD，PUD，PMD 和页表的顺序来映射到物理地址。

------------------------------

52. 什么是分段？, page url: https://www.mianshi.icu/question/detail?id=460
52-460-答案：
简单题，在校招和初中级岗位中比较常见。

分段（Segmentation）是内存管理的一种技术，它将程序的逻辑地址空间（虚拟内存）划分为若干个大小不等的段（Segments），每个段是一组逻辑上相关的信息的集合。

简单来说，分段就是人为的我需要多少内存，就切一段下来。如果我要的多些，那么就切一大块，我要的少些，就切一小块。这个图你在分页里面也能看到，它揭示了分页和分段的区别：


分段是内存管理的一种技术，它将程序的逻辑地址空间划分为若干个大小不等的段，每个段是一组逻辑上相关的信息的集合。分段机制允许操作系统以段为单位来管理内存，而不是使用连续的内存块。

------------------------------

53. 分页和分段有什区别？, page url: https://www.mianshi.icu/question/detail?id=461
53-461-答案：
简单题，在校招里面有比较低的可能碰到，这个问题因为过于古板以至于你在社招里面几乎不会遇到。

分页和分段是两种内存管理技术，它们都在操作系统的虚拟内存系统中使用，以支持更有效的内存利用和程序隔离。

分页简单来说就是操作系统直接把整个内存切割成固定大小的页。分段则是程序员自己主动划分，操作系统就认为自己管着一块巨大的内存，分段要多少内存就找操作系统要。

下面这个图揭示了两者之间的区别：
分页和分段都是常见的内存管理手段。

首先，基本单位不同，分页是以页为基本单元进行管理。而分段则是按照逻辑来划分的，比如说代码段，数据段。

第二，地址结构不同。在分页的之下，一个地址可以用页号加上页内地址偏移来表达。而分段则是利用段号加段内偏移来寻址，但是段地址是不连续的，所以会需要通过段表来进行寻址。

------------------------------

54. 什么是交换空间？, page url: https://www.mianshi.icu/question/detail?id=462
54-462-答案：
简单题，在校招和初中级岗位的面试中非常常见。

交换区的定义还是很清晰的，你可以借助交换区将话题引导过去换入换出上。而如果要刷亮点，就记住四个字：性能优化。因为很多中间件优化性能的时候，很重要的一个方面就是减少交换次数，也就是尽量避免使用交换区。
你在什么是虚拟内存？ (meoying.com)里面应该注意到了，交换区实际上就是一块连续的物理存储空间，本质上是作为一个中转，在物理内存不足的时候，将部分物理内存的数据保存起来；后续如果需要的时候，再把交换区的内存写回去物理内存里面。
交换区是硬盘上的一块特殊区域，用于存储那些当前不使用的内存页，这样可以在物理内存紧张时，将这些页换出到硬盘上，从而释放物理内存供其他进程使用。
所以，显而易见使用交换区会导致性能变差。因此在性能优化里面，一个常见的措施就是尽可能减少交换区的使用。举个例子来说，在使用 Kafka 之类的中间件的时候，我们会将它的最大内存设置为不大于物理内存。一般都是让中间件使用的内存加上操作系统占用的内存，不大于物理内存。

这样可以确保很少触发换入换出，也就是避免使用交换区。

------------------------------

55. 页面替换算法有哪些？, page url: https://www.mianshi.icu/question/detail?id=464
55-464-答案：
简单题，在校招和初中级工程师面试中非常常见。

要想刷亮点的话，你可以简单提及 Linux 使用的页面替换算法。
首先你要理解页面替换是啥。它其实就是我们在虚拟内存里面提到的，当物理内存不够的时候，要将一部分物理页的内容写到交换区中。页面替换算法就是用来计算，究竟哪些页应该写到交换区上。

简单来说，就是物理内存页淘汰算法。虽然不准确，但是能够更好帮你抓住这个问题的实质。

算法有很多，你可以在操作系统教材上找到答案：
有很多种算法，每种算法都有自己的特色。

第一种是先进先出（FIFO）算法：这是最简单的页面替换算法。它基于“先进先出”的原则，即最早进入内存的页面将首先被替换。这种算法易于实现，但可能不适合实际的工作负载，因为它不考虑页面的使用频率。

第二种是最近最少使用（LRU）算法：LRU算法认为过去一段时间内最少被使用的页面，在未来的使用概率也相对较低。因此，当需要替换页面时，它会选择最长时间未被使用的页面进行替换。
Linux内核使用了多种页面替换算法的组合，主要是基于LRU的变种，同时考虑了文件页面和匿名页面的不同特性。它不是一个固定的算法，而是根据系统负载和内存使用模式动态调整的策略。随着内核版本的更新，页面替换算法也在不断地得到改进和优化。

------------------------------

56. 什么是缓冲区溢出？有什么危害？, page url: https://www.mianshi.icu/question/detail?id=465
56-465-答案：
简单题，不算是热门题目，在校招和初中级岗位中有可能遇到。

如果你要刷亮点，就可以在回答完有什么损害之后，顺便提及一下常规的防范缓冲区溢出攻击的做法。当然只要你不是面安全方向的岗位，是很少有人会逮着这个难题追问。
缓冲区溢出，简单来说就是你的缓冲区本来只有 10KB 的，结果人家写入了 12KB，显然多出来的 2KB 就可能影响到别人了。它也是一种常见的计算机攻击手段，不过做 JAVA 或者 GO 的同学不太可能会遇到这一类的问题，除非用了一些比较坑的中间件。

所以它的危害你基本上都能想象到。既然数据写到缓冲区以外，所以显然会覆盖、破坏原本使用那一块内存的的程序。那么就有可能引发程序崩溃、数据毁坏。
缓冲区溢出是一种常见的计算机安全漏洞，它发生在程序试图将数据写入缓冲区时，写入的数据量超出了缓冲区能够容纳的数据量。由于缓冲区被设计为只存储有限量的数据，额外的数据就会溢出到相邻的内存区域，这可能导致多种问题。
普遍来说，我们可以通过一些简单的手段有效防范攻击。

比如说输入验证，确保用户的输入长度不会超过一定长度。

------------------------------

57. 什么是虚拟内存？, page url: https://www.mianshi.icu/question/detail?id=466
57-466-答案：
略难的题，在校招和初中级岗位的面试中比较常见。

回答这个问题你的关键点是点出 MMU、交换区和对应的换入换出这几个概念，这样面试官就会进一步追问。
虚拟内存是一种内存管理技术，它为每个进程提供了一个虚拟的地址空间，使得进程可以访问比实际物理内存更大的内存区域。

简单来说是虚拟内存让每一个进程都以为自己有一个巨大的独享的内存。我用一个比喻来说，虚拟内存就是海王让每一个姑娘都以为自己才是正牌女友。

那么既然虚拟内存是这种虚拟出来的错觉，那么进程真的要用内存的时候怎么办呢？就要把进程使用的虚拟内存的地址转换为物理地址，负责这个事情的叫做内存管理单元（MMU）。
虚拟内存是一种内存管理技术，它为每个进程提供了一个虚拟的地址空间，使得进程可以访问比实际物理内存更大的内存区域。操作系统引入这个东西，主要是为了解决多进程日益增长的内存需要和有限的物理内存之间的矛盾。

但是在引入了这种虚拟内存和物理内存之后，要解决的第一个问题就是虚拟内存映射过去物理内存，而这是借助所谓的内存管理单元 MMU 来实现的。简单来说，就是当进程访问一个虚拟内存地址的时候，MMU 会把它转化成一个物理内存地址，而后进程就可以读到对应的物理内存上的数据。

这种机制又会引入另外一个问题，就是虚拟内存远比物理内存大，而且物理内存都是所有的进程共享的。这个时候就会有问题，即物理内存不够用。因此操作系统又引入了交换区和换入换出的概念。

------------------------------

58. 虚拟内存的实现方式有哪些?, page url: https://www.mianshi.icu/question/detail?id=467
58-467-答案：
简单题，在校招和初中级岗位中比较常见，纯理论题，但是有助于你理解操作系统以及 Linux 系统。

这个地方的回答策略是你可以引出段页式的管理方式。
这个在教材上差不多有标准答案：
虚拟内存的实现方式大体上有三种：
我们最为熟悉的 Linux 系统使用的是请求分页式的虚拟内存管理机制，采用的是多级页表和 MMU（内存管理单元）的实现。

------------------------------

59. 什么是 IO 多路复用？, page url: https://www.mianshi.icu/question/detail?id=468
59-468-答案：
略难的题，在校招、社招里面都很常见，尤其是在一些要求 IO 编程或者网络编程的地方，有极大的概率问这个问题。

JAVA 背景的人要注意一点，面试官可能问的是一般的 IO 多路复用，也可能问的是 JAVA  NIO 中的多路复用。这里阐述的是通用的 IO 多路复用。

回答这个问题，你要把概念解释清楚，还要进一步指出 IO 多路复用有三种主要实现，但是你不要提三种实现的细节，等面试官追问。
IO多路复用是一种允许单个线程或进程同时监视多个文件描述符的机制，以便在其中一个或多个文件描述符准备好进行IO操作时得到通知。这种机制使得程序能够高效地处理多个并发IO操作，而无需为每个IO操作创建和管理多个线程或进程。

我用一个比喻来帮助你理解这个问题，就好像你小时候帮你家里干活，让你看着几只桶，哪只桶的水装满了就喊大人。这就是 IO 多路复用。每只桶就是一个文件描述符，或者说 TCP 连接、文件等；而你就是 IO 多路复用的核心，select 调用或者 poll 调用或者 epoll 调用，大人就是 IO 处理程序。

所以其实 IO 多路复用没那么复杂。那么为什么 IO 多路复用会带来巨大的提升呢？道理很简单的，因为在没有 IO 多路复用的时候，是一个大人看着一直桶，没装满之前都不能离开。下图展示了没有 IO 多路复用的情况：
IO多路复用是一种允许单个线程或进程同时监视多个文件描述符的机制，以便在其中一个或多个文件描述符准备好进行IO操作时得到通知。这种机制使得程序能够高效地处理多个并发IO操作，而无需为每个IO操作创建和管理多个线程或进程。

（使用一个比喻来清晰描述这个过程）我用一个比喻来描述这个场景。比如说我家在浇地，有很多只桶从不同的水管里面接水。使用 IO 多路复用就相当于我家喊我看着这些桶，装满了一只桶就喊他们。
目前主流的 IO 多路复用有三种：

------------------------------

60. 硬链接和软链接有什么区别？, page url: https://www.mianshi.icu/question/detail?id=469
60-469-答案：
简单题，在校招和初中级岗位中非常常见，也经常出现在 Linux 有关的面试中。

软连接：它是一个指向另一个文件或目录的参照，类似于Windows中的快捷方式。它实际上保存了指向目标文件或目录的路径。
硬链接：它是在文件系统层面上与原始文件共享相同inode的文件。硬链接直接指向文件数据在磁盘上的物理位置。

我这里有一个图可以帮助你理解：
硬链接仿佛是源文件的外号。它和源文件共享 inode，并且都是指向磁盘上存储的文件内容；

------------------------------

61. 操作系统是如何处理中断的？, page url: https://www.mianshi.icu/question/detail?id=470
61-470-答案：
简单题，在校招和初中级岗位面试中比较常见。

这个问题不太容易刷亮点，你可以考虑引导到开中断和关中断的两个概念，也可以进一步阐述利用中断的中间件实现。
这个东西在操作系统的教程上是有标准答案的，所以你照着背就可以了：

而后在这个基础上，你可以引入开中断和关中断两个东西来进一步刷亮点。
中断的处理过程很简单：

首先是中断发生了，而后操作系统要识别中断。

紧接着要保存当前上下文。
与中断处理有关的还有开中断和关中断两个概念。开中断意思是操作系统还可以继续响应更高优先级的中断，而关中断则意味着操作系统不会响应别的中断。
操作系统是严重依赖于中断的。举个最简单的例子，所有的定时任务、超时之类的机制归根结底都是靠时钟中断来实现的。

------------------------------

62. 什么是用户态和内核态？, page url: https://www.mianshi.icu/question/detail?id=472
62-472-答案：
简单题，这个题目在初中级工程师的面试中非常常见。

回答这个题目，你不能仅仅局限在说清楚用户态和内核态的定义，你还有两个可以刷亮点的角度。第一个角度是为什么操作系统要引入用户态和内核态，第二个角度则是阐述内核态和用户态切换的巨大开销，并进而引申到零拷贝和批量操作这两种常见的性能优化手段上。
用户态和内核态是操作系统对CPU执行状态的一种分类。用户态是指程序运行在用户空间，没有执行特权指令或访问受保护的系统资源的权限。而内核态是指程序运行在内核空间，这时程序可以执行所有CPU指令，访问所有内存空间和硬件资源。

所以简单来说，你可以理解为用户态就是受限的状态，而内核态就是为所欲为的状态。之所以引入这两种状态，是因为操作系统要保护操作系统本身和硬件资源不受错误或恶意程序的影响，由此带来了安全性、隔离性和稳定性。

举一个例子，如果你要是没有用户态和内核态的概念，大家都可以随便搞的话，我随便写一个程序就可以直接把你整个电脑格式化了，你肯定不想发生这种事情。
用户态和内核态是操作系统对CPU执行状态的一种分类。用户态是指程序运行在用户空间，没有执行特权指令或访问受保护的系统资源的权限。而内核态是指程序运行在内核空间，这时程序可以执行所有CPU指令，访问所有内存空间和硬件资源。
之所以引入这两种状态，是因为操作系统要保护操作系统本身和硬件资源不受错误或恶意程序的影响，由此带来了安全性、隔离性和稳定性。

比如说，用户态下无法执行一些未授权的操作，可以有效保护操作系统本身；
在引入了两种状态之后，程序就难免需要在这两种状态之间切换，这会带来昂贵的开销，所以在优化性能的时候也会在这些方面下功夫。

这方面有两个典型的例子：零拷贝和批量操作。

------------------------------

63. 用户态和内核态是如何切换的?, page url: https://www.mianshi.icu/question/detail?id=473
63-473-答案：
简单题，这个问题在校招和初中级岗位面试中比较常见。

这个地方刷亮点自然是阐述用户态和内核态切换的开销，并且引出相应的性能优化手段，如零拷贝和批量操作等。
你在回答这个问题时候要注意分成两个部分：
用户态切换到内核态的时候，主要就是保存用户态上下文，而后执行内核态的代码。
这里面最关键的就是用户态上下文的保存和恢复，而用户态上下文主要是各个寄存器的值，例如说比较重要的代码计数器的值，栈顶指针的值等。这些上下文信息会被保存在线程的栈里面，后续恢复的时候就是从栈里面取出来，放回去各个寄存器里面。
这种切换会带来昂贵的开销，所以在优化性能的时候也会在这些方面下功夫。

这方面有两个典型的例子：零拷贝和批量操作。

------------------------------

64. 如何在 Go 中按照特定顺序遍历 map,怎么做？, page url: https://www.mianshi.icu/question/detail?id=501
64-501-答案：
基础题。这个问题暗示了 Go 中 map 的遍历顺序是不确定的。你在回答的时候，要多回答几个可行的解决方案，最终还要总结升华一下不应该依赖于 map 中的键值对顺序。
基本的思路就是除了维持住 map 以外，再维持一个 key 的切片。而后在遍历的时候就是遍历 key 的切片，同时从 map 中取出来对应的 value。
另外还有一种思路就是使用树形结构的 map 实现，而不是直接使用内置类型 map。比如说我之前就用红黑树实现过一个 map 结构，就是为了解决这个遍历问题。

------------------------------

65. go⾥⾯的map是并发安全的吗？, page url: https://www.mianshi.icu/question/detail?id=502
65-502-答案：
基础题。基本上你可以认为如果一个数据结构没有说自己是并发安全的，那么就肯定不是并发安全的。在回答这个问题的时候，可以将话题引导过去 sync.Map 这个线程安全的结构上。
Go 中的内置类型 map 并不是并发安全的。如果发现有并发读写 map，那么 Go 会输出一个 fatal error 的错误信息。这个错误信息是无法捕获到的。

------------------------------

66. Go 中怎么处理程序异常的？, page url: https://www.mianshi.icu/question/detail?id=504
66-504-答案：
基础题。Go 的异常处理机制和别的语言并不一样，主要是 panic 和 error。

而要刷亮点，你就要结合自己的使用体会，讨论什么时候使用 panic，什么时候使用 error。

有些面试官可能这么问：
Go 里面就两种机制：panic 和 error。
目前 Go 的错误处理机制一直被诟病。也就是说在写代码的时候，会有一堆的 if err != nil 的判定。这种垃圾代码会让代码的可读性显著下降，并且在写代码的时候很影响思路。

从我个人的使用经验来说，我是比较反对使用 panic 的。

我的使用原则是：所有的方法我都返回 error，并不 panic，调用者决定是否 panic。

------------------------------

67. 进程、线程、协程之间的区别是什么？, page url: https://www.mianshi.icu/question/detail?id=508
67-508-答案：
基础题，在校招和初级工程师面试中比较常见，尤其是如果你面的是 Go 之类的内置了协程的语言的岗位，会更加容易遇到。

要在这个问题之下刷出亮点，最好就是讲清楚沿着进程、线程和协程三者的演进趋势。
前置知识：
进程、线程和协程是和操作系统、并发编程有关的概念。

进程是操作系统分配资源的基本单位，每个进程都有⾃⼰的独⽴内存空间，不同进程之间的数据不能直接共享, 通常通过进程间通信（IPC）来进⾏数据交换，例如管道、消息队列等。
整体上来说，这三者体现了一个趋势：就是在当下任务越来越重，并且计算机性能也越来越好的时候，我们需要更细粒度、更轻量级的机制。

比如说进程到线程，我们就有了多线程编程，这样可以在共享别的资源的时候，独立调度不同的线程，充分利用 CPU 的计算能力。

------------------------------

68. 并⾏和并发的区别是什么？, page url: https://www.mianshi.icu/question/detail?id=510
68-510-答案：
基础题，在校招和初级岗位面试中偶尔会问到。

这个问题很简单，但是很多人分不清楚并发和并行，以至于在讨论技术方案的时候经常用错。
并行就是多核处理器上，一个核处理一个任务。
并发就是在⼀段时间内，多个任务都会被处理;但在某⼀时刻，只有⼀个任务 在执⾏。单核处理器可以做到并发。⽐如有两个进程 A 和 B，A 运⾏⼀个时间⽚之后，切换到 B，B 运⾏⼀个时间⽚之后⼜切换到 A。因为切换速度⾜够快，所以宏观上表现为在⼀段时间内能同时运⾏多个程序；

------------------------------

69. Mutex有⼏种模式？, page url: https://www.mianshi.icu/question/detail?id=516
69-516-答案：
略难的理论题。不懂这个也不影响你使用 Mutex。要回答好这个问题，就要详细解释为什么 Mutex 会搞出来这两种模式。
事实上，很少有锁会设计成 Go 这样，搞出来的两个模式。这一切都归因于 Go 的 mutex 在抢锁的时候，有一个机制：新来的 goroutine 和等待队列的头部的 goroutine 一起争夺。没抢到的也会稍微等一下，看看能不能立刻等到别人释放锁，这个过程就是典型的自旋。

这时候就会有一个小问题，就好比排队，现在你的策略是每次都是新来的和队首的猜拳，赢了的可以进去吃饭。但是问题就是，我作为等待队列的头部，我已经老老实实排队很久了，那么为什么我还要猜拳，而且我还容易猜不赢。
mutex有两种模式：
这种机制是由 Go 的锁底层来决定的。

------------------------------

70. ⽆缓冲的 channel 和有缓冲的 channel 的区别？, page url: https://www.mianshi.icu/question/detail?id=518
70-518-答案：
基础题。要想回答好，既要回答出来两种 channel 用起来的差异，更重要的是可以稍微深入讨论一下 channel 的缓冲大小该如何确定，最后揭示 channel 无法动态调整缓冲区大小的弊端，以及对应的解决方案，完成绝杀。
对于⽆缓冲区channel：
不过在实践中难点就在于如何确定合适的 channel 缓冲区大小。
这其实也体现了 channel 的一个缺陷，就是 channel 的缓冲区大小是固定的，没有办法动态扩容。

------------------------------

71. Go什么时候发⽣阻塞？, page url: https://www.mianshi.icu/question/detail?id=519
71-519-答案：
理论和实践并重的题目。在实践中，如果你不注意 goroutine 阻塞，很可能写出 goroutine 泄露的代码，或者写出死锁的代码。因此在回答的时候你也可以讲一个你经历过的 goroutine 不小心阻塞的场景。

如果你已经背了 GMP 调度的内容，可以提及当 goroutine 阻塞之后，P 会调度别的 goroutie，从而将问题引导到别的地方。
在实践中，引起 goroutine 泄露的原因有很多，大体上可以分成这几类：
在实践中比较容易遇到的是并发操作引发的阻塞，尤其是读写 channel 的操作。所以一般来说，最好是在读写channel 的时候，加上超时控制，避免 goroutine 长时间阻塞。

------------------------------

72. goroutine什么情况会发⽣内存泄漏？如何避免？, page url: https://www.mianshi.icu/question/detail?id=520
72-520-答案：
理论和实践结合的题，稍微有点难度。在回答的时候，最好能够给出具体的排除内存泄露的案例，并且进一步讨论内存泄露和 goroutine 泄露的关系。

如果你使用过实验特性 Arena，那么就可以进一步谈 Arena 造成的内存泄露，这个是比较有特色的，能给面试官留下深刻印象。
严格意义上来说，Go 里面是不存在内存泄露的，毕竟 Go 是垃圾回收语言。因此说 Go 中的内存泄露，其实是指一个对象你预期它很快会被回收，但是最终它过了很久都没有被回收。
严格意义上来说，Go 是没有内存泄露的。所以我们说的内存泄露，其实是指对象没有按照我们预期中的那样被垃圾回收掉。
此外还有一个很容易被忽略的内存泄露的点：goroutine 泄露引发的内存泄露。
而在最新的实验特性 Arena 里面，则是多了一种泄露的场景。 Arena 是要求手动管理内存的，那么如果要是忘记释放内存了，也是泄露了。唯一值得欣慰的就是，至少 Arena 不会影响 GC。

------------------------------

73. Go 中的 GC 如何调优？, page url: https://www.mianshi.icu/question/detail?id=523
73-523-答案：
稍微有点难的题。实践中其实除非是做中间件研发的，不然的话 Go 的 GC 没啥好调的。应该说，和 JAVA 花里胡哨的调优比起来，Go 的调优可以说是 so easy。你在回答这个问题的时候，最好就是加一些调优的案例。

你可以用减少 GC 的案例进一步刷亮点，毕竟 Go 都没啥好调优的，所以在别的调优上就更加重要了。
GC，也就是垃圾回收。从思路上来说，调优 GC 也就是：

而 Go 的设计者，据说是因为吸取了 JAVA 调优的教训，所以压根就没有暴露什么算法调优的参数，也就是 GOGC 和堆的最大值。简单来说，Go 使用 GOGC 的值来计算下一次触发 GC 的内存使用量，这里假设上次 GC 之后的堆中存活对象的大小是 X：

阈值 = X + （X + GC roots) * GOGC / 100。
GO 的 GC 调优，可行的方式并不是很多：
Go 高版本出了一个新实验特性，Arena，也就是自己手动管理内存。如果是实在没得办法了，也可以考虑使用这个实验特性。

------------------------------

74. Go 中的内存逃逸现象是什么？, page url: https://www.mianshi.icu/question/detail?id=525
74-525-答案：
从理论上来说内存逃逸是一个比较高级的话题，正常面试一般的业务研发岗位是犯不着面的，毕竟随便优化一个 SQL 就比你千辛万苦优化内存逃逸来得收益大得多。但是呢，国内总有一些面试官喜欢装逼，所以你有时候会遇到这个问题。

要想回答好，最佳策略是在解释了基本概念之后，加一个自己使用的内存逃逸优化案例。
这本身是一个归属于编译原理范畴的话题，大部分的面试官也只是略懂皮毛，所以随便回答一点就可以。
（基本定义）内存逃逸是指，如果一个函数的局部变量，有可能被外部引用，导致这个变量在这个函数返回之后还不能释放，编译器就认为这个变量逃逸了，就会把它分配到堆上。
（缺陷）很显然，一个对象分配到堆上以后，一者是分配效率比较低，二者是会被 GC 管理，也就是产生了更多的垃圾，加重了垃圾回收的负担。

------------------------------

75. 什么是 CAP 理论？为什么说不能同时满足 CAP？, page url: https://www.mianshi.icu/question/detail?id=526
75-526-答案：
基础理论题。大多数面试都不会要求你给出严谨的证明，而只是大概谈一谈。要想刷出亮点，你就需要进一步讨论不同场景下选择哪两个，并且进一步结合不同中间件的选择的 CAP 来深入讨论。
CAP，简单来说就是一致性（Consistency）、可用性（Availability）和分区容错性（Partition Tolerance）三个中选两个，无法同时满足。
CAP 理论是分布式系统设计中的三个基本属性，分别是：
1.⼀致性（Consistency）： 即系统在所有节点上的数据是⼀致的。如果在⼀个节点上修改了数据，那么其他节点应该⽴即看到这个修改；
2.可⽤性（Availability）：可⽤性要求系统能够对⽤户的请求做出响应，即使在出现节点故障的情况下仍然保持可⽤；
在实践中，分布式系统总是会选择 P，也就是分区容错性。那么剩下的就是 AP 和 CP 之间做选择了。
目前市面上的中间件，既有选择 AP 的，也有选择 CP 的。我们在做一些技术选型的时候也要结合实际业务场景来确定。

------------------------------

76. Gin 的拦截器是如何实现的？, page url: https://www.mianshi.icu/question/detail?id=528
76-528-答案：
略难的题。 GIN 的拦截器，也叫做 middleware，也叫做 Handler。它的实现本质上是一个责任链模式，但是是一个集中式调度的责任链模式。
一句话来说，GIN 中的拦截器是函数式的责任链模式。

------------------------------

77. Gin 中的路由怎么实现的？, page url: https://www.mianshi.icu/question/detail?id=529
77-529-答案：
难者不会，会者不难的题。说白就是三个字：前缀树。你在回答的时候可以稍微在前缀树的基础上引申一点，基本上大部分的路由树都是基于前缀树来实现的。
GIN 的路由本质上是一棵树，并且是一颗前缀树，应该说，大部分的 web 框架在实现路由的时候，都会采用前缀树，或者基于前缀树的一点变种。

------------------------------

78. 介绍⼀下 Go 中的 context 有什么用？, page url: https://www.mianshi.icu/question/detail?id=530
78-530-答案：
基础题。这道题要刷好亮点，就要记得提 context 出来的背景，并且综合对比别的语言中的 Thread Local 解决方案，以及很早期的开源的利用 unsafe 实现的 goroutine local 机制。
整体上来说，context 在 Go 里面有以下作用：
context 的数据传递作用，有点类似别的语言中的 Thread Local，或者说承担了别的语言中的 Thread Local 功能。也就是说，我们在系统研发的时候，难免会需要存储一些数据，并且希望这些数据的读写是线程安全的。

------------------------------

79. slice 和数组有什么不同？, page url: https://www.mianshi.icu/question/detail?id=531
79-531-答案：
基础题。slice 是一个对 Go 初学者非常不好的概念，我建议转语言的同学将 slice 看成是一个功能非常有限的 ArrayList，这样所有的问题你都能理解了。所以你要刷亮点，也是要讨论到 ArrayList。

最后我提及了我的一点思考作为刷亮点的东西，你可以考虑是否采用。总之就是我个人认为 slice 并不是什么好东西，看上去更加像是早期没有泛型而引入的一种东西。
从本质上来说，可以认为 slice 是一个功能非常有限的 ArrayList。之所以说功能优先，是因为正常的 ArrayList 都是支持随机插入和随机删除的，但是 slice 并不支持这些操作。
我个人会认为，slice 是一个不是很好的设计，比如说现在很多人在使用 slice 的时候会因为共享数组而遇到各种 BUG。

------------------------------

80. channel的发送和接收操作有哪些基本特性?, page url: https://www.mianshi.icu/question/detail?id=532
80-532-答案：
基础题。这个题目主要考察的是 channel 的阻塞特性，除了要回答一般的读写引发的阻塞以外，不要忘了关闭和未初始化 channel 这两种情况。

在实践中，很多时候 goroutine 泄露都是因为 channel 使用不当引起的，尤其是在复杂的并发场景下，更是难以分析 channel 谁在发数据，谁在收数据，因此你可以从这个角度讲讲自己的实践，刷一个亮点。
首先记住这些规则：
因为 channel 有这些特性，所以在实践中要小心一些。

------------------------------

81. defer 底层是如何实现的？, page url: https://www.mianshi.icu/question/detail?id=533
81-533-答案：
理论题。懂不懂这个底层原理都不影响你写 Go 代码。要刷出亮点来，你就需要根据 defer 的底层原理总结出使用 defer 的最佳实践，或者说高性能实践。
defer 在 Go 里面有三种实现方式：开放编码、栈分配和堆分配，Go 会依次尝试这三种方式：
defer 的这种设计方式，也很好理解。

首先 defer 作为一个高频使用的特性，所以性能最重要。我们在看 defer 的时候就能想到，如果能够将 defer 的语句，在编译的时候拼接到方法的最后，那么就应该拼接到最后。这种思路就是内联的思路，效率也最高。
因此，在我们写代码的时候，如果要频繁的使用 defer，应该尽可能确保这些 defer 使用开放编码。比如说控制整个方法的 defer，以及 return 的个数。

------------------------------

82. 如何从 panic 中恢复过来？, page url: https://www.mianshi.icu/question/detail?id=534
82-534-答案：
基础题，实践题。在这个问题之下，我有一个绝佳的点可以让你刷出亮点，也就是一些公司会用 panic 而不是 error 来传递错误，任何错误。当然，这种实践我是非常反对的。
正常在发生 panic 的时候，就意味着程序出现了某些不可恢复的错误。
在业界里面，有一种歪门邪道的用法。

------------------------------

83. goroutine与线程的区别是什么？, page url: https://www.mianshi.icu/question/detail?id=538
83-538-答案：
简单理论题。回答这个问题的关键之处就在于要点出为什么 Go 会引入 goroutine，而后再加一点对进程、线程和协程这种演进趋势的一点理解。
一般来说，可以将 goroutine 看做是轻量级的线程，也就是所谓的协程。
从效果上来看，Go 引入 goroutine 之后并发编程的难度极大降低了。

------------------------------

84. select 是如何选择分支的？, page url: https://www.mianshi.icu/question/detail?id=543
84-543-答案：
基础题。要点是回答出来 case 是随机挑选，而后再进一步解释为什么要做成随机的。不过要想刷好亮点，就可以深入讨论 case 的时机问题。
具体来说，select 的执行步骤分成五步：
case 这种随机性是故意设计的，因为从使用 select 的效果上来说，你使用 select 就不应该对顺序有任何的假设。所以 Go 本身通过这种随机性来强制你不能依赖于 case 的顺序。
还有一个理解的难点在于“同时”。实际上在程序里面是没有严格意义上的同时的，总有一个先后顺序。那么这里的同时其实是指当你执行到 select 的时候，有很多的 case 这个时刻是已经就绪的。

------------------------------

85. 为什么 Go 要引入协程？, page url: https://www.mianshi.icu/question/detail?id=544
85-544-答案：
基础题。和别的语言比起来，Go 的一个优势就是直接在语法层面上支持了协程。而且在实践中，你也会经常开启 goroutine 来异步执行一些代码。要想回答好这个问题，最基本的回答就是和线程比，但是要想刷出亮点，就得深入讨论 Go 引入协程之后在一些编程模型上和别的语言的区别。对于应届生来说，这个问题应该会更加容易被问到。
最基本的为什么要有协程，这个属于操作系统的基本知识，这里不做赘述。

这里主要讨论一下引入了协程之后 Go 在解决一些问题上和别的语言的差异。

首先是因为协程过于廉价，所以在 Go 中很少会使用到协程池这种东西。而在别的语言里面，因为只能操作线程，而线程又是珍贵的资源，所以需要使用线程池。
一句话来说，就是在现代计算机体系下，我们需要更加细粒度、廉价的调度机制。
从编程模型上来说，Go 鉴于之前别的语言的发展历史，也有意借助于 goroutine 来降低并发编程的难度。例如说在没有协程的语言中，要想并发、异步编程，就必须要操作线程。而线程本身很昂贵，为了缓解这个问题，就引入了线程池。而线程池的创建和维护，很多人都搞不动。比如说线程池里面要有几个线程，等待队列要多大，什么时候让线程池开新的线程，什么时候回收老线程，都是很复杂的问题。

------------------------------

86. Go中协程之间怎么通信？, page url: https://www.mianshi.icu/question/detail?id=545
86-545-答案：
基础题，侧重于实践的问题。这个问题实际上考察的是你在实践中怎么协调协程，包括怎么传递数据，怎么达成同步，怎么发送信号。所以你可以尽可能罗列你知道的方式，引导面试官后续提问。但是最最基础的，channel 你还是需要回答出来的。并且，这个问题经常是用来开启 Go 并发编程这个面试主题的，所以你要有一个心理准备。
协程之间的通信方式，最典型的就是 channel。这和别的语言有很大不同，大部分语言里面提供了各种并发工具，并且是优先使用并发工具来达成并发安全的目的。

面试官之所以会问这个问题，可能也是想问到另外一个 Go 里面有名的箴言：Don’t communicate by sharing memory, share memory by communicating。
简单来说，Go 内部协程之间进行通信的方式有两种：
虽然 Go 官方推荐优先使用 channel，但是在实践中用好 channel 也很不容易。

比如说如果 channel 使用不当，就有可能引起死锁，或者 goroutine 泄露的问题。
不过，说到底，channel 的本质就是一个并发安全的队列。或者说功能有限的并发安全队列，这可以从 channel 的实现源码上看出来，它和 JAVA 那边的并发安全队列没太大区别。

------------------------------

87. Go的内存对⻬规则是什么？, page url: https://www.mianshi.icu/question/detail?id=546
87-546-答案：
看上去难，实际上并不难的题，主打的就是一个死记硬背。在面试的时候要想刷出亮点，除了要把 Go 的内存对齐规则讲清楚，还要考虑进一步延伸到不同语言的内存对齐原则，并且给出利用这种对齐规则的具体案例。另外，要小心面试官手写一个结构体让你计算这个结构体在内存中的长度。
几乎在所有的语言里面，都有类似的内存对齐规则。

之所以需要内存对齐，是因为 CPU 访问内存时，并不能任意访问，而是以字长为单位而访问的。

例如说 32 位的 CPU，也就是4个字节的字长，那么每次 CPU 访问一次内存，就是读写4个字节。而问题的关键在于，编程语言中，不同类型的字段、变量的长度是不一样，因此需要“对齐”。
简单来说，Go 的内存规则是：整数起点，能拼则拼，倍数总长；

整数起点是指，一个字段的起始地址，一定是这个字段的长度的整数倍；
能拼则拼是指，多个字段有可能放在同一个字长内部，但是不能违反上一条规则；
大部分语言都会有这种规则，不过具体的规则又有细微的不同。
与内存对齐类似的一个概念是 CPU 高速缓存对齐。CPU 高速缓存对齐一般是为了独占一行 CPU 高速缓存对齐。正常来说这个技巧在并发结构中比较常见。

------------------------------

88. Go 中 defer 执行顺序是什么？, page url: https://www.mianshi.icu/question/detail?id=548
88-548-答案：
基础题，实践题。你在实践中至少要知道 defer 的执行顺序，不然运行结果可能就不如你的预期了。在回答的时候，要记得强调一个点，即便是 panic 也不会影响 defer。
defer 执行起来类似于栈，执行起来是先进后出。也就是说，先定义的 defer 后执行，后定义的 defer 先执行。
在使用 defer 的时候一定要小心 defer 的求职时机。

------------------------------

89. Go 中 defer 使用闭包的时候，是如何引用外面的变量的？, page url: https://www.mianshi.icu/question/detail?id=549
89-549-答案：
稍微有点难的题，对于很多人来说很难理解它的行为，记住一句话就是，defer 对修改是可见的。
为了让你记住这个面试题的答案，这里有一段代码，你可以自己运行一下：
在 defer 结合闭包使用的时候，如果闭包引用了外面的变量，那么这个变量的值，只有在闭包执行的那一刻才会真的读过来。

------------------------------

90. 在微服务中，怎么避免请求被发送到一个有问题的服务节点上？, page url: https://www.mianshi.icu/question/detail?id=550
90-550-答案：
有点难度的题，兼具理论和实践的意义。这道题本质上就是考察容错中的一个措施：选择合适的节点。它和负载均衡这个话题联系很紧密，也和微服务路由机制有关。正常来说你可以在负载均衡那边将话题引导过来。

这个话题如果要是回答得很好，能够让你赢得竞争优势，并且能让面试官相信你的在微服务方面理论扎实，实践丰富。
正常来说，一个微服务会部署很多节点。这里假设我们有一个用户（User）服务，那它部署了 A，B，C 三个节点，而后又有一个会员（VIP）服务需要调用它。那么这个问题就是相当于问你，如果这三个节点中 A 出现了问题，会员服务怎么避免把调用到 A。

你显然能够意识到，这个东西和负载均衡是很像的，而负载均衡本质上也就是回答：“谁是处理这个请求的最合适的节点？”。那么避开不合适的节点，自然就是应该要考虑的事情了。当然，微服务的路由机制也是筛选节点的机制，只是叫法不一样。

回到前面的例子上来，回答这个问题，关键在于两点：
避免将请求发送到有问题的节点上，核心要做到两点：

第一点本质上也就是判定节点的健康状态。常见的方式是根据的响应情况来判定，比如说根据响应是否超时，是否连续超时，响应时间是否大幅增加。当然比较罕见的做法就是
更加进一步来说，也不仅仅是微服务里面需要考虑这种问题，实际上在分布式环境下和对等节点通信都需要考虑这种问题。

------------------------------

91. 为什么要分库分表？, page url: https://www.mianshi.icu/question/detail?id=551
91-551-答案：
略难的题。有很多人经常犯的错就是解释说数据量大了就要分库分表，但是这个解释不够深入。并且，这是一个很好的打开的分库分表话题，或者说引导面试官的机会，所以你要尽可能在回答的时候提到尽可能多的点，方便接下来面试官追问。

有些时候，面试官问这个问题，可能也是在质疑你为什么要分库分表，其它的手段如分区表、加从库能不能解决问题。
总体来说，不管怎么解释，会考虑分库分表就只有一种情况：遇到了读写瓶颈。

接下来我们分别分析一下读写瓶颈。

大多数时候读瓶颈是可以通过的分区、加从库来解决的。只有在数据量非常大的时候，这种情况下分区表或者加从库可能效果不太好。比如说数据量大了之后，B+ 树的高度变高了，索引过于庞大以至于没法装进去内存中等。读瓶颈可以通过分表、分库或者同时分库分表来解决。
一般来说只有在遇到了读写瓶颈的时候，常规 SQL 优化也没效果的时候，就要考虑是不是要执行分库分表了。
写瓶颈不同于读瓶颈，如果只是单纯的分表，效果不能说没有，只能说很差。因为这些表都还在同一个库上，还是共享各种硬件资源。因此要想效果好，就要分库，并且在分库的基础上进一步将不同库分散到不同机器上。
严格来说，分库分表的说法还是有点含糊。真正的说法应该是分区分表分库分实例（或者分集群），也就是先考虑一张表分区，再考虑分表，再考虑分库，最终考虑分实例。比如说正常一个公司出来讲自己的分库分表方案，一定会提及自己用了多少个数据库主从集群。

------------------------------

92. 为什么你不使用分区表，而是使用分库分表？, page url: https://www.mianshi.icu/question/detail?id=552
92-552-答案：
略难的题，兼具理论和实践的意义。当数据库遇到读写瓶颈的时候，有很多种解决方案，分区表和分库分表就是两种比较常用并且效果也比较好的方案。你在回答这个问题的时候，要注意结合实践来讨论。
这个问题几乎可以等价为为什么使用分库分表。只不过在这个问题下，限定了只和分区表进行比较。

简单来说：分区表和分库分表都能解决读写瓶颈，但是分库分表在大数据场景、写瓶颈下效果会更好。

你可以设想，一张表在分区之后，每个分区的数据量都减少了，因此读写效率都能提高，也就是解决了读写瓶颈，或者说至少极大的缓解了读写瓶颈问题。
分区表还是有自身的局限性的。

从理论上来说，分区表也能解决单表数据大、读写瓶颈的问题。但是分区表有两个弱点：

------------------------------

93. 在分库分表的时候，怎么解决主键问题？, page url: https://www.mianshi.icu/question/detail?id=553
93-553-答案：
略难的经典题目，兼具理论和现实意义。大部分公司在没有分库分表的时候，都是推荐使用自增主键的。因此在切换到分库分表之后，自增主键就不能用了，这时候就需要有额外的解决方案。在这里我给你一个比较高级的方案，可以确保你赢得竞争优势。
常见的主键解决方案有：

这三种算法也可以看做是全局唯一 ID 生成的解决方案。
在分库分表里面，常见的主键解决方案有：uuid，控制步长的自增主键，雪花算法。

uuid 的优缺点都很明显，优点是极其简单，不依赖于任何东西。缺点则是 uuid 不是自增的，并且长度比较长，需要比较多的存储空间。
雪花算法的精髓在于，可以根据自己的业务需要灵活设定每一个段。一个典型的例子就是某外卖平台用用户 ID 的后四位来分库分表，于是在订单的主键生成算法里面就用用户的后四位取代了机器序列号，而后在序列号部分则是随机生成。


显然这么做之后，拿到了订单 ID 就能直接推断出来数据在哪个库哪张表里面。
在高并发的时候，也要考虑进一步优化生成 ID 的性能。有一些比较常见的优化方案：

------------------------------

94. 什么是雪花算法（snowflake）？, page url: https://www.mianshi.icu/question/detail?id=554
94-554-答案：
基础题，理论意义大于实践意义。一般来说这个题目只会出现在和分库分表有关的场景中，要想回答好这个问题，就要在讲清楚雪花算法原理的基础上，讨论清楚怎么灵活控制雪花算法中的段，以及怎么解决雪花算法应用不当带来的一些潜在问题。

最后还可以通过讨论怎么优化性能来赢得巨大的优势。
雪花算法的精髓就是分段。
雪花算法常用在分库分表里面生成主键。

具体来说，雪花算法是通过将一个 64 比特的数字进行分段，每一段有不同的含义：
雪花算法的精髓在于，可以根据自己的业务需要灵活设定每一个段。一个典型的例子就是某外卖平台用用户 ID 的后四位来分库分表，于是在订单的主键生成算法里面就用用户的后四位取代了机器序列号，而后在序列号部分则是随机生成。


显然这么做之后，拿到了订单 ID 就能直接推断出来数据在哪个库哪张表里面。
在将雪花算法集群独立部署，业务方通过 RPC 服务或者 HTTP 服务获取 ID 的时候，有一些额外的优化手段：

------------------------------

95. 分库分表怎么确定该分多少库多少表？, page url: https://www.mianshi.icu/question/detail?id=555
95-555-答案：
略难的题目，兼具理论和实践意义。这个问题的本质就是容量评估，而所有的容量评估都是有固定套路的，这个套路就是已有数据量 + 预期数据量 + 读写瓶颈三者合一。而难点则在于怎么评估预期数据量。
所有的容量预估题目都是固定套路：
分多少库和分多少表主要考虑三个点：
已有数据量是很简单的，只需要统计一下就可以。所以难点就在于如何确定预期数据量和读写瓶颈。

正常来说，在评估预期数据量的时候，会先确定评估的时间长度，比如说评估未来五年的数据量。至于是三年五年还是十年，都是可以的。在确定了时间长度之后，要考虑的就是这段时间究竟能够产生多少数据。
在分库分表里面还要额外考虑一个内容，就是分多少个主从集群，每个主从集群上放多少个库。这个本质上也是根据读写瓶颈来确定的。

------------------------------

96. 问题模板：为什么你用 A（而不用 B）？, page url: https://www.mianshi.icu/question/detail?id=556
96-556-答案：
典中典题目，几乎每次面试都会碰到，基本回答思路就是总结 A 的优点、缺点，结合具体的业务场景得出 A 更加适合的结论。
这是很大一类的题目， 面试官的问法大体上可以分成这三类：

举例来说：

大部分情况下，这个决策并不是你做出来的。比如说你进去公司的时候，公司就已经用 Kafka 了，这个时候面试官问你你为什么用 Kafka，你可能觉得莫名其妙，因为你不是决策的那个人。这种时候，如果你回答“我进去的时候公司就已经用 Kafka 了” ，是一个比较掉价的事情。

------------------------------

97. 为什么要在数据库上使用读写分离？, page url: https://www.mianshi.icu/question/detail?id=557
97-557-答案：
简单题。大部分公司只要是稍微上了点规模，都会考虑引入数据库主从集群，而后在业务查询的时候，就执行读写分离了。

要想在这个问题下刷好亮点，有两个点：一个是讨论主从延迟；而另外一个思路则是需要讨论如何强制走主库、强制走从库的问题。
用一句话来回答为什么要读写分析，那就是为了性能。
读写分离是为了减轻主库压力，提高性能。

正常来说，在没有使用读写分离的时候，所有的读写请求都是落在主库上的，那么主库的压力就会非常大，进一步影响响应时间。
在使用读写分离的时候，要注意一个问题：主从延迟的问题。也就是说，主库的数据同步到从库上，是需要时间的。如果一个数据被更新了，还没同步到从库上，那么你此时查询从库读取到的就是老的数据。在这种情况下，就是需要强制指定读请求走主库了。
我之前在做数据库迁移，校验数据的时候，就搞了点小花招。理论上来说，数据校验这个东西只能是通过主库来校验的，毕竟主库和从库之间的数据是存在延迟的。

------------------------------

98. 分库分表中，怎么解决分页查询问题？, page url: https://www.mianshi.icu/question/detail?id=558
98-558-答案：
略难的题。这个问题是指如果要是一个分页查询可能命中很多张表，应该怎么解决。解决思路有很多，不过每一种无损的解决思路都稍微有点复杂。所以你在准备面试的时候，要多背背。

在这个题目下，你只要能够把常见的解决方案说清楚，就已经赢得竞争优势了。

你要注意和优化分库分表中的分页查询这个问题相区别。
分页查询，常见的思路有：

当然，实在不行还可以换用别的中间件。


全局方案也是大部分分库分表中间件使用的方案。例如说，当搜索的是 SELECT * FROM users LIMIT X, Y。
简单来说，常见的思路有四种：
我之前就优化过一个分库分表查询。

其实问题也很简单，就是用户在查询数据的时候，反馈说翻页越往后越慢。而我们测试的时候，因为测试环境里面根本没有那么多数据，所以就没发现这个问题。

我去排查的时候很容易就看出来了，在查询数据的时候，使用的是 LIMIT X，Y 这种写法，那么在往后翻页的时候，每次从数据库里面拿出来的数据就是 LIMIT 0, X + Y。
我个人认为，第一种方案和第二种方案，其实没有明显的优劣对比。甚至于可以说，在利用归并排序优化性能之后，大多数时候第一种方案比第二种方案还要好。比如说在 X 还小的时候，第一种方案少了一次查询，反而更快。但是在 X 很大的时候，那么第二种方案就占优了。而且，二次查询这个方案会方法数据库查询的流量，增加数据库的压力。

------------------------------

99. MySQL 使用自增主键有什么优点？, page url: https://www.mianshi.icu/question/detail?id=559
99-559-答案：
简单题。基本上在数据库内容的面试中，这是一个必问题目。换一句话来说，绝大部分人都能回答出来这个问题，所以你更加要想办法刷出亮点，引导面试官追问。

而要想回答好这个问题也很简单，就是要点出页和页分裂的问题。并进一步从页和页分裂的角度总结自增主键的优点，而后要尝试将话题引入到分库分本表中。
一句话解释自增主键，就是为了提高性能。自增主键从两方面提高性能：
一般来说，只有在没有进行分库分表的时候，我们才会选择自增主键。在分库分表的时候，自增主键就不太好用了。

------------------------------

100. 云原生中的 sidecar 是指什么？, page url: https://www.mianshi.icu/question/detail?id=560
100-560-答案：
基础概念题，随着云原生这个概念火爆，在整个面试过程中，你还是有比较大的概率遇到这个问题。

要回答好这个问题，你不仅仅需要解释 sidecar 这个概念、优缺点，你还要举例子 sidecar 的使用场景，最好是能够加上自己的理解。
sidecar 从字面意思上来解释，应该叫做“边车”，当然这个翻译有点不伦不类了。

我觉得最佳的解释，应该是“僚机”。就好比，你的应用是战斗机，但是战斗机不能什么事情都这么做，于是就会有僚机来分担一些职。

另外还有一个有争议的说法，你可以考虑要不要使用。
sidecar 是一个云原生的概念，是指将核心功能（一般是指业务）和非核心功能（支撑业务的功能）分离的模式，并且一般来说 sidecar 本身也是一个独立部署的进程。
而目前来说，在这两年的降本增效里面，也有一些 no-sidecar 的趋势。

------------------------------

101. 怎么在读写分离中，强制 SELECT 使用主库？, page url: https://www.mianshi.icu/question/detail?id=561
101-561-答案：
基础题。如果你使用过任何支持读写分离的中间件，你就知道它们都会提供接口给你指定走 SELECT 语句走主库。

回答这个问题最好就是在指出解决方案之后，总结一些自己的态度，就是 SELECT 走主库不是一个很好的实践，要小心使用。
正常来说，很少有场景会要求 SELECT 走主库。毕竟我们读写分离的目的就是为了让 SELECT 语句走从库，从而降低主库的性能压力。
一般来说，支持读写分离的中间件都会提供接口来指定某个查询走主库。
但是不管怎么说，SELECT 走主库这种用法还是要尽量避免的。

因为最开始读写分离就是为了降低主库的性能压力。而如果还是要让主库来执行 SELECT 语句，就是放弃了这最大的优势。

从实践上来说，通过良好设计，或者在应用层面上缓存数据是能够避免 SELECT 走主库的。
此外，我之前在做数据迁移的时候，还搞过一个优化。

按照道理来说，数据校验就是应该用主库的数据来校验的，因为从库有主从延迟所以可能不准。

------------------------------

102. 你了解 GMP 调度吗？, page url: https://www.mianshi.icu/question/detail?id=562
102-562-答案：
略难的题，纯理论题，你了不了解 GMP 调度都不影响你写出来牛逼的代码。这个题目之所以难，纯粹是因为要背诵的内容比较多，很多人会觉得全部背下来有点困难。

要回答好这个问题，能够将原理解释得很清晰，就可以赢得竞争优势了。如果你还能深入讨论 GMP 为什么设计成这个样子，就能在面试中赢得巨大的竞争优势了。
这边我教你一个从设计者的角度去回答这个问题。首先，GMP 是指：

现在我们站在一个设计者的角度，我们已经决定要使用 goroutine 了，而在操作系统上它只认线程，根本不认我们搞出来的 goroutine。

那么现在我要调度 goroutine，就需要让 goroutine 绑定一个线程。或者说，找一个线程来执行 goroutine。那么考虑到我们的 goroutine 有很多，而线程数量又不能无限多，因此最简单的做法就是搞一个线程池之类的东西，而后再搞一个队列放 goroutine，每个线程不断从队列里面找一个 goriutine 来执行。
GMP 调度是 GO 运行时调度 goroutine 运行的机制。简单来说：

那么 GMP 调度，就是指 Go 运行时会将 G 分配给 P 绑定的 M 来执行。

整个过程比较复杂。
GMP 的这种全局队列 + 局部队列的思路还是很常见的。
我在 XXX 项目里面还用过这个思路来优化分布式锁的竞争。

------------------------------

103. 在 GMP 调度中，P 是怎么获取 G 的？, page url: https://www.mianshi.icu/question/detail?id=563
103-563-答案：
略难的题，可以看做是 GMP 调度下的一个子问题。

回答这个问题要求你深刻理解全局队列 + 局部队列 + 工作窃取这三点。同样刷亮点，也是通过举例子说明这种思路在实践中很常见。
你可以在GMP调度这个问题下找到详细的分析。
简单来说，就是先从 P 的队列里面去获取 G，如果 P 的队列里面都没有 G，就从全局队列里面去获取；如果全局队列里面也没有 G，那么就从别的 P 的队列里面窃取。
之所以这么设计，其实还是为了性能。
我在 XXX 项目里面还用过这个思路来优化分布式锁的竞争。

------------------------------

104. Go 为什么使用 GMP 调度？ GM 行不行？ GP 行不行？, page url: https://www.mianshi.icu/question/detail?id=564
104-564-答案：
略难的题。你如果机械背诵八股文，那么遇到这种问题就可能没有思路。

其实这种题目回答起来都是差不多的套路，具体到这里就是根据 GMP 调度原理总结优缺点，而后得出结论 GMP 就是行，GM 就是不行，GP 也是不行。
详细的 GMP 调度你可以从GMP调度找到。
从目前来看，GMP 是一个非常好的解决方案了。

首先可以明确的是 G 代表的是 goroutine 肯定是不能少的。而线程才是操作系统的 CPU 调度单位，因此代表了线程的 M 也是不可少的。这也就是解释了为什么最开始的时候 Go 用的是 GM 调度。

------------------------------

105. Go 为什么最开始用 GM 调度，但是后面引入了 P，变成 GMP 调度？, page url: https://www.mianshi.icu/question/detail?id=565
105-565-答案：
略难的题，不过依旧是基于 GMP 调度原理衍生出来的问题。
在GMP 调度里面我详细解释了这个问题。

那么这里可以稍微简化一下逻辑链条，方便你理解。

GM 调度有一个极大的缺陷，就是全局队列，它就是性能瓶颈，因为所有的 M 去抢 G 都需要加锁，而且是同一把锁。
简单来说，Go 要引入 P 就是为了提高性能。

------------------------------

106. 为什么说引入 P 之后充分利用了局部性原理？, page url: https://www.mianshi.icu/question/detail?id=566
106-566-答案：
略难的题，同样是 GMP 调度衍生出来的问题。

在回答的时候，要想刷出亮点，就可以深入讨论局部性原理对程序设计的影响，并且举几个例子。当然了，这个问题的答案你也可以用在回答 GMP 调度相关问题中，刷一个亮点。
有关 GMP 调度的内容请参考题目你了解 GMP 调度吗？ (meoying.com)。要回答这个问题，你还是要从引入了局部队列这个角度去回答。

我们强调过，引入局部队列本身最主要是为了解决全局队列的锁竞争问题。但是它也带来其它的好处：
引入了 P 之后，总体上是充分利用了时间局部性和空间局部性。
除了在 GMP 调度这里，在计算机里面，局部性原理在很多地方都有体现，核心都是为了提高性能。

------------------------------

107. Go 是怎么管理内存的？, page url: https://www.mianshi.icu/question/detail?id=567
107-567-答案：
基础题。一般来说，面试官问这个问题，指向的是垃圾回收，也就是他真正想问的是垃圾回收的问题。不过他问法不够准确，因为内存管理包含两方面的内容：正向的内存分配和反向的垃圾回收。

因此你在回答的时候要既要提及内存分配，也要提及垃圾回收。并且因为这个问题问的内容非常宽泛，所以你可以在回答中尽可能提及非常多的技术点、词汇，从而引导面试官接下来的追问。
这个问题因为问得宽泛，所以你回答也可以宽泛，而后等面试官来追问。

所以你的回答就把最关键的点说出来，形成引导：
Go 的内存管理总体上来说分成两方面：
栈分配是一个非常高效的分配方式，所以我们会优先考虑让对象分配到栈上，而如果一个对象没有被分配到栈上，那么就是内存逃逸。

------------------------------

108. 技巧：能而示之曰不能, page url: https://www.mianshi.icu/question/detail?id=568
108-568-答案：
大部分面试官有一个臭毛病，就是他们喜欢追问你不会的，而不喜欢追问你看上去就很懂的知识。准确来说，他们特别喜欢追问你看上去会一点，但是又会得不多的内容。一方面可以试探出你的技术水平究竟有多高，另外一方面当你回答不出来的时候，会给他们带来巨大的心理愉悦感和成就感。

所以这个技巧的核心就是让你在回答问题的时候，可以适当漏出破绽，以引导面试官继续追问。
能而示之曰不能，要点就是你要让面试官觉得有一个点你好像知道，但是只知道一点点。

要达成这个目标，首先是从表情上来说，你需要露出适当的心虚的表情。比如说眼神突然看向别的地方，飘忽不定；或者说话的时候语调突然发生变化，显得中气不足那样。

我有一个简单的技巧，你只要在回答的时候想想你某次撒谎被父母吊着打的经历，脸上估摸着就会带上不自然、心虚的表情。

------------------------------

109. 在 GMP 调度里面，发起一个系统调用之后会发生什么？, page url: https://www.mianshi.icu/question/detail?id=569
109-569-答案：
略难的题，也是基于 GMP 调度衍生的题目。这个问题的核心并不是真的讨论系统调用的流程，而是讨论如果系统调用阻塞了 M，GMP 调度会怎么办。

而这一句话就能说清楚，M 和 P 解绑，完成之后再绑定，重新调度。
请先查看你了解 GMP 调度吗？ (meoying.com)

在理解了 GMP 调度之后，很容易就想到 M 既然是代表了操作系统上的线程，那么在发起系统调用的时候，并且 M 会被阻塞的时候，从设计的角度来说，最佳策略就是让 M 和 P 解绑，从而让 P 可以去执行别的 G，高效利用系统资源。一旦 M 从阻塞中恢复过来，那么 M 会重新尝试和一个 P 绑定，继续执行后续的代码。


那么总结起来就是：
首先 GMP 会把 M 标记为系统调用中，而后根据系统会不会引起阻塞分成两种情况：

------------------------------

110. 如果要你来优化 GMP，你会怎么优化？, page url: https://www.mianshi.icu/question/detail?id=570
110-570-答案：
可以简单，也可以难的题。如果你面试的岗位很高端，那么可能面试官是真心想和你探讨；但是如果你面试的岗位不是很高端，那么大概率他就是随口一问。在后一种情况下，你差不多随便回答一点就可以。
如果你有自己的思考，那么你可以用自己的。如果你没有的话，就随便使用我下面的这个，记住在这种开放性的问题之下，只要你言之有理就可以。
我个人感觉可以考虑将 P 搞成动态的。也就是说在系统负载高的时候，减少 P 的数量；在系统负载低的时候，增加 P 的数量。P 的数量即使增长，也不会超过用户设置的最大数字。

------------------------------

111. 识别 PUA：你这垃圾系统，犯得着用这种复杂的方案吗？, page url: https://www.mianshi.icu/question/detail?id=571
111-571-答案：
纯 PUA + 秀优越的说法。其实如果正常面试官问你“这系统需要使用这么复杂的方案吗？”，你最多就是觉得有点不好回答。但是有些面试官就是嘴臭，或者说就是纯粹看你不顺眼，或者觉得你面试过程中表现得对自己的方案过分自信了，他不爽就会这么问。

你在回答的时候，要千万记得自己是来求职的，不是来和 SB 一般见识的，所以可以适当解释一下。
这一类问题，一般是当你聊项目，聊完你的解决方案之后，面试官质疑你的方案选型。

那么面试官为什么会质疑呢？有两种原因：

总之非常关键的点就是，人在屋檐下不得不低头。当然了，如果要是你对这家公司也不是特别上心、特别想去，那么直接嘲讽或者对喷都是可以。

------------------------------

112. 识别 PUA：你这学历，进来我们公司是很勉强的, page url: https://www.mianshi.icu/question/detail?id=572
112-572-答案：
学历歧视，而且还想杀价。

记住核心要点的就是，如果介意你的学历，你简历筛选都过不了；如果开始面试了，尤其是到 HR 谈薪资了，那么提起这个话题就是为了杀价。反正你不要管这个 PUA，该要多少钱就要多少钱。
可能你想不到的是，即便是 985，211 的也会被歧视学历。比如说常见的话术：

所以学历被歧视，并不是说因为你的学历真的很低，而是他们总要找些理由来否定你。有些人 PUA 你纯粹是为了自己的心理优越感，有些就是为了压价了。

最典型的场景就是 HR 面，或者在谈薪资的时候巴拉啦一大堆之后来一句：

------------------------------

113. 一个 G 被 P 调度之后，会一直运行直到结束吗？, page url: https://www.mianshi.icu/question/detail?id=573
113-573-答案：
略难的题。如果你第一次遇到这个题目，你可能都不知道面试官是什么意思。

实际上，这个问题问的是如果一个 G 需要很长时间来运行，比如说一个无限循环，那么在它被调度到一个 P 之后，是不是它永远不会的让出这个 P，直到自己运行结束？

你大概率能够想到系统调用和等待锁两种情况会导致 G 让出 P，但是实际上还有别的东西，并且可以进一步扩展以引申到别的方面，刷出亮点，赢得竞争优势。
很显然，这个问题的答案是否定的。

最容易想到的两个理由就是：

此外，还有两个条件，这两个条件是 Go 运行时的机制，你比较难想到：
以下这些情况，G  都会被暂停：
所以，目前来说我们并不需要担忧类似于无限循环导致 G 一直占用 P，导致 P 没有办法调度别的代码的问题。
时间片机制，或者说检查点机制其实是一种很通用的设计手法。

比如说现代的 CPU 调度差不多也是这种搞法。在别的语言里面，涉及到调度方面的内容，也差不多是这种，比如说 JAVA 的线程调度。

我之前在设计一个分布式任务调度平台的时候，借鉴过类似的思路。当时我要解决的问题是，如果有一些任务会运行很长的时间，比如说循环处理一千万条数据这种。

------------------------------

114. Go 是怎么分配内存的？, page url: https://www.mianshi.icu/question/detail?id=574
114-574-答案：
略难的题，因为要背的东西稍微有点多。为了防止面试官故意找茬，你在回答的时候要同时提及栈分配和堆分配，并且要详细解释堆分配的过程。

在这个问题下有一个极好的刷亮点方式，就是和 Linux 的 SLAB 进行对比，又或者和你了解的其它任何的内存分配原理进行对比。

如果你是应届生，那么你也可以进一步将这个内存分配过程和你学习的内存管理知识结合起来。
这个问题主要问的是堆上面的内存分配。但是为了防止有些面试官故意 PUA，比如说如果你只回答堆分配，他就会反问你连栈分配都不知道这种情况出现，你就要提及栈分配。

为了理解这个问题，建议你先阅读 GMP 调度的内容：你了解 GMP 调度吗？ (meoying.com)。

Go 堆上分配的过程还是有点小复杂的。具体来说它有三个核心结构：
（最顶层的分类）Go 的内存分配分成了栈上分配和堆上分配两种。

栈上分配也就是分配到 goroutine 的栈上，这个过程是通过编译器在编译期间计算好如何分配的。

但是如果在编译期间，编译器发现没有办法分配到栈上，那么就会分配到堆上，这也就是所谓的内存逃逸。这个时候，运行时要根据对象大小来判定执行哪一种分配：
Go 的这种内存方式和 Linux 的 SLAB 分配很像，比如说从设计思路上来说，两者都是分层结构，而且都是局部缓存-全局缓存这种结构。
我也用过这种思路来设计过 buffer pool，从而优化性能。

简单来说，就是如果线程需要一个 buffer 的时候，并不是只拿够当下使用的。而是拿一块比较大的 buffer，而后线程内部自己切割这个 buffer，每次使用的时候只使用其中一小块。等整个 buffer 全部用完的时候，再去 buffer pool 里面取一块新的。

反过来，如果整个 buffer 都没有再被使用了，就归还给 buffer pool。

------------------------------

115. 如果我有一个极小对象，那么 Go 会把内存分配到哪里呢？, page url: https://www.mianshi.icu/question/detail?id=575
115-575-答案：
简单的，带陷阱的题。

如果要是面试官水平一般，他可能就是问你极小对象分配器。如果是高端面试官，他可能就是在这个题目里面设置了陷阱。但是不管怎样，你都要假设对面的面试官是一个高水平的面试官，防止翻车和 PUA。

我其实很难理解就是一些面试官在这种纯理论题下设置陷阱的心态，因为你又不是高考，搞这种陷阱就相当于“茴香豆的茴字你会几种写法”，没有任何意义。
这个题目的陷阱就在于极小对象只是说对象的大小是小于 16B 的。
有三种情况：

------------------------------

116. 在 Go 的内存分配里面，为什么要使用 mcache？, page url: https://www.mianshi.icu/question/detail?id=576
116-576-答案：
略难的题。难点就是在于你要识别出来这个问题本质上就是问你 mcache 的优点，你就千万不要回答你不是设计者，你也不知道为什么用 mcache。

回答的时候，你要进一步和 P，以及 GMP 调度中的局部队列关联在一起，刷亮点的同时，还有可能将话题引导到 GMP 调度上。
你可以在Go 是怎么分配内存的？ (meoying.com)里面找到详细的内存分配过程分析。
mcache 主要是为了提高内存分配的速率。
mcache 的这种设计思路和 GMP 调度的局部队列是一样的。都是通过引入局部数据结构，来降低全局竞争。
我也用过类似的思路来解决问题。

------------------------------

117. 在 Go 里面，为什么大对象会直接从 mheap 里面分配呢？, page url: https://www.mianshi.icu/question/detail?id=577
117-577-答案：
简单题。唯一的难点就是你可能不知道什么是大对象，它一般是指 32KB 以上的对象。
大对象太大了，如果要是经过的 mcache-mcentral 这一套来分配内存，那么性能很差，而且还会影响到小对象的分配。

------------------------------

118. 在 Go 里面分配内存的时候，为什么要专门对极小对象进行特殊处理？, page url: https://www.mianshi.icu/question/detail?id=578
118-578-答案：
略难的题。这个题目有两个点，第一个点是看你知不知道 Go 内存分配一堆极小对象做了什么特殊处理；第二个点是看你怎么理解这种特殊处理。

你在回答的时候，可以引用 Go 文档中的解释，从而证明你是一个看过源码的人，刷出亮点。
Go 针对极小对象有一个专门的分配器，一般也叫做极小分配器。
这种考虑是为了在性能和内存利用率之间取得平衡。

在 Go 程序里面，有大量的小字符串等极小对象，如果要是按照现有的 mcache-mcentral-mheap 这条路来分配，那么现在最小一级的 span 是切割成 32B 一个对象的。

那么一个 8B 的小对象，也必须要给它 32B，那么就会浪费 24B，这显然是不可接受的。
在这里，选用 16B 作为最小分配单位，也不是随便选的。在 Go 源码的解释里面：

------------------------------

119. Go 的极小分配器（tiny allocator）是如何分配内存的？, page url: https://www.mianshi.icu/question/detail?id=579
119-579-答案：
难的题，难就难在有很多细节。不过大部分情况下，面试官也不懂，尤其是那些搞业务开发的，没看过源码也说不出个所以然来。

因此你只要回答出来足够的细节了，尤其是怎么和 mspan，mcache，mcentral，mheap 结合起来，就足够刷出亮点了。
首先你需要阅读Go 是怎么分配内存的？ (meoying.com)并深入理解正常的内存分配流程是如何运作的。

而后，现在我们这里补充一下极小分配器的细节了。

首先声明的一点是，极小分配器一样用的是 span，并且 span 也是按照 mcache-mcentral-mheap 来获取的。但是极小分配器用的是 16B 一级的 span，作为对比，小对象分配用的是 32B ~ 32KB 这些级别的 span。
极小分配器使用了一种比较特殊的策略来进行分配。

首先极小分配器获取 span 的过程和小对象的差不多，都是经过 mcache-mcentral-mheap 来获得的。

------------------------------

120. Go 是如何执行垃圾回收的？, page url: https://www.mianshi.icu/question/detail?id=580
120-580-答案：
略难的题。对于刚接触垃圾回收的人来说，要背的东西还是有点多的。但是对于 JAVA 那边转语言过来的人来说，这点子内容还是很简单的。

回答这个问题的时候，只需要尽可能把知道的关键技术点说出来就好，但是不需要把所有的细节都说完，等着面试官追问。

如果你之前已经了解过别的垃圾回收算法、实现，那么就可以进行对比，刷出亮点。
一句话就可以说清楚 Go 使用的垃圾回收算法：并发-标记-清扫（Concurrent-Mark-Sweep）算法，很类似于 JAVA 的 CMS 算法。

建议你先看 Go 正向的内存分配内容：Go 是怎么分配内存的？ (meoying.com)。

整个垃圾税收可以看成是四步：
Go 的垃圾回收用的是并发标记清扫算法，整个过程可以分成四步：
在整个过程中，也可以从内存管理的角度去看。标记阶段，是使用三色标记法沿着对象之间的引用链进行下去的；而清扫阶段，则是遍历所有的 span。
和别的语言中的垃圾回收比起来，Go 的垃圾回收还是很有特点的。

------------------------------

121. 在 Go 里面，GC root 包含哪些？, page url: https://www.mianshi.icu/question/detail?id=581
121-581-答案：
比较简单的题。

要在这里刷亮点，非常关键的点就是指出寄存器里面的值，在不能确定类型的时候，只能假设这是一个指针，也要执行扫描。
在 Go 扫描的过程中，如果当前扫描的是一个指针，就需要沿着指针指向的地址继续扫描。但是这里有一个问题，即 Go 在一些情况下是不知道这个东西是不是指针的。

典型的场景就是寄存器里面值。如图，当寄存器里面的值是 0xAB 的时候，它有可能是一个地址，指向 Go 堆里面的一个对象；它也有可能就是数字 0xAB。而为了防止 GC 出错，只能是保守当做指针。


Go 里面的 GC Root 主要包含三个部分：
这里要额外讨论的是寄存器对象。Go 在执行 GC root 扫描的时候，其实无法判定寄存器里面存的是一个值，亦或是一个指针。

------------------------------

122. Go 使用的三色标记法是如何运行的？, page url: https://www.mianshi.icu/question/detail?id=582
122-582-答案：
略难的题。这个题目，你把最基础的三色标记法答出来也算过关，你进一步回答强弱三色不变性就能进一步刷亮点，当你把读写屏障就回答出来了，就差不多算是刷出来很多亮点了。

这里要记得进一步揭露读写屏障的本质，才算是功德圆满，无可挑剔。
三色标记法，其实一句话就能说清楚：开始是白色，处理中是灰色，处理后是黑色。

那么处理是指什么呢？也就是扫描它的儿子。也就是说，最开始它是白色的，如果正在扫描它的儿子，还没扫完，就是灰色的，扫描完之后就是黑色的。

过程如下图：
三色标记法的的原作原理还是比较简单的：
但是在并发标记阶段，因为业务 goroutine 也在运行，也就是对象的引用随时可能发生变化，比如说新建了对象等，又或者删除了引用等。在这种情况下，Go 就还要保证弱三色不变性。
混合写屏障难理解的点就是它的实现比较晦涩难懂。

最为直观的理解就是，编译器会在编译的时候在修改引用的地方插入安全点，并且执行检查的代码。因此明面上我们看上去修改引用就是简单的一句 a.b = c 这种写法，但是在编译器看来，这个操作就会被最终编译为：

------------------------------

123. 你们为什么要使用 Go 语言？, page url: https://www.mianshi.icu/question/detail?id=583
123-583-答案：
基础题。这个题目比较经常在跨语言的场景下问出来，例如说你们公司从某个语言转到了 Go。回答这个问题其实也很简单，就是列举 Go 的优点，如果发问的场景和跨语言有关，也就是面试官其实隐藏了希望你比较不同语言之间的优缺点，那么你就增加优缺点比较的部分。

这个问题本身也是一个宽泛的问题，所以你可以在回答的时候尝试加入一些关键引导词。
总结来说，Go 本身的优点很多，从实践上来说：

你可以根据自己的理解来回答这些优势。而后你要看这个问题问出来的时机，以及你的面试策略中想刷什么样的 Go 语言亮点。
Go 语言本身是一个很优秀的语言，有很多优点。

第一，Go 本身的语法简单，周边框架也简单，所以上手很容易。这样不管是转语言还是招聘，都比较容易；

第二，Go 是一个静态强类型语言，所以和 Python PHP 比起来更加适合大规模团队合作，以及构建复杂规模庞大的系统；
（如果要是你希望面试官问你并发编程，或者网络编程相关的内容，就补充这两段，这两个都可以认为是高级工程师才需要掌握的高级编程话题）

------------------------------

124. Go 中的切片是如何实现的？, page url: https://www.mianshi.icu/question/detail?id=584
124-584-答案：
基础题上加了一点点难度的题，在 Go 语言面试中非常常见，是一个很好的刷亮点的机会。从实践中来说，你知不知道这个东西，都可以写出很好的代码。

回答这个问题，要在回答了底层原理的基础上，重点讨论扩容的过程，以及最为关键的扩容因子的选择。
切片，可以看做是一个功能有限的 ArrayList，也就是数据结构里面讲的那个 Array List。

之所以说功能有限，是因为切片只支持 append 操作，不支持随机插入，随机删除这些常规的 Array List 都会考虑支持的操作。

所以你的总体思路就是回答 Array List 的基本原理，而后针对切片进行一些简要的分析，最后总结搞个对比，升华一下主题就可以。
切片的底层原理很简单，基本上就是一个 Array List。

切点本身维持了长度、容量和一个指向底层数组的指针。当我们用下标访问切片的数据的时候，其实访问的就是底层数组的数据。
整个切片实现中，最为关键的就是扩容因子的设计。从理论上来说，如果要是扩容因子太小，那么容易出现频繁扩容的问题；而如果要是扩容因子过大，则会浪费内存。

Go 的切片在这里采用了一种复合策略。在容量比较小的时候，是按照 2 倍来扩容；在容量比较大的时候，是按照 1.25 倍来扩容。这个阈值（用 2 还是 1.25 的分界线）在早期的版本里面是 1024，而在高版本里面是 256。
事实上，所有的有扩容机制的数据结构都要考虑扩容因子的问题。比如说 hashmap 这种结构，也要考虑什么时候扩容，以及扩容之后的容量该怎么确定。基本上都是要在扩容频率和内存利用率之间得到一个平衡。

------------------------------

125. Go 中的切片是值传递还是引用传递？, page url: https://www.mianshi.icu/question/detail?id=585
125-585-答案：
基础题，不太常见的问题。

之前流行的一种说法，是 Go 中的切片是引用传递。当然我个人是不认同这种说法的，在 Go 里面只有值传递一种。因此切片必然是值传递的，只不过传递的是切片本身。
在Go 中的切片是如何实现的？ (meoying.com)中说到，切片本身就是一个结构体，里面有一个指针指向了底层数组。这是它的源码定义：
Go 的切片是值传递的，只不过这个值是代表了切片本身的 slice 这个结构体。
我个人认为，在 Go 里面是只有值传递的。只不过如果是结构体，那么传递的是结构体本身；如果值是指针，那么传递的就是指针本身，而不是指针指向的数据。

------------------------------

126. Go 中的切片是如何扩容的？, page url: https://www.mianshi.icu/question/detail?id=586
126-586-答案：
简单高频题。在大部分 Go 的基础面试里面，这都是必问题目，而且在校招里面，面的会更加多。

这道题是一个极好的刷亮点的机会。一个是要解释清楚 Go 的扩容机制，尤其是扩容因子，以及平滑变化的过程；第二则是要提及在扩容机制之后扩展到其他类似机制上，并且进一步深入讨论扩容的时机、权衡。

大部分情况下，你的竞争对手最多就是回答清楚扩容的流程，而没有办法在这个基础上深入讨论，因此你可以借助这个问题给面试官留下非常深的印象。
Go 的切片的扩容原理，你可以在这里Go 中的切片是如何实现的？ (meoying.com)找到。

这里我们具体总结一下容量的计算过程。

这里要稍微再解释一下为什么扩容因子到从 2 倍到 1.25 倍需要引入一个平滑的过程。其实道理就是，如果我们的不做这种平滑的过程，那么在新版本里面，老容量 255 还是按照 2 倍扩容，但是 257 就是按照 1.25 来扩容。这种情况下你会发现 255 扩容之后是 510，但是 257 扩容之后只有 321。这种跃变是很没有道理的，毕竟 255 和 257 就差了 2。
切片的扩容还是比较复杂的。整体来说，扩容就是新建一个数组，而后将老的数组中的元素复制过去。和一般的切片操作比起来，扩容性能是比较差的，尤其是要重新分配内存，以及执行元素复制。
但是比较新的 Go 版本里面，它并不是突然之间从 2 倍扩容变到 1.25 倍扩容的，而是有一个过度过程。
进一步来说，所有的跟扩容有关的，都要考虑这个问题：即扩容频率和空间浪费之间取得一个折中。比如说，早期 Go 是否按照 2 倍扩容的阈值是 1024，后面切换到了 256，也就是认为 256 更能节省空间。

------------------------------

127. 在 Go 中，你什么时候用切片，什么时候用数组？, page url: https://www.mianshi.icu/question/detail?id=587
127-587-答案：
简单题，本质上就是问你两者之间的区别，但是他这个问法，你就要举出一些具体的实践场景。

这应该算是 Go 的专属问题了，毕竟在别的语言里面没有那么奇葩的切片。
你可以参考slice 和数组有什么不同？ (meoying.com)。这个问题主要是讨论两者之间的异同点，和这个问题很类似。

另外一个就是，数组有一个极大的毛病，即它的容量必须是常数（constant value）。也就是说，如果我们希望写这种代码：
在 Go 里面，应该说我绝大多数情况下，都是使用切线的，几乎没有使用数组的场景。从功能上来说，数组和切片是差不多的，或者说数组的功能更加弱，比如说数组连 append 操作都不支持。
我只有在及其罕见的写代码的时候就知道我的数组容量的地方才会使用数组。

------------------------------

128. 请你介绍一下你的这个云原生数据库代理项目, page url: https://www.mianshi.icu/question/detail?id=588
128-588-答案：
简单题。正常来说，这个面试项目的技术含量有好几层楼那么高，所以他们普遍会比较感兴趣，这种时候就会要求你先做一下项目介绍。

具体项目介绍你可以参考项目详情中的项目介绍部分。这里我们强调一下项目介绍的一些要点：首先项目介绍是跟着你的简历写法走的。也就是你的简历如何写的，项目介绍的时候就是在简历的基础上增加一些内容。

------------------------------

129. 为什么你要做一个新的数据库代理？, page url: https://www.mianshi.icu/question/detail?id=589
129-589-答案：
简单题。基本上你就可以根据自己的实际想法来回答，那么如果你不知道怎么回答，就可以参考我们给出的回答。
这个问题的潜台词就是：现在开源的，或者各种自研的已经有很多了，你做这个有什么意义呢？也可以认为，面试官质疑你做这个的价值。

所以这个问题的答案其实也很容易回答：即你的这个项目的优点、相比已有解决方案的改进点在哪里。

而如果你是把这个项目用作你的工作经历参与到社招，那么就还需要进一步解释为什么你们公司要自研这个东西。
在当下，大部分的支持分库分表、读写分离之类的数据库解决方案：

------------------------------

130. 你是如何实现分库分表的？, page url: https://www.mianshi.icu/question/detail?id=590
130-590-答案：
略难的题，大概率会问到。如果你要是本身就对分库分表有深刻的理解，那么你很容易回答出来，也就是那么几个步骤而已。如果你没接触过，那么每一个步骤你都可能很难记住。

但是这个问题是一个极好的面试切入点，你要接着这个机会，尽可能提及一些高级的关键字，做好引导的工作。
这个问题非常宽泛，所以你只需要泛泛而谈，列出和分库分表有关的话题，而后等面试官进一步追问。

你的回答就按照这个三个步骤来回答，每一个步骤你能想到的点，都可以顺带一提，但是不要详细说明。


这里用图来演示一个简单的 SQL 执行过程，方便你理解。当然，你也可以在系统设计中找到对应的内容。
分库分表实现起来非常复杂，大体上来说可以看做是三个步骤：
从性能上来说，合并结果集是最大的性能瓶颈。例如说在排序或者计算聚合函数的时候，可能需要把所有的数据查询过来，而后进行排序、聚合运算，会消耗大量的网络带宽、CPU 和内存。
在整个实现过程中，其实有一个很大的难点就是保证代码整体的扩展性和可读性。这主要是因为 SQL 并不是一个非常结构化的语言，因此我们的没有办法设计一套代码去处理各种情况。

------------------------------

131. 在读写分离的时候，如果你有多个从库，你是怎么选择从库来执行查询的？, page url: https://www.mianshi.icu/question/detail?id=591
131-591-答案：
稍微有点难的题，不太常问，踩过有关从库的坑的面试官会喜欢问。

这个问题有一个极好的引导和刷亮点的点：选择合适的从库，既是负载均衡，又是容错问题。所以这里可以直接将话题引导过去一般意义上的负载均衡和容错这两个点上，也可以结合微服务治理深入讨论，凸显自己在微服务治理方面的积累。
这个问题可以分成两个部分来看：

如果你了解过微服务的治理问题，就知道在微服务里面也是要深入讨论这两个点的。在回答这个问题的时候，你要重点提及从库的负载均衡，而后引出容错简单讨论一下就可以，等面试官进一步追问如何容错。
我在代理中实现了多种负载均衡机制，其核心是一个叫做 Slaves 的接口，里面有一个 Next 方法，返回的就是选中的节点。
从负载均衡这个角度来说，从库的负载均衡算法没有微服务的那么多花头。例如说在微服务部分，我们可以做到根据服务端节点的性能来做负载均衡。但是在数据库从库这里是很少用这个策略的。
当然了，更加复杂的问题其实是从库的容错。也就是说如果从库出错了怎么办。

------------------------------

132. 在读写分离中，怎么解决从库故障的问题？, page url: https://www.mianshi.icu/question/detail?id=592
132-592-答案：
略难的题，很少出现。

大部分公司在使用主从同步的时候，只会考虑主库崩溃的问题，但是很少考虑从库崩溃的问题。而在实践中，从库崩溃之后如果容错策略不当，很容易引发业务故障，或者把主库也拖垮。
这就是考察从库的容错的问题。首先明确一点，如果是只有一个从库，而且它也出现了问题，那么你的代理是没有什么办法的。所以这里讨论的就是有多个从库（大部分公司撑死也就是两个从库），当其中一个从库出现问题的时候，怎么避免选中它。

从理论上来说，从库的容错和微服务中容错是差不多的，都是要先探知节点出现了故障，而后要么是降低这个节点权重，那么选中的几率就会下降；要么是直接将这个节点挪出可用节点的列表，后续探知到节点恢复了之后，再放回去。

所不同的是，在一些比较罕见的情况下，可以考虑在从库崩溃之后将读查询放过去给主库。前提是主库能够撑住这些从库上的流量。
要想针对从库故障进行容错，那么最关键的就是两个点：

第一个问题，探知从库出现故障，能用的手段并不多，主要有两种做法。一种是根据之前的查询执行情况来判定这个从库是否还正常运作，例如说响应时间显著延长，或者频繁超时，就可以认为从库已经出现问题了。另外一种则是通过心跳来判定从库是否正常。
在我的 dbproxy 这个项目里面，从库的容错取决于具体的 Slaves 接口的实现，也就是如果选择从库的实现。

------------------------------

133. 在实现分库分表的过程中，你是如何改写 SQL 的？, page url: https://www.mianshi.icu/question/detail?id=593
133-593-答案：
较难的题，主要是因为不同的 SQL 改写方案都不同，所以要记住的内容比较多。

但是这个问题是一个非常好的问题，因为抛开面试不谈，理解了这个问题，你就能理解后续执行 SQL 和合并结果集两个步骤的复杂度了。更进一步来说，理解了这个过程，你就理解了大多数和分库分表有关的问题。

而在面试中，由于大多数面试官都没有做过类似的业务，所以当你能把过程说清楚，说明白就已经赢得了很大的竞争优势。
一句话就可以说清楚：使用 ANTLR 来根据查询特征改写 SQL。

首先就是我们在这个项目里面的技术选型用的是 ANTLR，也就是抽象语法树（AST）的工具。你可以把它看做是一个代码生成工具，它会根据我们写的 SQL 语法生成解析 AST 的代码。所以在代码中使用起来就和使用增删改查没什么区别。

具体的 SQL 怎么改写，我们在系统设计、项目难点里面都有描述过，这里再次重复一下：
改写 SQL 是一个非常关键的部分，我采用的是 ANTLR 改写 AST 的方法。

具体来说就是按照 ANTLR 的规则编写 SQL 的语法规则，而后生成解析 SQL 的代码，这部分比较简单。

而真正难的地方在于要根据查询特征改写 SQL，具体来说：

------------------------------

134. 在处理分库分表的时候，你是怎么解析 SQL 的？, page url: https://www.mianshi.icu/question/detail?id=594
134-594-答案：
简单的题，它实际上是“你如何改写 SQL” 的一个子问题。

回答这个问题，要顺便提及我们的 HINT 设计，这个设计能够充分体现你在 ANTLR 上的深厚造诣，确保你将来入职之后可以顺利使用 ANTLR。
如果你本身已经有一定的工作年限了，那么就可以在 ANTLR 解决分库分表的基础上，引申到使用 ANTLR 来解决别的问题。
主要就是使用 ANTLR。
而之所以选用 ANTLR，主要是因为 ANTLR4 的功能比较强大，让我可以在 SQL 语法的基础上，增加一些新的自定义的语法特性。
ANTLR 用途其实很广泛，我之前用过 ANTLR 来设计基于 SAGA 的分布式事务框架，这个框架其实很像工作流引擎。也用 ANTLR 来解决分布式任务调度中的任务编排等问题

------------------------------

135. 为什么你要使用 ANTLR 来解析 SQL？还有别的方式吗？, page url: https://www.mianshi.icu/question/detail?id=595
135-595-答案：
简单的题目，记住 ANTLR 可以为所欲为。而你在回答的时候，就要突出 ANTLR 这个特性，并且进一步引申到 ANTLR 可以用在别的场景下。
选择 ANTLR 的原因很简单，ANTLR 支持自定义语法。比如说 dbproxy 强依赖的 HINT 特性，就是借助 ANTLR 的自定义语法来实现的。而借助这个 HINT 特性，我们就可以实现全链路追踪，AB 测试，全链路压力测试等功能。

更进一步来说，我们完全可以在 dbproxy 已有功能的基础上，提供一些独有的语法。比如说可以提供类似于：
除了ANTLR，也没别的选项了。唯一值得考虑的是 tidb 研发的 parser。从性能上来说，它的性能要好一些。但是借助它比较难实现我们预想中的自定义语法的功能。
ANTLR 也不仅仅是可以用于这里，后续但凡涉及到一些需要自定义规则的地方都可以使用类似的技术。比如说我曾经借助 ANTLR 来实现一个基于 SAGA 的分布式事务框架，以及一个支持任务编排的分布式任务调度系统。

------------------------------

136. 分库分表里面，你怎么求聚合函数？, page url: https://www.mianshi.icu/question/detail?id=596
136-596-答案：
有点难度的题，但是如果你已经掌握了改写 SQL 部分的内容，那么这个问题就是送分题。

回答这个问题的时候，要着重指出平均值的求法是不一样的，它最为特殊。而要想刷亮点，那么就要进一步讨论一个非常高级的聚合函数和 GROUP BY 结合的用法。你只要能够将聚合函数和 GROUP BY 这部分说出来，保证吊打你的竞争者。如果面试官本身没有做过分库分表中间件，你同样可以吊打他。
这里我用例子来给你说明。

首先是不带 GROUP BY 的情况。
第一个是计数 COUNT。注意 COUNT 里面的字段，以及 WHERE 部分都不影响整体算法；
聚合函数在分库分表里面是一个难点，总体来说可以分成求平均值和求非平均值。

对于 COUNT 和 SUM 来说，就是执行分库分表之后，将结果汇总之后再次求和；
当然，这里还要进一步结合 GROUP BY 来考虑。
但是有一种最为特殊的情况，就是如果要是 GROUP BY 后面恰好是分库分表键，那么就汇总结果之后直接返回就可以，不需要再次计算了。

------------------------------

137. 在分库分表里面，怎么求平均值？, page url: https://www.mianshi.icu/question/detail?id=597
137-597-答案：
略难的题，但是如果你看过改写 SQL 以及如何求聚合函数之后，这个题就很简单了。

求平均值其实就一个关键点，改写 SQL 的时候要将 AVG 改写为 SUM 和 COUNT，最后再汇总计算平均值。
平均值在聚合函数里面是属于比较特殊的。在没有 GROUP BY 的情况下，直接将 AVG 改写为 SUM 和 COUNT，汇总之后再次计算平均值。
在结合 GROUP BY 之后，如果 GROUP BY 后面是普通的列，那么就需要在 SELECT 里面加上 GROUP BY 的列，汇总之后再次分组计算平均值。
如果 GROUP BY 本身恰好是使用分库分表键，那么就不需要这么改写 SQL，只需要汇总一下数据就可以了。

------------------------------

138. 你提到的 HINT 是什么东西？, page url: https://www.mianshi.icu/question/detail?id=598
138-598-答案：
简单的题。 HINT 本质上就是一个遵循了特殊格式的 SQL 注释，我们借助这个注释来传递一些链路信息。

你回答这个问题的时候，如果对微服务非常熟悉的话，就要把话题朝着微服务上链路相关的内容上引导，包括但不限于AB 测试，全链路追踪，全链路压测等。这可以展示你在这方面有很深入的思考。
为了防止你还没看过项目里面的系统设计和别的和 HINT 有关的面试题，这里再次介绍一下 HINT。

HINT 其实是我们自己定义的一种语法，用来传递上下文信息，这些上下文信息取决于具体的业务场景。我举几个例子：
HINT 是我们设计的一种携带查询上下文信息的机制，它的本质就是一种特殊格式的注释。

------------------------------

139. 在 dbproxy 分库分表中，你怎么支持事务的？, page url: https://www.mianshi.icu/question/detail?id=599
139-599-答案：
难题。这个难度源自两方面，一个是分库分表中的事务包含了跨库事务，它的本质是分布式事务，而分布式事务的理论非常多，你需要记忆很多东西；另外一方面就是要彻底理解为什么我强调分布式事务解决不了一致性问题，甚至于可以说 ACID 里面它一个都保证不了。

在实际面试中，你基本上只要讨论到分布式事务没有办法做到强一致性，就可以超出绝大多数的面试官的想象了，因为他们实际上也就是背背八股文，很少深入思考这些问题。
你可以在系统设计里面也找到关于数据库事务和跨库事务的讨论。

总的来说，在 dbproxy 里面，数据库事务的支持分成三种：


下面两张图你可以在系统设计里面找到，它们分别描述了延迟事务和饥渴事务。下图是延迟事务，你可以注意到，虽然有三个目标库，但是整个事务只在 db_a 和 db_b 上执行，db_c 本身完全用不上。
在 dbproxy 里面，我主要是支持了三种事务：
（做一个简单分析）从实践上来说，我最推荐的是直接禁用跨库事务，因为在分库分表之后本身不管是跨表还是跨库都不是好的实践，而跨库事务就是更加不好的实践了。在禁用了跨库事务之后，就是一个普通的数据库事务了，这种情况下是能够保证 ACID。
当然，从理论上来说，跨库事务本质上就是一个分布式事务，而分布式事务还有很多选择，包括 TCC，SAGA 以及所谓的 AT 事务。

------------------------------

140. 在延迟事务里面，如果遇到了部分失败的问题怎么办？, page url: https://www.mianshi.icu/question/detail?id=600
140-600-答案：
略难的题。准确来说，在分布式系统相关的面试下，最麻烦的就是部分失败，你在每次回答的时候都要仔细考虑每一种部分失败会带来什么后果，以及可行的后果是什么。

在这个问题之下，你记住最为关键的就是点出所有的分布式事务（跨库事务）都没有办法彻底解决部分失败的问题，所有的解决方案也难有高下之分，只是说看你如何做取舍了。
其实部分失败在 dbproxy 里面就是一句话：天要下雨娘要嫁人，dbproxy 都无能为力。
先来看一下部分失败的分析：

这个要具体分析部分失败发生在哪里，典型的有三个：
我们的选择就是不搞反向补偿。因为反向补偿有明显的三个弊端：

举个例子来说，如果现在用户在事务里面执行一个 DELETE 操作。那么我们要先生成反向补偿的操作，就必须先查询数据库拿到被删除的数据，而后转为 INSERT 语句。

如果真的很不幸在提交的时候部分失败了，那么这个时候业务就会发现，数据被删了一部分，出现不一致的问题。
当然，也不仅仅是说 dbproxy 有这种问题，所有的分布式事务解决方案都有这种问题。

比如说 TCC，在 Confirm 的时候也会面临一样的问题。比如说一些节点 Confirm 成功了，一些节点 Confirm 失败了。当然也可以在失败的时候继续重试，但是重试都会失败。类似地，那些 Confirm 成功的业务数据，就已经可以被别的业务看到了。

------------------------------

141. 在 dbproxy 里面，如果一个 SELECT 语句命中了多个表，但是部分表没有返回数据，怎么办？, page url: https://www.mianshi.icu/question/detail?id=601
141-601-答案：
简单题，因为 SELECT 不涉及数据修改。很少面试官会这么问，要问一般也是问分布式事务或者跨库事务有关的问题。
很简单，直接报错就可以，因为 SELECT 是不涉及数据修改的，也就不存在什么数据一致性和 ACID 之类的问题。

------------------------------

142. dbproxy 里面的事务，能保证 ACID 吗？, page url: https://www.mianshi.icu/question/detail?id=602
142-602-答案：
结论简单，论证难的题。也就是说，在跨库事务里，保证不了 ACID。

但是这并不是 dbproxy 设计或者实现的缺陷，而是所有的分布式事务解决方案都有的缺陷。
很显然，在在延迟事务里面，如果遇到了部分失败的问题怎么办？ (meoying.com)里面你就已经可以注意到，ACID 一个都保证不了。
如果不使用跨库事务的话，那么就是一个普通的数据库事务，可以保证 ACID。

而如果是跨库事务的话，保证不了 ACID的。

------------------------------

143. 客户端和 dbproxy 是怎么通信的？, page url: https://www.mianshi.icu/question/detail?id=603
143-603-答案：
略难的题，正常面试官不会问，只有遇到做数据库中间件研发并且和 MySQL 协议打交道的面试官可能会问一下。

简单来说，dbproxy 基于 TCP 实现了 MySQL 协议，把自己伪装成了一个 MySQL 数据库，所以客户端就仿佛在和一个普通的 MySQL 数据库一样通信。这样做的好处就是已有的代码可以无缝切换。
在系统设计部分，你已经看到了这部分的内容。MySQL 协议的实现是一个比较复杂的过程，这张图可以看出来：
客户端和 dbproxy 的通信使用的是基于 TCP 协议的 MySQL 协议。

------------------------------

144. 使用你这个项目，我作为业务方怎么查到我某个 SQL 的执行情况？, page url: https://www.mianshi.icu/question/detail?id=604
144-604-答案：
简单题，前提是你听懂了面试官问的是什么。

这个问题实际上问的是问题排查，也就是如果一个业务方因为线上出了故障，所以想要知道这个 SQL 在 dbproxy 上的执行情况，应该怎么办。你要借助这个机会将话题引导到 HINT 设计和可观测性支持上来，这样就可以刷出亮点。
实践中有两种做法，一种是传递 transaction id，也就是业务概念上的 ID，比如说某个订单的 ID。那么在发现业务有 BUG 之后，拿着这个 transaction ID 去查询 dbproxy 采集的监控数据。

另外一种是借助全链路追踪的 trace id，那么 dbproxy 也会生成对应的 span，组成一条完整的链路追踪信息（trace）。但是基本的架构都是如下图：
dbproxy 本身提供了完整的可观测性支持，包括性能数据和日志信息，但是这些功能是需要开启的。

------------------------------

145. 使用你这个项目，我怎么做全链路压测？, page url: https://www.mianshi.icu/question/detail?id=605
145-605-答案：
简单的题。你不要被全链路压测这几个字唬住了，当然从实践上来说全链路压测很难做，但是它的难不在技术端，在组织端。也就是说，全链路压测关键点在于你要有足够高的地位和权力去推进，而后顺你者昌，逆你者亡，就很容搞起来。不然各个部门推诿你一辈子都搞不起来。

而在 dbproxy 里面，支持全链路压测也很简单，就是一种特殊的分库分表规则而已，业务方借助 HINT 把查询是不是压测请求的查询传递过来，而后选择恰当的数据库数据表就可以了。
用一张图来给你展示这个 dbproxy 对压测的支持：
dbproxy 里面预留了支持全链路压测的设计。

------------------------------

146. 什么是快照读？, page url: https://www.mianshi.icu/question/detail?id=606
146-606-答案：
简单题，关键就在于你是否听过这个名词，听过就很简单，没听过就很容易回答不上来。

回答这个问题最重要的就是把话题往 MySQL 上引，并且要揭示快照读并不是一个标准的隔离级别。
快照读就是指读快照，是指在事务开始的时候生成一个数据快照，后续所有的读操作都是读到这里面的数据。

------------------------------

147. 为什么现在互联网的应用中比较多使用已提交读隔离级别？, page url: https://www.mianshi.icu/question/detail?id=607
147-607-答案：
简单的题，你只要听过这个问题就很容易记下来并且回答好。

要注意的是，虽然我们认为主流是已提交读，但是在一些特定的业务里面，或者场景之下，还是会使用可重复读，只是说用的比较少而已。
互联网的应用有一个特点：高并发，但是每个事务都是短平快。短平快还有一个意思，就是操作都很简单，很少出现重复查询某个数据的场景。即便有这种业务场景，也多半可以通过缓存来解决问题。

------------------------------

148. 在实践中，如果让你来做决策，你会选择什么隔离级别？, page url: https://www.mianshi.icu/question/detail?id=608
148-608-答案：
简单题，你可以从已提交读和可重复读中随便编一个。

注意的是，未提交读和串行化是几乎不可能使用的，前者脏读会引发非常多的问题，后者则是性能太差。
这个问题有一个不好的地方，就是有些面试官虽然明面上问的是开放性的题目，实际中缺失如果你回答不合他心意，就会跟你过不去，
如果是我的话，我会看具体的业务吧。比如说如果业务是偏向互联网的那种高并发但是短平快的，就可以考虑使用已提交读。

------------------------------

149. 你了解 MySQL 的 MVCC 吗？, page url: https://www.mianshi.icu/question/detail?id=609
149-609-答案：
略难的题，高频面试题。难点主要在于 MVCC 本身内容很多，很难全部记住，而且从 MVCC 衍生出来的内容也太多，问法多样，初学者很容易栽在这里。

这个问题一般是深入面 MVCC 的开场白，所以你在回答的时候只需要把关键点点出来，留下引导就可以了，具体细节可以等面试官追问的时候再说。
MVCC，全称多版本并发控制，是 MySQL 中 InnoDB 引擎使用的实现事务的底层机制，你理解 MVCC 就要从多版本这三个字下手。而后我建议你先去看了有关隔离级别的内容再来看这里，就能充分理解为什么 MySQL 要搞什么多版本控制了。

多版本是指每一条（具体到数据库中就是一行）都有很多版本，有些是“稳定版”——也就是已经提交了的版本，有些是“不稳定版”，也就是你事务正在执行过程中出现的。

因此，MVCC 的关键点就是两个：
MVCC，全称是多版本并发控制，是 MySQL InnoDB 中用来实现事务，以及事务隔离级别的核心机制。

它的关键点有两个：存储不同版本的数据以及如何控制事务读取哪个版本的事务。

就存储来说，MVCC 使用了版本链。每一条数据都有两个额外的列，一个是事务 ID，也可以看做是版本号；一个是回滚指针，MVCC 利用回滚指针将数据不同的版本串联在一起，并且将这个版本链存储到了 undo log 日志中。

------------------------------

150. MySQL 支持快照读吗？, page url: https://www.mianshi.icu/question/detail?id=610
150-610-答案：
简单的题，支持。

这个问题有一个小陷进，就是虽然 MySQL 虽然事实上实现了快照读的效果，但是它并没有称为快照读，而是依旧是说可重复读。因此你在回答的时候要指出这一点，防止面试官抬杠。
严格来说，MYSQL 的隔离级别里面没有快照读，但是事实上 MYSQL 在可重复读的基础上解决了幻读问题，因此基本可以看做是支持了快照读。

------------------------------

151. MYSQL 中的 Read View 是什么？有什么用？, page url: https://www.mianshi.icu/question/detail?id=611
151-611-答案：
略难的题，难在理解并记住。

在回答这个问题的时候，一定要紧密结合 MVCC 和隔离级别来讨论。
在你了解 MySQL 的 MVCC 吗？ (mianshi.icu)中我们详细分析了 MVCC 的原理。这里再次强调一下 MVCC 中的 Read View 的作用。

大部分情况下，我觉得你记住我给出的简易好理解原理就可以了，至于具体的 ReadView 里面的什么高水位低水位的，没有太大意义，面试说不说得出来区别都不大，因为简易版原理就足以证明你知道 ReadView 了。

Read View 简单来说就是可见性控制。Read View 有一个核心的字段 m_ids，它记录了生成 Read View 时刻数据库上正在执行但是还没提交的事务，要注意，这些事务可能在使用这个 Read View 的过程中就提交了。
Read View 是 MVCC 的一部分，MYSQL 主要用它来控制事务中的数据可见性，或者说支持事务不同隔离级别。

------------------------------

152. 在已提交读中，执行 SELECT 语句会生成 Read View 吗？, page url: https://www.mianshi.icu/question/detail?id=612
152-612-答案：
难题，能够问出这个问题的面试官对 MYSQL 应该是非常熟悉的。

大部分时候时候，网上（包括我）在说已提交读的 Read View 生成时机的时候，都会说每一次执行 SQL 就会生成 Read View，这种说法是不准确的，或者说是简化之后的，准确说法是非一致性锁定读的时候，才会生成 Read View。
并不是，因为只有非一致性锁定读才会生成的 Read View。举个例子来说，SELECT FOR UPDATE 就不会生成 Read View。

------------------------------

153. 在 MYSQL 中，增删改的时候会使用到 Read View 吗？, page url: https://www.mianshi.icu/question/detail?id=613
153-613-答案：
难题，这个问题需要对 Read View 有很深的理解才能问出来，当然也才能回答出来。
不会，因为 Read View 只会用于查询，而增删改这种修改数据的，一般使用的是锁。

------------------------------

154. 在 MYSQL 中，undo log 有什么用？, page url: https://www.mianshi.icu/question/detail?id=614
154-614-答案：
略难，出题可能性比较高的题目，一般在考察 MVCC 的时候都大概率问到。

回答这个问题，就要和版本链，Read View 联动，同时要注意延伸扩展到其它中间件所声称的事务上。也就是记住这个结论：但凡一个中间件声称自己支持事务，但是又没有类似 undo log 和 redo log 的机制，你就可以认为，它们支持事务，但是不支持 ACID 的事务。
undo log 这个名字叫就已经揭示了它的作用，它的核心作用有两个：

回滚操作进一步划分也可以分成是用户主动回滚——调用了 ROLLBACK，或者被动回滚——数据库崩溃之后恢复，但是核心步骤是类似的。

undo log 中记录了两大类东西：
undo log 在 MYSQL 中记录了事务操作的补偿动作，主要应用于事务回滚和 MVCC。事务回滚可以是用户主动回滚，也可以是数据库崩溃之后恢复过程中回滚未执行成功的事务。

首先，就应用于事务回滚来说，undo log 记录了每一个对数据库修改的记录。

对于 INSERT 来说，undo log 记录了 INSERT 插入的数据，在回滚的时候则执行删除操作；
undo log 和 redo log 合并在一起，才能达成我们预想中支持 ACID 的数据库本地事务。很多中间件虽然声称自己支持事务，但是本身并没有类似的机制。比如说 Redis 也有事务的概念，但是显然 Redis 的事务并不满足 ACID，最多可以说 Redis 的事务满足隔离性。
更进一步来说，大部分分布式事务解决方案也同样是做不到 ACID 的。比如说 TCC 这种分布式事务解决方案，很明显的就是做不到 ACID。

------------------------------

155. MYSQL 是如何删除一条数据的？, page url: https://www.mianshi.icu/question/detail?id=615
155-615-答案：
略难的题。你如果第一次听到这个题目你都可能觉得很懵逼，毕竟从直觉上来说删除一条数据，就是直接从磁盘上删除数据。而实际上，它考察的关键点是延迟删除。

所以在回答这个问题的时候，一个是要清楚说清楚步骤，而后则是要将话题引导到 undo log、碎片整理上。此外，如果你能够解释清楚为什么搞这种标记-延迟删除的做法，就更加出彩了，毕竟大多数候选人最多回答到延迟删除的特性，你回答之后的赢面还是很大的。

简单来说，整个删除步骤可以分成几步（这里我们只讨论 InnoDB 引擎）：

这里关键点就是怎么理解这个 purge thread，也就是异步删除的特性。下图展示了这个原理：


DELETE 语句在 MYSQL 执行中分成四步：
从数据库设计的角度来说，这种标记-延迟删除主要还是为了提高性能。
但是删除操作会带来一个小问题，就是所谓的碎片问题。

------------------------------

156. 你了解 MYSQL 的锁吗？, page url: https://www.mianshi.icu/question/detail?id=616
156-616-答案：
简单的题，高频的题。一般这个问题会作为深入考察锁机制的开场白。正如之前你看到的大部分宽泛的问题，你在回答的时候，要点在于尽可能说出关键字，而后等待面试官追问。

你刷亮点落在：提及索引与表锁的问题，提及乐观锁以及已提交读下使用更少的锁。当然这也是一种引导，而后你就可以深入讨论这几个问题，打出组合拳。
MYSQL 的锁机制，有很多角度可以回答:

而后你要记得提起一个点，就是如果在查询中查询没有命中任何索引，那么就会使用表锁。这种情况下，性能衰减非常可怕。
MYSQL 的锁有很多种说法：
在 MYSQL 中，什么时候加锁，加什么锁都不是我们能够控制的。比如说我们经常使用的 SELECT FOR UPDATE 的写法，这算是强制要求数据库加锁，但是从这个语句上我们其实是看不出来 MYSQL 究竟加什么锁。
不过我从个人角度来说，我是非常不建议在数据库中直接使用 SELECT FOR UPDATE 之类的这种悲观锁，而是更加倾向于使用乐观锁。我之前在优化数据库查询的性能，就通过使用乐观锁来优化了很多 SELECT FOR UPDATE 的这种写法，一方面提高了性能，一方面也规避了死锁等问题。

------------------------------

157. 什么是记录锁、间隙锁和临键锁？, page url: https://www.mianshi.icu/question/detail?id=617
157-617-答案：
略难的题。难度源自两方面，一个是三个概念本身其实在实践中很少用得上，容易混淆并且还有一个垃圾译名临键锁；另外一方面是这三者的加锁时机真的一时半会讲不清楚。

这里有一个值得在面试中提起的点，就是 MYSQL 记住临键锁在可重复读这个隔离级别下解决了幻读的问题。除此以外，我认为这个题目的性价比比较低，所以你只需要回答出来它们三个的基本定义，而具体地什么时候加什么锁，你有闲情逸致的时候再慢慢学习。
先来看记录锁，例如说 SELECT * FROM users WHERE id IN (2, 5) FOR UPDATE：


那么就在 id 为 2, 5 的记录上分别加了一把记录锁。
记录锁、间隙锁和临键锁是 MYSQL 中的三种锁。

首先记录锁针对的是一条条的数据，例如说在使用等值查询的时候，一般加的就是记录锁。

其次间隙锁针对的是某一个范围，但是间隙锁一般不包含端点。
此外临键锁和间隙锁只工作在可重复读这个隔离级别下，并且 MySQL 利用临键锁解决了幻读的问题，因此 MYSQL 的可重复读也被认为是快照读。

------------------------------

158. 什么情况下 MYSQL 会使用表锁？, page url: https://www.mianshi.icu/question/detail?id=618
158-618-答案：
简单题。这个题目就是考察一个问题，也就是如果实在查询没有命中任何索引的时候，就会使用表锁。

当然，除了这个考点以外，还有一些其他的原因，你要是记得也可以顺便提出来。而后你可以提一下自己优化过表锁，从而将话题引导过去怎么优化查询上。
在很多情况下，MYSQL 都会使用表锁，包括：
早期我在做性能优化的时候，就优化过因为没有命中索引而导致使用表锁的问题。不过这种错误过于低级，常见于历史代码，或者说经验不足的实习生写的代码中。

------------------------------

159. 为什么 MYSQL 的 InnoDB 引擎在没有索引的时候，就会使用表锁？, page url: https://www.mianshi.icu/question/detail?id=619
159-619-答案：
简单的题，你记住关键点锁是利用索引来实现的就可以了。

但是如果你要是能够站在设计者的角度解释为什么做成这样的设计，并且明确指出归根结底是依赖聚簇索引来实现的，就可以刷出亮点。
其实要解释这个问题也很简单。首先你想你现在要加行锁，你的锁总要有一个地方放。那么考虑到你有很多个查询都需要加锁，你的锁就不能放在各个查询里面自己维护，而是只能让数据自己维护。比如说锁对应的 Lock 结构体，肯定是和数据绑定在一起，而不是和查询绑定在一起。形如：




而更进一步讨论，这个锁必然是放在内存中的，不可能放在磁盘上，因为你不可能每次操作锁都要触发磁盘 IO。
总的来说，MySQL 上的 InnoDB 引擎是借助了索引来实现的，而且最为核心的是依赖聚簇索引。
更加具体来说：

------------------------------

160. MYSQL 在可重复读的隔离级别下，是怎么解决幻读的？, page url: https://www.mianshi.icu/question/detail?id=620
160-620-答案：
简单题，利用的是临键锁。

在这道题下，你可以深入讨论临键锁究竟是怎么解决幻读的，并且结合加锁的实际情况指出有些时候可能是间隙锁解决了幻读。而后将话题引导到快照读上。并且还可以稍微提一下在互联网应用里面，其实并不怎么使用可重复读这个隔离级别。
回忆一下幻读：在事务中读到了新插入的数据，如下图：



而在 SELECT 语句里面，MySQL 会加上一个临键锁，这个临键锁覆盖了 id = 3 这个位置，所以你无法插入进去：
在可重复读这个隔离级别下，MYSQL 主要是借助临键锁来阻止新数据插入，从而解决了幻读的问题。
在解决了这种幻读之后，MYSQL 可以认为支持快照读了。也就是说在数据库事务执行期间，可重复读这个级别下，可以认为同样的查询得到的结果总是相同的，不存在幻读的问题。

------------------------------

161. 在 MYSQL 中，redo log 是什么？有什么用？, page url: https://www.mianshi.icu/question/detail?id=621
161-621-答案：
简单的题，也比较常问。redo log 要背的内容明显要比 undo log 少，所以要简单一些。

redo log 这边刷亮点要着落在讨论 redo log 这种 WAL 机制在不同中间件中的应用，而后进一步指出 redo log 的刷盘时机会对事务的最终效果有影响。
redo log 的主要作用是确保事务的持久性，即使在发生故障时也能保持数据库的一致性。它和 undo log 是相辅相成的，undo log 是在崩溃的时候回滚，而 redo log 是在崩溃的时候重新执行事务。

所以用 undo log 回滚还是用 redo log 重做事务，就取决于事务有没有被提交。

你可能很难理解，就是为什么事务提交了，数据库崩溃了，还要用 redo log 来重做事务。这其实有一个点：事务修改数据只是修改 buffer pool，后续再异步的刷新到磁盘上。也就是说当你的业务收到 COMMIT 成功的响应的时候，数据可能还在磁盘上。
redo log 在 MYSQL 中用于支撑数据库事务，它的作用主要有两个：
redo log 其实是一个典型的 WAL 日志，在很多中间件里面都能看到类似的设计，比如说 Kafka 的日志文件也是类似的设计。当然 undo log 其实也是类似的。
但是即便使用了 redo log，也会有刷盘的问题，因为我们通常说的写入 redo log，只不过是写到了内存里面，并没有真的写到磁盘上，只有在恰当的时机才会刷新到磁盘上。在这之前，这些数据都有可能丢失。

------------------------------

162. 在 MYSQL 中，事务提交之后就一定不会丢吗？, page url: https://www.mianshi.icu/question/detail?id=622
162-622-答案：
难题。准确说，如果你从来没有听过类似的题目，那么你大概率会寄了。但是如果你听过，你就肯定能够回答出来。

这个问题的关键点就是 redo log 的刷盘实际的问题，但是大部分人都不会去调整这个参数，所以很少有人遇到过。而你要想刷出亮点来，你就可以进一步延伸到所有中间件，以及所有分布式中间件都要面临的“写入”语义问题，这个是你要重点掌握的。
首先你需要背熟这个问题的答案：在分布式环境下，写入数据的语义有哪些？ (meoying.com)
大多数情况下，数据库事务提交成功，就不会再丢失，这是我们讨论的 ACID 中的持久性保证的。

但是在极端的情况下，还是有可能丢失，例如说调整了 redo log 的刷盘时机。
当然，这也不仅仅是MYSQL 的问题，应该说大多数的跟刷盘操作有关的中间件都会有类似的问题。
更进一步讨论来说，在分布式环境下的写入语义就更加复杂了。

比如说主从结构下，写入语义可能是有多种。

第一种是写入到主节点就认为写入成功了。这种其实也会丢失数据，因为数据还没同步到从节点，如果此时主节点宕机了，新选的主节点上也没有数据。

------------------------------

163. 在 Go 里面，你怎么优化性能？, page url: https://www.mianshi.icu/question/detail?id=623
163-623-答案：
略难的题。其实从理论上来说都很简单，只是说你可能实践中根本没有做过，所以理解起来会有点难。

正常来说，我都是建议你在简历上写自己擅长优化性能，包括在面试中也写自己擅长优化性能。因为擅长优化性能是一个极大的竞争优势，特别有助于你拿到 offer。而在这个问题之下就是展示你在 Go 语言层面上的优化性能的能力。

基本上，你能够回答出来两三点就可以赢得很多竞争优势了。
这也是一个宽泛的问题，所以你最开始的时候需要宽泛地回答，而后等面试官详细询问你是如何优化的。注意一点的是，这里我们认为面试官考察的是语言层面上的性能优化，而不是业务、流程或者架构上的性能优化手段。

Go 语言的性能优化只有两大角度：

其实 CPU 也能优化的，但是可以做的事情不多。
我用过很多种语言层面上的性能优化手段，主要是优化内存和并发优化。

内存优化最基础的就是使用对象池和 buffer pool。在业务研发中，对象池用得比较多。比如说我们有一个高并发接口，要查询并且返回某个对象，那么我就简单引入了一个对象池来缓存这些对象，减少了内存分配。

buffer pool 一般用在一些工具类，中间件上，也是为了减少内存分配。比如说我们严重依赖于 JSON 序列化，我就使用了 buffer pool 来减少 JSON 序列化产生的内存。
还有一些不太常用的，但是偶尔也会使用的优化技巧。就内存分配来说：

第一个是内存逃逸分析，也就是减少分配到堆上的内存；

第二个则是考虑使用实验特性 arena，直接在对外内存上分配对象，自己手动管理内存；
不过在实践中，如果是业务研发的话，在语言层面上优化性能是不太值得的，因为收益很小。业务性能优化，应该优先聚焦在查询优化、缓存方案上。这两个手段任何一个都能取得比语言层面上优化更好的效果。

------------------------------

164. 你是怎么搞逃逸分析和优化的？, page url: https://www.mianshi.icu/question/detail?id=624
164-624-答案：
略难的题。逃逸分析是一个在中间件研发中很常见的性能优化手段，但是业务研发基本上接触不到。

你在回答的时候，最好是有具体的例子加以辅助证明。
逃逸分析其实很简单，只需要在执行 Go 命令的时候加上参数 gcflags '-m' 就可以看到 Go 编译器的输出，里面会详细说明每一个变量是分配在栈上还是分配到堆上，当然还能看到别的信息，比如说是否内联。

例如你执行命令：
那么你可以看到类似的输出：
在 Go 里面，提供了基本的分析工具来查看内存在编译阶段的分配类型，可以在编译的时候传入 gcflags '-m' 参数，就能看到一个对象的分配情况。如果内存逃逸了，也就是分配到了堆上面，那么能够看到类似 escape heap，或者 leaking param 这种输出。
当然这种优化并不是没有代价的。大多数内存逃逸优化都是依赖于值传递特性，那么就会出现值传递引起的复制问题。

------------------------------

165. 为什么在使用切片的时候，要尽可能准确预估容量？, page url: https://www.mianshi.icu/question/detail?id=625
165-625-答案：
简单题。

很多初学者在使用切片等内置类型的时候，不注意预估容量，这可能会导致代码运行的时候引起不必要的扩容。在面试的时候，你可以将这个做成你的性能优化案例，也就是通过修改代码，准确预估容量来减少扩容，或者减少内存浪费。
在 25K 回答中使用的切片例子，你可以参考ekit/slice/find.go at dev · ecodeclub/ekit (github.com)
在使用任何和容量有关的数据结构的时候，都应该尽量估算准确所需的容量。
所以，我们以前在优化程序性能的时候，就会去找类似的这种场景，而后修正预估的容量，尽量做到刚好。

但是也有一些场景是没有办法准确计算的。比如说我之前开发过一个切片的工具方法集合里面，有一个方法叫做 FindAll，也就是遍历所有的元素找出符合条件的元素。在这种场景下你是不知道究竟有多少元素会符合条件。因此我做了一个简单的策略：我认为符合条件的是少数，或者说极少数，因此我预估找到的元素数量是原始个数的 1/8 再加一。

------------------------------

166. 你用 Kafka 解决过什么问题？, page url: https://www.mianshi.icu/question/detail?id=626
166-626-答案：
基础题，高频面试题，这个问题可以看做是面试官准备和你深入讨论 Kafka 相关内容的开场白。

回答这个问题的基本思路就是列举自己平时使用 Kafka 的场景。但是如果你只是列举一些简单的、没啥心意的 Kafka 使用场景，那么你很难赢得竞争优势，并且打开话题。所以最好是在面试之前准备一些高级的 Kafka 使用场景，并且是某个复杂解决方案中的一部分，这样就可以将话题引导到你精心准备的高级方案下，从而拖过面试时间，并且赢得竞争优势。
要回答好这个问题，你需要根据自己的实际经验来准备几个使用场景。但是挑选这些实际场景要注意几个方面：
我用 Kafka 解决过很多问题。最经常使用的场景就是利用 Kafka 来解耦下游。
我在服务治理中也大量借助了 Kafka。

举个例子来说，在设计一些限流、降级方案的时候，在业务方允许的情况下，我可以将被限流、被降级的请求转储到 Kafka 上，后续再处理。也就是说，同步转异步。

------------------------------

167. 为什么你要用 Kafka 而不是直接发起同步调用呢？, page url: https://www.mianshi.icu/question/detail?id=627
167-627-答案：
略难的题，如果你之前只是遵循公司传统使用 Kafka 而没有思考为什么要优先考虑 Kafka，那么这个问题你很可能回答不出来。

回答这个问题，要从解耦、削峰、性能、可用性这些角度去论证 Kafka 这种机制要比同步调用好，并且进一步推导出一个结论：就是在设计系统的时候，应该并且总是应该优先考虑使用 Kafka 或者说异步操作的。
这个问题其实是问在一些和下游解耦的场景下，为什么你要解耦，而不是直接调用。

也就是下面这两种方式的对比：


和同步调用比起来，使用 Kafka 的优势是比较明显的。

首先是解耦，解耦之后应用就不再关心下游了。既不关心有哪些下游，也不关心下游接口怎么样；

第二个是削峰。例如说应用收到一波突发流量之后，将消息发送到 Kafka 之后，下游可以根据自己的真实处理能力慢慢处理，而不会被这种突发流量打崩；
当然借助 Kafka 之后还有一些更加高端的用法。例如说消费者可以根据自己的实际性能来处理 Kafka 上的消息，这一点就可以有很多种玩法。

------------------------------

168. Kakfa 是如何做到高性能的？, page url: https://www.mianshi.icu/question/detail?id=628
168-628-答案：
略难的题，高频面试题。Kafka 的设计者使用了非常多的技巧来提高 Kafka 的性能，而这些性能优化手段互相之间也没太多逻辑上的联系，所以你记忆起来会比较困难。

但是从面试的角度来说，这个问题又是一个非常好的打开话题，引导面试方向的问题。所以你在回答这个问题的时候，就要尽量提及你记得的所有的能够提高 Kafka 性能的点，而后等待面试官进一步追问。
这里我们就先简单罗列的一下 Kafka 使用过的提高性能的手段，具体每一个手段的细节你可以在相关的面试题下找到详细的分析。

整体上来说，Kafka 使用的性能优化手段包括：
Kafka 使用了非常的技巧来优化性能，主要有：
当然，这些性能优化手段也并不仅仅是只有 Kafka 才会使用。严格来说，大部分的跟网络 IO，磁盘 IO 有关的中间件都会使用这些技术。

例如说在 MySQL 中，也大量利用了顺序写的特性。比如说我们熟悉的 MVCC 机制使用的 undo log 和 redo log，都是一种 WAL，也就是顺序写的日志。再比如说在 Redis 使用 AOF 也是充分利用了顺序写的高性能。

------------------------------

169. 什么是零拷贝？, page url: https://www.mianshi.icu/question/detail?id=629
169-629-答案：
基础题。如果你是科班出身，那么你在学习操作系统的时候几乎肯定会学习到这个概念。

要想回答好这个问题，一定要讲清楚零拷贝的原理，而后引申到零拷贝在不同中间件中的应用。
零拷贝（zero copy）技术是一种计算机系统中用于优化数据传输的技术，它的核心思想是减少或消除在数据传输过程中不必要的数据拷贝操作，从而提高数据传输的效率和性能。

需要注意的是，零拷贝是指 CPU 参与的数据复制次数是 0，但是不是说整个过程都没有零拷贝技术。它还是需要 DMA（直接内存访问，Direct Memory Access）来拷贝的。

为了理解零拷贝技术，你需要先回忆一下普通的 IO 是怎么搞的。
零拷贝是一种优化数据传输的技术，核心思想是尽量避免数据在内存之间的拷贝，并且做到 CPU 完全不参与拷贝，这也就是零拷贝的零的含义。

（用一个例子来解释）举个例子来说，如果要从文件里面读取数据并且发送到网络，那么在没有零拷贝技术的时候过程是：
零拷贝技术有很多种实现方式。一种实现方式就是 sendfile 这种，也就是我前面描述的那个过程。另外一种实现是所谓的内存映射文件，也就是 Memory Mapped File。这种实现方式是让应用通过读写一块虚拟内存来间接操作文件。

------------------------------

170. 为什么顺序写的性能那么好？, page url: https://www.mianshi.icu/question/detail?id=630
170-630-答案：
基础题。相信你肯定听说过顺序写的性能极好，但是一旦让你分析为什么顺序写的性能好，你就回答不出来了。

所以只要你能从操作系统的角度去解释清楚顺序写的特点，就能赢得竞争优势。
要想理解这个问题，你得先知道一个磁盘 IO 究竟是怎么写的。简单来说，当我们要往磁盘上写数据的时候，就如同一个老式的唱片机播放音乐：



简单来说就是两个步骤：
顺序写快的原因有很多。
当然，当下广泛使用的 SSD 也有类似的特性，但是原理上有些区别。这其中比较大的一个差异是虽然 SSD 也要寻址，但是没有机械硬盘那么慢，也不需要挪动磁头。

------------------------------

171. Kafka 的端到端压缩是怎么回事？, page url: https://www.mianshi.icu/question/detail?id=631
171-631-答案：
基础题，Kafka 面试里面有时候会问到，不算很高频。

你要在这个问题里面刷好亮点，有两个关键点：一个是指出端到端压缩和非端到端压缩的区别；另外一个则是指出你平时使用压缩技术解决性能问题的案例，最好是类似的端到端压缩技术的案例。两者结合就能赢得非常强的竞争优势。
先来看为什么我这里一定要加前缀端到端压缩，而不是直接说压缩。下面这个图就展示两者之间的区别：



所以假设在非端到端压缩的情况下，你可以注意到 Kafka 收到生产者发过来的数据需要解压缩，存起来；而后在投递给消费者的时候，再压缩。这就会增大 Kafka 的负担。
Kafka 的端到端压缩其实很简单，就是生产者会压缩数据，发送到 Kafka 上；而后 Kafka 直接存储压缩后的消息；等消费者消费的时候，Kafka 直接投递压缩后的数据。
Kafka 的这个设计精髓就在于端到端。因为常规的中间件虽然也会提供压缩方案，但是一般都不是端到端的。也就是说，中间件存储数据需要先解压缩，转发数据也需要重新压缩。在这种情况下，就是一个 CPU 换空间的解决思路。
通过压缩来提高性能也可以说是优化性能的常见方案了，只不过有些时候可能用不了端到端的压缩解决方案。我也用过类似的解决方案。

------------------------------

172. Kafka 为什么要设计分区？为什么不能直接一个 Topic 一个分区呢？, page url: https://www.mianshi.icu/question/detail?id=632
172-632-答案：
简单的题，不太常问，并且是典型的难者不会会者不难的问题。大多数人都知道 Kafka 是有很多分区的，但是很少有人会深入思考为什么要搞分区，毕竟我们已经有了 Topic 这个逻辑上和业务一一对应的东西了。

要在这个问题底下回答好，就要从为什么引入分区进一步讨论到合理的分区数量，而后又可以进一步打开话题讨论分区数量不足引起的消息积压之类的问题。总之这个问题问得非常好，抓住机会你就能装一波大的。
首先记住，分区就是分而治之的体现。

如果没有分区，也就是只有一个分区，很显然，单一的一个 Topic 肯定撑不住现在的这些并发量，因为一个分区只能放到一个 Kafka 服务器上，这个服务器的性能就决定了这个分区的性能，也就决定了这个 Topic 能够承载的并发的上限。

而很显然，引入了分区之后，不同分区可以分散到不同的 Kafka 节点上，那么理论上来说这些 Kafka 服务器加一起的性能上限，才是这个 Topic 的上限。
简单来说，引入分区就是为了分而治之，提高性能。

在 Kafka 里面，Topic 代表的是特定业务，或者说特定的业务场景。假定没有分区，或者说一个 Topic 只有一个分区，那么一个 Topic 就只能存放在一个 Kafka 服务器上。
所以说在考虑创建一个 Topic 的时候，分区数量是一个很重要的权衡点。分区数量过少，容易出现性能瓶颈，撑不住那么高的并发，又或者出现消息积压。当然分区数量也不是越多越好的，因为分区数量过多也会影响到 Kafka 的性能。
当然，类似的设计在别的中间件里面也能见到。比如说同样是消息队列，Kafka 里面叫做分区，在别的消息队列中间件里面就叫做队列，总之都是有类似的结构的。

------------------------------

173. 你了解 Kafka 吗？请你介绍一下 Kafka, page url: https://www.mianshi.icu/question/detail?id=633
173-633-答案：
简单题，很常见，尤其是在校招或者初级工程师的招聘里面。

这个问题的回答要点在于两个：一是介绍 Kafka 的基本概念，提及尽可能多的关键字，从而引导面试方向；另外一个则是要简单介绍一下自己使用 Kafka 解决过什么问题，最好就是用一个复杂方案中的 Kafka 来介绍，从而将话题引导到那个复杂的解决方案上。
这个问题可以说是入门问题，所以你只需要简单介绍一下 Kafka 的基本结构，而不需要深入讨论，可以等着面试官来追问。
在大部分的业务里面，Kafka 都是一款优秀的消息队列中间件

Kafka 整体上可以看做由三个主要部分组成：

从逻辑上来说，Kafka 设计了 Topic 来代表不同的业务场景，而一个 Topic 可以有多个 Partition（分区），每个分区至多只有一个消费者。一个 Topic 的不同分区会尽量分散到不同的 Broker 上去。
在实践中，我用 Kafka 解决过很多问题。

第一种用法就是用于解耦。比如说最典型的就是我在设计用户服务的时候，会根据用户的不同行为来生产不同的消息，比如说用户注册消息，开通会员消息等。下游可以根据自己的业务情况来订阅这些消息。
我还用过更加高级的用法的，比如说我就用 Kafka 设计过事件驱动架构，也就是系统的核心业务流程里面几乎没有同步调用，系统组件与组件之间的通信都是依赖于发送和消费消息来实现的。

------------------------------

174. Kafka 有什么缺点？, page url: https://www.mianshi.icu/question/detail?id=634
174-634-答案：
略难的题。其实这个题有歧义，有些面试官可能指的是 Kafka 自身有什么缺点，有些面试官可能是指你用 Kafka 来解决问题有什么缺点。

这个问题回答起来也不是特别难的，而且根据你回答的点，可以将话题引导过去不同的点上，从而刷出亮点。
你可以从以下选项里面随便挑一两个来回答，当然如果你全部记得那么全部回答出来会更好。
从我个人角度来说，我认为 Kafka 有三个缺点给我造成了很大的问题：
这三个问题如果单独处理倒也还好，但是有些情况下三个一起出现就很要命了。

------------------------------

175. 在 Kafka 中如何解决顺序消息的问题？, page url: https://www.mianshi.icu/question/detail?id=635
175-635-答案：
简单的题，高频考题。之所以说这个是简单的题，是因为问得太多以至于差不多人人都会了。

回答这个问题的要点在于区分清楚全局有序和单一业务内部有序这两种情况，而要刷出亮点，则要在顺序消息的基础上进一步讨论顺序消息积压的问题，或者说引出我给你准备的顺序消息积压的解决方案。
顺序消息是指消费顺序和生产顺序保持一致。也就是如果要是先生产了消息 A，后生产了消息 B，那么消费者一定要先消费 A，后消费 B。但是这种顺序有两种：全局有序和业务内部有序。

举个例子，假如有一个订单，先产生了消息 A1，再产生了消息 A2。而后还有一个订单，先产生了消息 B1，再产生了消息 B2。我们假定在真实环境下，这四条消息的顺序是 A1 B1 A2 B2。

如果是全局有序，那么要求消费者严格按照 A1 B1 A2 B2 的顺序来消费。
要解决有序消息的问题的，要看是需要全局有序还是需要业务内部有序。
但是这两个解决方案都是有一个隐患的，也就是消息积压的隐患。

------------------------------

176. Kafka 的分区是什么？有什么特点？, page url: https://www.mianshi.icu/question/detail?id=636
176-636-答案：
略难的题，不太常问。

回答这个问题的最大的关键点就是要指出 Kafka 同一个分区的消息是有序的，但是不同的分区的消息是无序的，从而将话题引导过去如何保证消息有序上，进一步引出你的解决方案。
这里你要始终注意一点，在不同的语义下，分区可能有不同的含义。有时候是指单一的一个分区，有时候是指由一个主分区和多个从分区组成的，逻辑上属于某个 Topic 的分区。

最好理解 Topic 和分区的关系就是把 Topic 看做是目录，而分区则是这个目录内部的文件。每次发送消息到 Kafka 上的时候，消息就是被追加到某个分区的末尾，也就是代表这个分区的文件的末尾。


分区可以看做是 Kafka 组织数据的基本单位。
这两个特定会带来一些比较棘手的问题。

------------------------------

177. 如果业务消息要求全局有序，但是消息又积压了，怎么办？, page url: https://www.mianshi.icu/question/detail?id=637
177-637-答案：
略难的题。不过这个问题很少面到，因为很少有面试官会用全局有序的消息模型。

记住，回答所有的有序消息的怎么解决的问题，你就从这个问题Kafka 中消息积压了怎么办？ (meoying.com)的解决方案里面挑选。

根据Kafka 中消息积压了怎么办？ (meoying.com)中的描述，你可以想到：
在要求全局有序的情况下，能做的事情就不多了。
那类似于降级，如果要是消费者本身还承担着业务功能，那么可以把业务功能停掉或者说把业务流量分发给别的节点；也可以专门部署一个节点，只用作消费者。

------------------------------

178. Kafka 中消息积压了怎么办？, page url: https://www.mianshi.icu/question/detail?id=638
178-638-答案：
略难的题，高频题目。

大部分人就只会回答异步消费什么的，但是这个方案属实有点烂大街了，毫无竞争力。你需要的做的是先区分是临时积压还是永久性积压。

回答这个问题的关键有两个，一个是要把你能记得的各种方案都讲一下；另外一个关键点是你要把话题引导过去顺序消息的消息积压解决方案上。
消息积压的话，首先你要区分是临时性积压还是永久性积压。

所谓的临时性积压是指你的消费者处理能力是足够的，但是因为突发流量的问题，导致消息积压了很多。但是从长期来看，这些消息会在接下来一段时间内被消费掉。这种情况下，你可能需要处理，也可能不需要处理，就看你对这种情况的容忍度。比如说我之前有一个消息积压问题，因为预期会在 24 小时内消费完，所以我就懒得处理。但是后面又遇到过一个类似的场景，这一次预期要用一个星期才能消费完，而业务上并不能等那么久，所以只能着手处理。

而永久性积压就是你的消费者消费能力不足，这种情况下积压的消息只会越来越多，所以你是必须要处理的。
解决消息积压的方案有很多。

首先，如果要是消费者的数量还没达到分区的数量，那么就可以直接增加消费者。这也是最简单，不需要任何改动的方法。

其次，如果要是消费者数量已经达到最大值了，但是消息依旧积压，那么这个时候可以考虑增加分区。如果增加分区不可行的话，那么就创建一个更多分区的新 Topic，把生产者流量切过去，再部署更多消费者。
降级的这个思路，还有一些变种做法。
还有一些涉及比较大的改造的解决方案。

第一种是在消费者这端批量消费，批量提交。比如说一次性拉 100 条消息，将数据合并在一起批量处理；

第二种是消费者这端异步处理，批量提交。一般没有批量处理接口的时候，就可以考虑使用这种策略；

------------------------------

179. 在使用顺序消息的时候，如果要是消息积压了怎么办？, page url: https://www.mianshi.icu/question/detail?id=639
179-639-答案：
略难的题。一般来说面试官至多问到如何解决消息积压，但是很少会问顺序消息积压。

解决的顺序消息积压的思路也解决一般消息积压的思路差不多，只不过其中有些手段不能用到顺序消息上。
根据Kafka 中消息积压了怎么办？ (meoying.com)中的描述，你可以想到：

首先，加消费者是可以用的，但是会有临界点的问题。假设说 A 一定要先于 B 消费，那么增加分区可能出现的就是引起 Kafka 的 rebalance，进而出现 A 被消费者1 消费，但是 B 被消费者 2 消费。


顺序消息的消息积压解决思路和一般的消息积压解决思路很像，但是要时刻注意会不会引起失序。

首先，如果还有消费者在消费多个分区，可以考虑增加消费者，达到每个消费者只消费一个分区。但是增加消费者的时候要注意临界状态，比如说 A 应该先于 B，然后增加分区前 A 在消费者 1 上消费，增加消费者后，B 在消费者 2 上消费，导致 B 比 A 先处理掉。但是实践中这种情况几乎不可能，因为增加消费者不是突然就能完成的事情，在这个过程中消息 A 不至于在消费者 1 上滞留那么久。
降级的这个思路，还有一些变种做法。
还有一些涉及比较大的改造的解决方案。

第一种是在消费者这端批量消费，批量提交。比如说一次性拉 100 条消息，将数据合并在一起批量处理。但是在顺序消息的场景下，只有批量接口也能保证顺序的前提下，才可以使用。

第二种是消费者这端异步处理，批量提交。一般没有批量处理接口的时候，就可以考虑使用这种策略。但是在异步处理的时候，也要引入额外的队列，确保说同一个业务的消息会被传递到同一个队列上，被同一个线程处理，保证消息的顺序。

------------------------------

180. 在你的顺序消息解决方案里面，如果按照业务ID 来选择分区，会有什么问题？怎么解决？, page url: https://www.mianshi.icu/question/detail?id=640
180-640-答案：
略难的题。大部分情况下，面试官都发现不了这个问题。

回答这个问题的思路也很简单，刷亮点则是要和哈希类的再平衡过程扯一起，并且结合 Redis 的策略来进行分析。
首先解释一下这个问题。

我在讲顺序消息解决方案的时候说道过，要确保业务内有序，就要保证同一个业务的消息一定发送到同一个分区上。但是业务之间热度是不同的，所以会造成分区之间 QPS，数据量不平衡的问题。

举个例子，假如说我们现在有一个 Topic 要保持有序，但是是对同一个商家保持有序。那么你可以想到，如果要是这个是一个大商家，就会产生天量的消息。例如说瑞幸咖啡这种入驻的商家产生了巨量消息之后，偏偏都要存在同一个分区里，这个分区就会出现消息积压。
在这种场景下，首先就要考虑优化消费者的性能了，这个跟具体业务有关，要具体分析。当然要是有钞能力的话，直接换更好的机器，更多的 CPU，更大的内存。
降级的这个思路，还有一些变种做法。
不过实际中，如果要是引起分区积压的原因是多个业务，那么可以考虑再平衡这些业务对应消息的分布。

举个例子，假如说三个大商家瑞幸咖啡，星巴克和库迪的消息是引起这个分区积压的关键，那么就可以调整生产者的选择分区的算法，将这三个商家的消息分散到不同的分区上，比如说一个大商家用一个分区。

这种思路其实就是解决于 Redis 中的槽设计与槽分配的思路了。比如说生产者可以根据业务 ID 计算一个哈希值，这个哈希值分成 0- 1023，而后指定某个段到某个分区，例如说 0-100 到分区0，100-300 到分区1。

------------------------------

181. Kafka 是如何保证高可用的？, page url: https://www.mianshi.icu/question/detail?id=641
181-641-答案：
略难的题，高频问题。

要刷亮点你就需要综合比较不同中间件的高可用方案，当然也没什么好比的，毕竟能够做到高可用的解决思路就那么几个。
简单来说，大部分中间件保证高可用就是通过对等结构 +  主从结构来完成的，目前现代化的中间件都是这种混合结构。

首先来看对等结构，在 Kafka 里面，每一个服务端实例都叫做 Broker（中间人），我一般就叫做 Kafka 服务器。那么正常的一个 Kakfa 的集群会有很多个 Broker，这些 Broker 地位是平等的，所以从这个角度上来说，Kafka 是一个对等集群。


Kafka 的高可用主要是通过三点来保障的。
这种设计也不仅仅是只有在 Kafka 上能够见到。

我们通常说的分库分表也可以看做是类似结构。比如说一个 User 表分成了 10 张表，那么这 10 张表地位平等，也就是组成了一个对等结构。显然如果其中一张表出问题了，其余 9 张表也能正常对外提供服务。

------------------------------

182. Kakfa 的分区是怎么进行数据同步的？, page url: https://www.mianshi.icu/question/detail?id=642
182-642-答案：
略难的题，不太常问。这个问题是指一个逻辑分区的主分区和从分区是怎么进行数据同步的。

说白了就是大部分的主从同步都是类似的，但是 Kafka 里面有一个特殊之处就是它引入了 ISR 的概念。所以你在回答的时候带出 ISR 这个概念，等待面试官追问就可以了。

此外还有一个可以引导的话题，就是数据同步对 acks=all 这个设置的影响。
一个 Topic 有很多逻辑分区，而一个逻辑分区由一个主分区和若干个从分区组成。主分区需要和从分区保持数据同步，这样万一主分区崩溃了，从从分区中选举出来的新的主分区，也很可能有最新的数据，也就是说数据没有丢失。

主分区和从分区之间的数据同步，同样的是 pull 模型。或者说，从分区就是将自己伪装成一个普通的消费者，定期拉取主分区上的数据到本地。

那么这里就会有一个关键问题：从分区可能不能及时拉走最新数据。原因可能有很多，比如说网络故障、自身性能太差。为了应付这种情况，Kafka 引入了一个 ISR（In Sync Replica）的概念，也就是说从分区分成两类，一类是跟上了节奏的，一类是没有跟上节奏的——即数据已经差太多了。
总体来说，Kafka 的主分区和从分区的数据同步采用的是拉模型。也就是说从分区类似于一个普通的消费者，定期从主分区里面拉取消息。
这种拉模型和 ISR 的设计，还和另外一个机制有关，也就是生产者发送消息的 acks 机制。
从中间件设计的角度来说，ISR 算是一个很新奇的设计了，毕竟在别的中间件里面找不到类似的概念。比如说在 MySQL 的主从同步里面我们就没什么 ISR 的说法，当然如果要是手动选择从库提升为主库，倒是可以人为选择数据最新的那个。

------------------------------

183. Kafka 使用的是 pull（拉）模型还是 push（推）模型？, page url: https://www.mianshi.icu/question/detail?id=643
183-643-答案：
简单的题，不算高频题目。

不同的中间件在 push 和 pull 模型中会做不同的决策，核心就是以谁为主。在这个问题之下，你要刷亮点就可以系统讨论 push 模型和 pull 模型在不同的中间件中的应用。而如果你要是设计过一些的复杂的中间件，也可以讨论自己设计的中间件使用的是 pull 模型还是 push 模型。
假设说我们现在有数据要从 A 传递到 B，那么：

其实你从消息队列要解决的问题上来看，也能想到 Kafka 这种用在解耦、削峰场景下的中间件，能做的选择也不多，那就是应该把便利留给用户，把难题留给自己。

首先是对于生产者来说，显然是应该用 push 模型。因为 Kafka 本身的性能极好，即便是生产者无脑发送，多半也不会触发什么性能瓶颈问题。Kafka 自己可以专注提高性能，用户就简单了，默认可以无脑发送。
Kafka 在生产者这一端用的是 push 模型，好处是生产者可以无脑发送消息，降低了 Kafka 的使用门槛。

而在消费者这一端，使用的是 pull 模型，好处是消费者可以根据自己的消费能力，主动拉取消息。
一般在考虑使用 pull 模型还是使用 push 模型的时候，核心就是以谁为主。这也导致在不同的中间件里面，采用的模型就不太一样。比如说在 Prometheus 里面，采用的就是 pull 模型，也就是 Prometheus 会主动去应用服务器上面拉数据。

------------------------------

184. Kafka  的 acks 参数有什么用？, page url: https://www.mianshi.icu/question/detail?id=644
184-644-答案：
简单题，你要是用过 Kafka 就差不多会知道这个东西。

在这个题目之下刷亮点，就是要把话题引导过去写入语义上。也就是当你写入数据到一个分布式中间件的时候，你是希望写入到主节点，还是也要写入到从节点。而这一类的机制，是很常见的，基本上涉及到主从结构的中间件都有这个问题。

另外一个刷亮点的，就是你要讨论 acks 这个参数虽然定义了分布式下的写入语义，但是并没有定义怎么刷盘，所以即便是在 acks=all 的场景下依旧有数据丢失的可能，只不过非常罕见而已。
你需要先背熟这一篇：在分布式环境下，写入数据的语义有哪些？ (meoying.com)

生产者在发送消息的时候，可以指定 acks 参数的值，它有三个：


acks 有三个值：
简单来说，就是如果对消息丢失容忍度比较高，并发也比较高的，可以考虑使用 acks = 0；如果要是对消息丢失完全无法容忍，并发也不是很高，那么就可以考虑 acks=all。其余情况就是用默认就好。
但是 acks 参数有一个缺陷，即 acks 完全没有讨论刷盘的问题。

------------------------------

185. 在分布式环境下，写入数据的语义有哪些？, page url: https://www.mianshi.icu/question/detail?id=645
185-645-答案：
难题。

很少有面试官会问这个问题，但是这个问题很重要，贯穿在大部分中间件的面试中。比如说讨论 MySQL 事务提交，Kafka 消息丢失等问题都会涉及到分布式环境下的写入语义问题。

所以你千万要记住这里讨论的问题，一旦你发现面试官问“数据会不会丢”，“写入到了哪里”你就用这部分内容来回答，保证可以刷出亮点。
写入语义要考虑两重，一重是节点内部的写入语义，一重是分布式环境下的语义。

先来看第一重语义。当我们说写入数据的时候，这个时候有三种可能：

这三重语义如图：
这个要分成两部分来说，先说第一部分，也就是单一节点的写入语义。当我们说写入数据的时候，这个时候有三种可能：
另外一部分是分布式语义下的，一般是主从结构。也就是说，在说写入的时候，有几种语义：
那么在分布式环境下的写入语义，就是要综合考虑这两重语义。

------------------------------

186. Kakfa 中的消费者组是什么？为什么要引入消费者组？, page url: https://www.mianshi.icu/question/detail?id=646
186-646-答案：
简单的题，在初中级工程师和校招面试中比较常见。

在这个题里面，你可以稍微提及消费者加入到消费者组中会引起 rebalance 的问题，但是不需要详细说，等着后续面试官追问。
消费者组和 Topic 一样，是一个划分业务的概念。也就是说，针对同一个 Topic，不同的业务方就归属于不同的消费者组。

------------------------------

187. Kafka 中的 rebalance 是指什么？, page url: https://www.mianshi.icu/question/detail?id=647
187-647-答案：
略难的题，这个题目在 Kafka 里面不太常问。

可以先回答 rebalance 的基本定义，而后指出常见的触发 rebalance 的场景，并且指出常规的避免方案，而后可以谈及自己处理 rebalance 的经历。
Kafka 的 rebalance（再平衡）指的是Kafka集群重新分配分区给消费者组内各个消费者的过程。这种机制确保了每个消费者能够从一个或多个分区中消费数据，从而维持了消费的均衡性和系统的高可用性。

其实四个字就能说明白：负载均衡。

就好比你是组长，你们组有十个任务，原本有三个人。那么不管是有一个人离职，还是有新人加入进来，你都得重新分配任务。这个就是 rebalance 的本质。
rebalance 指的 Kafka集群重新分配分区给消费者组内各个消费者的过程。这种机制确保了每个消费者能够从一个或多个分区中消费数据，从而维持了消费的均衡性和系统的高可用性。
rebalance 有几个触发时机：协调者发生变化，分区发生变化，消费者组成员发生变化。

其中最常见的就是消费者成员变化。而消费者的变化有些时候是正常的变化，有些时候是不正常的变化。比如说上线下线这种，其实是可以预期的，倒也无所谓。

但是有些时候，或者消费者节点有问题，或者网络有问题，也会导致误判这个消费者已经下线了，触发 rebalance。举个例子来说，在解决延迟消息的时候，一个解决方案就是分区设置不同的延迟时间，而后消费者根据消息的延迟时间来睡眠。在睡眠的时候，这个消费者就有可能被判定为已经崩溃从而引发 rebalance。
我之前还遇到过一个很有意思的 rebalance 案例。

在之前的项目中，我们的业务流量增长迅速，Kafka 消费者需要处理的消息量持续增加。这一过程中，我们观察到消费者频繁发生 rebalance，系统吞吐量明显下降，延迟增大。通过排查，我们发现 Java 的 Full GC 是导致问题的关键原因。

由于我们使用的 Kafka 消费者是基于 Java 构建的，随着业务量增长，应用的内存占用越来越大，导致频繁触发 Full GC。Full GC 过程中，应用线程被暂停，导致消费者无法及时向 Kafka 发送心跳，从而触发了 Kafka 的 rebalance 机制。频繁的 rebalance 使系统性能下降，并且增加了消息处理的延迟。

------------------------------

188. 当你收到消息发送成功响应的时候，消息就不会丢吗？, page url: https://www.mianshi.icu/question/detail?id=648
188-648-答案：
略难的题，有些面试官为了恶心人会故意这么问。

当然如果你已经很熟悉我几次强调的写入语义的问题，也很熟悉 acks 参数的意义，那么这就是送分题。刷亮点，还是要立足在讨论分布式环境下的写入语义。
我在很多问题下都说过，但凡问“xx 会不会丢”，答案只有一个就是必然会丢。

这个问题有点类似于Kafka 的 acks 参数有什么用？ (meoying.com)，但是面试官没有单纯问 acks，所以你就需要在回答完 acks 之后，根据这个问题在分布式环境下，写入数据的语义有哪些？ (meoying.com)中的思路去回答。
在使用 Kafka 的时候，收到发送成功的消息也还是会有丢失的可能，至于是怎么丢的，要看 acks 的取值究竟是什么。

如果 acks 取值 0，那么这时候丢失的概率就很大了。因为 acks 只是说把数据的发到发送缓冲区就算发送成功。那么后续不管是网络原因，还是生产者崩溃，那么 Kafka Broker 都可能压根没有收到这条消息。
其实 Kafka 这种数据会不会丢失，说白了就是分布式环境下的写入语义问题。

首先是针对单节点来说，不同的中间件有不同的做法，但是无非就是三种：
那么在分布式环境下的写入语义，就是要综合考虑这两重语义。

------------------------------

189. 在 MySQL 的分页查询中，如果要是偏移量很大了（深度分页），会有什么问题？, page url: https://www.mianshi.icu/question/detail?id=649
189-649-答案：
简单题，较为常面。

这个问题有一个很好的引导点，就是将话题引导过去分库分表的分页查询中。因为深度分页这个问题在分库分表里面遇到的性能问题更加严重，但是解决思路反而是类似的。
所谓的深度分页，就是指在数据库查询中分页查询的时候偏移量非常大的情况，比如说 LIMIT 10000, 10 这种。

这里就要解释一下 MySQL 的分页处理原则了。用查询


为例，给你解释一下MySQL 的处理步骤，它可以简化为：
深度分页最大的问题就是性能可能会很差。

以 LIMIT 10000, 10 为例，MySQL 执行分页查询的基本步骤可以简化为：
而解决思路有两种。
但是分页这个问题也不仅仅是出在单库单表上，在分库分表上这个问题更加严重。分库分表的情况下，深度分页会导致大量的数据传输到分库分表中间件上，而后分库分表中间件执行排序，最后选出最终的数据。

------------------------------

190. 你用 Redis 解决过什么问题？, page url: https://www.mianshi.icu/question/detail?id=650
190-650-答案：
基础题，高频题。一般可以看做是深入讨论 Redis 的前奏。

回答这个问题的关键要看你平时准备了 Redis 哪些案例。而后你在整个回答的过程中你就可以稍微提及这些案例，但是不需要阐述细节，等待面试官进一步追问。而且还要注意的是，你选择的案例不能太大众化，要比你当前求职的岗位稍微高级那么一点点。
核心是列举自己解决过的问题。这里我们准备一些很常见的场景，你可以自由挑选两三个：


需要注意的是，你选择案例应该考虑两个因素：
也就是，你可以在回答的时候给出三个回答，但是至少有一个是相比于你当下求职的岗位来说，是比较有技术含量的。
最典型的使用场景就是用 Redis 来作为缓存，一般来说根据缓存的数据不同，可以是缓存字符串，或者是 List 结构、哈希结构。一般来说引入 Redis 作为缓存之后，性能会有数量级的提升。
我还用 Redis 处理过更加复杂的问题，比如说榜单问题。这个榜单问题的核心就是利用 ZSet 来排序，对应的 score 则是根据排序规则来计算。比如说根据时间、点赞和评论数等计算一个热度，把这个热度作为 score。每次获取前几名，也就是执行一次 Range 而已。
我还用 Redis 来设计过分布式锁。使用 Redis 来设计分布式锁的关键点有两个：使用 SETNX 来排他性的设置一个值；要考虑续约的问题。也就是在设置了一定的过期时间之后，如果在快要过期了业务还没执行完毕，那么要考虑续约的问题，防止分布式锁因为过期而失效。

------------------------------

191. 你为什么要用 Redis？, page url: https://www.mianshi.icu/question/detail?id=651
191-651-答案：
简单题。

这个问题其实稍微有点考验你的反应能力。因为正常我们会认为使用 Redis 是一件不言自明的事情——也就是大家都这么用，我也用。所以要是突然问一下你为什么用，你就会懵逼了。

回答这个问题的要点是要举几个例子证明没有别的中间件能做得比 Redis 更好了，而且最好是选择你项目中的、有特色的案例，从而引导话题并且刷出亮点。
其实这个问题，一句话就能够说清楚，Redis 是一个高性能、数据结构丰富、能支撑高并发的中间件。
由于 Redis 拥有卓越的高性能和高可用性，它已成为提升应用性能的优选工具。作为一个完全基于内存的存储系统，Redis 通常被用作缓存，以加速数据访问并显著减轻核心数据库的负载。
就目前来说，能和 Redis 匹配的竞品几乎没有。

我举几个离开了 Redis 就几乎没有啥好解决方案的场景。第一个是高并发下的榜单问题，如果要考虑实时计算或者近实时计算，那么 Redis 就是最好的选择。

------------------------------

192. 在异步消费，批量提交方案中，为什么你要批量提交而不是异步消费完就直接提交？, page url: https://www.mianshi.icu/question/detail?id=653
192-653-答案：
略难的题。如果你实践中没有真的落地过这个解决方案的话，你这里大概都不懂面试官问的是什么意思。

这个问题归根节点就是一句话，处理消息可能失败，所以就要考虑到如果异步消费偏移量较小的消息失败了，那么即便偏移量较大的消息即便消费成功了也不能提交，否则后面就没办法再次重新消费失败的消息。

这个问题，面试官的意思是，既然都异步消费了，为什么不异步提交呢？也就是说面试官的疑问是为什么代码不写成这个样子？

而异步消费批量提交的代码你可以在案例库里面找到：interview-cases/case1_10/case8 at main · meoying/interview-cases (github.com)。它的代码可以看做是这个模式：

你看到两者之间的区别了吗？实际上批量提交是一个不得已而为之的手段。我举一个例子你就明白了。假设说现在我们异步消费了两条消息，偏移量为 5 的 msgA 和偏移量为 6 msgB。
批量提交是为了解决异步消费失败的问题而引入的措施。

举个例子来说，假如说我们消费偏移量为 5 的 msgA 和偏移量为 6 的 msgB，并且假设消费 msgA 失败了，但是消费 msgB 成功了。那么如果要是异步消费直接提交的话，就相当于我们提交了偏移量为 6 的消息。这时候 Kafka 就会认为所有偏移量小于等于 6 的消息都消费成功了。

------------------------------

193. 在你的消息积压解决方案里面，如果凑不够一批怎么办？, page url: https://www.mianshi.icu/question/detail?id=654
193-654-答案：
略难的题。还是那句话，如果你要是实践中没有落地过，就有可能寄了。

这个问题有一个非常好的地方，就是你可以借助它将话题引导过去单个转批量这种常规的性能优化方案上。也就是说你在解决消息积压的时候要考虑凑不够一批的问题，那么你在设计单个转批量的性能优化方案的时候，同样要考虑凑不够一批怎么办。
在消息积压的两个主要解决方案里面，有多个方案都涉及到凑够一批，使用批量接口来优化性能。比如说异步消费-批量提交，批量消费-批量提交，生产者聚合消息。

这几种方案都有一个问题要考虑：就是凑不够一批怎么办。

举个例子来说，在批量消费-批量提交这个解决方案里面，假设说我们一批是 10 条数据，但是我 Kafka 上只有 5 条，你怎么办？你要等吗？如果等一分钟都没凑够 10 条，你还要等吗？
我凑一批不是说一直等直到凑够一批。
这个问题也不仅仅是我这里才有的。

------------------------------

194. 你知道负载均衡吗?, page url: https://www.mianshi.icu/question/detail?id=655
194-655-答案：
简单题, 基本概念题, 在各层级的程序员面试中都很常见，尤其是在微服务架构有关的面试中更加常见。这个问题也是属于非常宽泛的问题，所以你在回答的时候要尽可能做到提及各个点，从而引导面试官进一步追问。

在这个问题之下刷亮点，一个是讨论微服务架构下负载均衡算法和容错的问题，另外一个总结拔高负载均衡在各种中间件的体现，将话题引导过去不同的中间件上。
负载均衡是一种分布式系统技术，它的主要目的是将工作负载（如网络请求、计算任务等）均匀地分配到多个处理节点上，以优化资源的使用、提高系统的吞吐量和可靠性，以及改善用户的响应时间。

简单来说，负载均衡的本质就是为了找出最适合的节点。

你在回答这个问题的时候可以考虑从以下角度回答：
负载均衡是一种分布式系统技术，它的主要目的是将工作负载（如网络请求、计算任务等）均匀地分配到多个处理节点上，以优化资源的使用、提高系统的吞吐量和可靠性，以及改善用户的响应时间。

从实现的角度来说，可以分成软件负载均衡和硬件负载均衡。不过一般来说软件负载均衡应用更加广泛，算法也更加多样。而硬件负载均衡主要是在追求极高性能的场景下才会考虑使用。
在实际应用中，负载均衡可能需要根据具体的业务场景和性能需求选择合适的负载均衡方案，并结合服务发现、健康检查、故障转移、容错等机制才能取得良好的效果。
大多数人在考虑负载均衡的时候，只会考虑到针对请求的负载均衡。而实际上负载均衡的含义不仅在于此。

仅仅从负载均衡针对的对象上来说，虽然大多数负载均衡针对的都是 QPS，但是实际上也会有一些特殊的场景负载均衡针对的是数据量等其他因素。

而且负载均衡也不是只出现在类似于 nginx 或者微服务架构里面，很多地方都能看到类似的设计。比如说我在设计分布式任务调度系统的时候，就是要考虑将任务调度到负载最低的节点上，本质上也是一个负载均衡过程。

------------------------------

195. 你了解哪些负载均衡算法?, page url: https://www.mianshi.icu/question/detail?id=656
195-656-答案：
简单题, 基本概念题，在校招和初中级岗位上比较常见，在求职高级岗位的时候，一般是和可用性之类的话题混在一起面。

要想在这个问题之下，要想刷亮点的话，就不能只回答轮询这种人尽皆知的算法，至少也要回答出来最少连接数之类的算法。而要想实现必胜一击，还是得有自己设计的骚气的负载均衡算法。
先将负载均衡算法分为“动静”两大类， 然后再分别阐述每种负载均衡算法的原理、优缺点及适用场景。

常见的负载均衡算法大致可以分为两大类:
在我的理解中，常见的负载均衡算法大致可以分为两大类：静态算法和动态算法。

先说说静态算法吧，这类算法不考虑服务器的实时负载情况，权重是预先设定的。比较常见的有轮询、加权轮询、平滑加权轮询、源IP哈希和一致性哈希。

轮询就是轮流分配请求给每台服务器。简单公平是它的优点，但缺点是不考虑服务器的实际负载。它适合那些服务器性能相近，而且请求处理时间比较短的场景。
这些算法各有优缺点，选择时需要根据具体的应用场景和需求来决定。例如，如果服务器性能相近，可以选择简单的轮询算法；如果服务器性能差异大，可以考虑加权算法；如果需要会话保持，可以使用源IP哈希算法；如果应用场景对响应时间非常敏感，可以选择最快响应时间算法。
而不管是什么负载均衡算法，都有一个绕不开的核心难点，即怎么准确衡量"负载"这个概念。不同的算法对"负载"的理解和衡量方式都不太一样。

比如说，轮询算法就简单地认为所有服务器的处理能力相同，就轮流给它们分配请求。加权轮询呢，就稍微聪明一点，它知道服务器之间处理能力可能有差异，所以会给能力强的服务器多分一些请求。

再比如，最少连接算法就认为，谁的连接数最少，谁就最闲。而最快响应时间算法则觉得，谁曾经的响应最快，谁的负载就最轻。

------------------------------

196. 四层负载均衡和七层负载均衡的优缺点是什么？, page url: https://www.mianshi.icu/question/detail?id=657
196-657-答案：
简单题, 基本概念题，校招和初中级工程师面试中常见。

你可以先简单罗列他们的优缺点，而后可以尝试总结一下选择四层负载均衡和七层负载均衡的总体原则。
先分别回答什么是四层负载均衡？、什么是七层负载均衡？再对比它们的优缺点，
四层负载均衡通常指根据IP地址和端口号等信息对请求进行转发。因只用到了OSI网络模型中二、三、四层协议中的内容所以，所以它具有分发效率高，处理速度快、消耗资源少，可以处理任何基于TCP/UDP的协议等优点，缺点则是无法根据请求内容进行更智能的负载均衡。
总体来说，我认为选择四层负载均衡还是选择七层负载均衡的原则有几条。

第一条是看性能要求。如果性能要求极高，那么七层负载均衡肯定不如四层负载均衡。例如说大部分系统的边缘部分，或者流量入口部分，都是使用四层负载均衡，图的就是性能。

------------------------------

197. 在 Go 里面，调用 runtime.GC() 一定触发 GC 吗？, page url: https://www.mianshi.icu/question/detail?id=658
197-658-答案：
简单题。

其实答案就是一定会触发 GC。这个题目你可以理解为针对 JAVA 背景的程序员的精准狙击，因为在 JAVA 里面也有一个类似的方法，但是 JAVA 却并不一定执行垃圾回收。
回答这个点，你要是 JAVA 背景的，你就可以补充说明 JAVA 里面的 System.GC() 明面上看上触发垃圾回收，而实际上，是否真的触发了垃圾回收和 JVM 有关，和垃圾回收器也有关。主流的实现都是把这个调用当做放屁。
在 Go 里面，调用这个方法会真的触发垃圾回收。不过在实践上来说，我是不建议使用这个方法手动触发垃圾回收的。

------------------------------

198. 负载均衡是如何提升系统的性能、可用性及可扩展性的?, page url: https://www.mianshi.icu/question/detail?id=660
198-660-答案：
略难的题，这个问题其实不太常见，因为大部分时候面试官只会关注你是否了解负载均衡算法，而不会考察你是否理解负载均衡的意义。

回答这个问题最佳的刷亮点的策略就是结合自己设计的负载均衡算法来论证提升了性能、可用性和可扩展性，从而将话题引导到自己设计的负载均衡算法上。
负载均衡提高性能、可用性和可扩展性的。
其实很简单。

首先就性能来说，因为负载均衡会尽可能挑选出来负载比较低的节点，负载低的节点处理请求就会更快，也就是带来更好的性能。这种提升一方面是响应时间比较短，另外一方面则是吞吐量也会比较大。
举个例子来说，我曾经在业务里面设计过一个比较复杂的负载均衡算法。

它是动态调整权重的负载均衡算法的一个变种，它综合考虑超时、服务端限流、服务端降级、网络异常等多种问题，动态调整不同节点的权重，从而控制流向该节点的流量。

------------------------------

199. 为什么要引入虚拟内存空间而不是直接使用物理内存？, page url: https://www.mianshi.icu/question/detail?id=661
199-661-答案：
简单题，在校招和初中级岗位里面非常常见。

这个问题其实有一点猥琐，它就是我说的那种反直觉问题。也就是正常你都是知道有虚拟内存，但是比较少去反思为什么要有虚拟内存。所以如果你平时没有准备，就比较容易寄了。

回答这个问题你可以从虚拟内存这种设计的引申到一般性的增加中间层的设计理念上刷亮点。
我用我自己的理解来帮助你理解这个问题。

首先我们可以说，如果要是只有一个进程，那么其实虚拟内存搞不搞意义都不大，因为这个内存是你独享的。而一旦你要搞多进程了，那么问题就来了，你只有一个物理内存。很显然你的做法可以考虑，将物理内存平分，每个进程一块。但是问题在于，你不知道有多少个进程以及将来会有多少个进程，而每个进程需要的内存也有多有少，所以这个思路是搞不了的。

那按照我们程序员说的，遇事不决加中间层的理念，显然就是加一个中间层，这个中间层就是虚拟内存。也就是每个进程我都给你一个虚拟内存空间，让你以为你独享整个内存。
首先，如果系统中只有一个进程，那么物理内存直接分配给它使用就可以了。但是，在多进程的环境中，我们面临一个物理内存共享的问题。简单地平分物理内存给每个进程是不现实的，因为进程的数量和它们的内存需求是动态变化的。

为了解决这个问题，操作系统引入了虚拟内存的概念。虚拟内存为每个进程提供了一个假想的、连续的内存空间，即虚拟地址空间。进程认为自己独享整个内存，而实际上，操作系统会在进程需要时动态地分配物理内存给它。
在计算机系统中，类似的设计理念也被应用于其他领域。例如，虚拟文件系统（VFS）就是这样一个设计。VFS是操作系统文件管理的一个抽象层，它为用户和应用程序提供了一个统一的文件操作接口，而不管底层存储设备的实际类型是什么。这样，不同的文件系统实现（如EXT4、NTFS、FAT32等）可以在操作系统中共存，用户和应用程序不需要关心文件实际上是如何存储的。

------------------------------

200. 你知道什么硬件负载均衡和软件负载均衡的区别吗？, page url: https://www.mianshi.icu/question/detail?id=662
200-662-答案：
简单题。

这个题目当你还是初中级工程师的时候就只有一点理论意义，基本上用不到。当你后面成长为高级工程师独当一面的时候，需要为公司设计复杂系统的时候，就可能有机会要综合考虑在系统中引入何种负载均衡。
在什么是硬件负载均衡(meoying.com)和什么是软件负载均衡(meoying.com)你已经见到了详细的描述，这里再次额外总结一下软件负载均衡和硬件负载均衡的区别。
在业界，我们通常把有负载均衡功能的组件叫做负载均衡器。这些负载均衡器大致可以分成两类：硬件的和软件的。

这两种负载均衡器有几个主要的区别。

首先是成本，硬件负载均衡器通常造价比较高，而软件负载均衡器就便宜很多。

------------------------------

201. 你知道DNS负载均衡吗?, page url: https://www.mianshi.icu/question/detail?id=663
201-663-答案：
简单题, 基本概念题，校招和初中级岗位比较常见。有些时候，在面试大规模分布式架构的场景下，也会讨论 DNS 负载均衡的问题。

在这个问题之下刷亮点，可以讨论 DNS 负载均衡的弊端，以及在微服务架构下的影响。
DNS 整体就是一个非常热门的面试点。

DNS 负载均衡简单来说就是你每次去访问 DNS 的时候，DNS 会执行负载均衡策略，返回 IP 给你。举个例子，假设说 DNS 使用轮询作为负载均衡算法，某个域名对应的机器有三个 ABC，那么你第一次去问，那么会返回 A 的 IP，第二次是返回 B 的 IP，第三次则是 C 的 IP。

问题在哪里呢？在客户端缓存，也就是我客户端在第一次问了 DNS 拿到 A 的 IP 之后，我会缓存下来。我可能缓存个几分钟，那么在这几分钟内，我都是用同一个 IP。显然这段时间我的请求都是落到 A 上面的，A 的负载就会比 B 和 C 大很多。如下图：
DNS负载均衡是一种利用域名系统（DNS）来分散网络流量的方法。它的工作原理大致是这样的：

DNS负载均衡实现简单，不需要额外的硬件。但是，DNS负载均衡也有一些局限性：
DNS负载均衡虽然常用，但确实存在一些问题，比如：客户端DNS缓存问题，会导致负载分配不均和故障转移延迟。

说到负载分配不均，是因为客户端解析域名后会在本地缓存结果。这可能导致大量用户在一段时间内都访问同一个服务器IP，造成某些服务器特别忙，而其他服务器却很闲。

再说故障转移延迟，就是当某个服务器出问题时，因为DNS缓存的存在，用户可能会一直尝试连接这个不能用的IP，直到缓存过期。这就会让故障的影响时间变长。

------------------------------

202. 你知道LVS吗?, page url: https://www.mianshi.icu/question/detail?id=664
202-664-答案：
略难的题，一般来说针对普通的业务研发很少考察这方面的内容。但是如果公司需要的是全干工程师，或者你应聘的岗位是偏向运维的，那么这个问题就很常见。

首先了解以下缩写含义:


NAT (Network Address Translation) 即网络地址转换，包括 SNAT (源地址转换) 与 DNAT (目标地址转换). 它先修改请求包的目标 IP 地址 (同时可能会修改目标端口) 为选中的某台 Real Server 的 RIP 地址然后再转发。在请求与响应过程中期间，无论是进来的流量，还是出去的流量，都必须经过 LVS 负载均衡器.

LVS是Linux Virtual Server的缩写，是一个跑在Linux内核上的开源负载均衡解决方案。它主要是做四层负载均衡的，就是通过IP地址和TCP/UDP端口来分发流量。

LVS主要由两部分组成：一个是ipvs，这是在内核里跑的模块，负责实际的负载均衡；另一个是ipvsadm，这是我们用来配置ipvs的工具，就像是遥控器一样。

LVS有三种主要的转发模式，每种模式都有它的特点。

------------------------------

203. 什么是四层负载均衡?, page url: https://www.mianshi.icu/question/detail?id=666
203-666-答案：
简单题，基本概念题，在校招和初中级岗位中常见。

在这个地方刷亮点，可以揭露出四层负载均衡的实现细节，并且进一步讨论有状态和无状态服务对四层负载均衡的影响。
坦白来说，四层负载均衡其实跟一般的后端研发没什么关系，但是不知道为啥有一段时间这个问题热门过一段时间。

而实际上，最佳的面试方式其实是在 OSI 网络模型有关问题里面主动提起四层负载均衡，所以你最好先学习 OSI 网络模型之后再来看这个问题。

四层负载均衡就是发生在传输层的负载均衡措施，也就是说，根据 IP + 端口来运作。不过在实践中，所有发生在二、三、四层的负载均衡我们都叫做四层负载均衡。
四层负载均衡通常指根据IP地址和端口号等信息对请求进行转发。

因只用到了OSI网络模型中二、三、四层协议中的内容所以，所以它具有分发效率高，处理速度快、消耗资源少，可以处理任何基于TCP/UDP的协议等优点，缺点则是无法根据请求内容进行更智能的负载均衡。如果想要根据请求内容进行进行负载均衡，七层负载均衡更加合适。
从实现的角度来说，四层负载均衡根据负载均衡算法筛选出目标节点后，就会把报文里面的目标 IP 和端口修改为目标节点的 IP 和端口。如果是利用 NAT 机制，那么它也会修改源 IP，这样目标节点就以为是负载均衡器发过来的请求，也可能不会修改源 IP，也就是直接转发。

对于无状态服务来说， 四层负载均衡是十分合适的。 因为它不需要考虑请求之间的关联性， 只需要选取恰当负载均衡算法，尽可能高效地将请求分发到后端服务器即可。

而对于有状态服务来说， 四层负载均衡只能看到IP地址和端口号，对于应用层的信息，比如用户登录状态、会话信息等一无所知。如果直接使用， 就可能会将同一个用户的请求分发到不同的服务器上， 导致应用出现异常。 为了解决这个问题， 可以尝试以下几种策略:

------------------------------

204. 什么是七层负载均衡?, page url: https://www.mianshi.icu/question/detail?id=667
204-667-答案：
简单题，基本概念题，在校招和初中级岗位中比较常见。

七层负载均衡中最大的一个分支，就是我们平时用的微服务下的那一堆花里胡哨的负载均衡策略，所以你可以尝试将话题引导过去微服务负载均衡、网关负载均衡这些话题上。

另外在这个问题之下，有一个非常好刷的亮点就是有状态服务和会话保持，你可以从这里将话题引申到不同的会话保持技术上。
从面试策略上来说，你可以在回答 OIS 网络模型的时候主动提起七层负载均衡的问题。

七层负载均衡通常指根据应用层协议的内容（HTTP头、URL、Cookie等）对请求进行转发。七层负载均衡的工作原理大致如下：

其实简单来说就是抽取出请求内容，而后根据请求内容来执行负载均衡。
七层负载均衡通常指根据应用层协议的内容对请求进行转发。

整个过程比较简单。首先，负载均衡器接收到客户端的请求后，解析应用层协议的内容，如HTTP请求的头部信息、URL等。而后据设定的负载均衡算法以及解析信息选择一个合适的服务器，将请求转发过去。

因只用到了OSI网络模型中五到七层协议中的内容，所以它可以根据请求内容进行负载均衡，支持更复杂的负载均衡算法和功能，如基于内容的路由、会话保持等。但相应地，它的资源消耗较大，处理速度相对较慢，分发效率不如四层负载均衡高。
七层负载均衡处理有状态服务上面有独到的优势，例如说解决所谓的粘性会话的问题。

因为七层负载均衡能理解HTTP这样的应用层协议，所以可以根据请求的内容，比如URL啊、HTTP头啊或者Cookie啊来分配请求。这些信息还可以用来追踪用户会话，这样其实就能实现粘性会话了。
更进一步说，微服务负载均衡是七层负载均衡的一种升级版。它不仅实现了七层负载均衡的基本功能，还添加了一些新特性，以适应微服务架构的特点。

首先，七层负载均衡主要是处理请求的转发，就是把用户的请求分配到不同的服务器上。比如说，一个传统的电商网站，可能会用Nginx做负载均衡，根据URL把请求分发到不同的应用服务器上。

但是在微服务架构中，情况就复杂多了。除了请求转发，微服务负载均衡还需要处理很多其他的事情。

------------------------------

205. 四层负载均衡有什么优缺点？, page url: https://www.mianshi.icu/question/detail?id=668
205-668-答案：
简答题，基本概念题，在校招和初中级岗位中比较常见。

实际上，你的最佳面试策略应该是在讨论 OSI 模型的时候直接挑起四层负载均衡，又或者你是面试比较高级的岗位的时候，提到四层负载均衡来进一步优化架构，提高性能和并发。
因四层负载均衡只用到了OSI网络模型中二、三、四层协议中的内容所以，所以它具有分发效率高，处理速度快、消耗资源少，可以处理任何基于TCP/UDP的协议等优点。

------------------------------

206. 七层负载均衡有什么优缺点？, page url: https://www.mianshi.icu/question/detail?id=669
206-669-答案：
简单题，基本概念题，在校招和初中级岗位里面比较常见。

如果你了解七层负载均衡的概念，你就能轻易回答出来这个问题。你可以通过列举一些七层负载均衡的例子，来将话题引导到别的地方。
你要先看这个问题什么是七层负载均衡? (meoying.com)
因七层负载均衡只用到了OSI网络模型中五到七层协议中的内容，所以它可以根据请求内容进行负载均衡，支持更复杂的负载均衡算法和功能，如基于内容的路由、会话保持等。
七层负载均衡能够理解HTTP、HTTPS这些应用层协议，这就让我们可以根据请求的具体内容来制定更灵活、更智能的负载均衡策略，从而提高整个系统的性能、可用性和用户体验。我来举几个例子：

比如说，可以根据URL的不同部分来分发请求。像是所有"/product"开头的请求就发到产品服务器，"/order"开头的就发到订单服务器，这样就能让专门的服务器处理专门的业务，这也是 nginx 最常用的一个场景。

再比如，可以根据HTTP请求的方法来区分读写操作。GET和HEAD请求通常是读操作，可以把它们发送到专门优化过的只读服务器。而POST、PUT、DELETE这些写操作，就发送到支持写入的服务器。这样就能更好地优化资源利用。

------------------------------

207. 什么是进程？, page url: https://www.mianshi.icu/question/detail?id=670
207-670-答案：
简单题，高频题，在校招的时候尤其常见，初级工程师也有可能遇到，中级工程师以上就不常见了。
进程是操作系统中的一个核心概念，它代表了一个正在运行的程序的实例。进程不仅包括程序的代码和数据，还包括程序执行时相关的系统资源，如打开的文件描述符、安全属性、处理器状态等。进程是操作系统进行资源分配和调度的基本单位。

------------------------------

208. 什么是线程？, page url: https://www.mianshi.icu/question/detail?id=671
208-671-答案：
简单题，概念题。在校招、初级工程中比较常见，再往上就比较少问到了。

在回答了这个问题之后，你可以提及多线程并发编程的难度，这样有可能将话题引导过去多线程编程有关的问题上。也可以进一步塔伦为什么在一些现代编程语言里面又引入了协程的概念。
如果你面试的是特定语言的岗位，比如说 JAVA，那么被问到的概率就会更加大，这代表面试官可能想从这个角度深入探讨 JAVA 的线程模型。

线程是操作系统能够进行 CPU 调度的最小执行单位，它是进程中的一个实体，被系统独立调度和分派的基本单位。线程自身基本上不拥有系统资源，只拥有一点在运行中必不可少的资源（如程序计数器、一组寄存器和栈），但它可以与同属一个进程的其他线程共享进程所拥有的全部资源。
简而言之，线程是CPU调度的基本单位，作为进程的一部分存在。一个进程可包含多个线程，它们共享进程资源如文件描述符，但保持独立的执行栈和程序计数器。作为轻量级进程，线程在资源消耗、创建和销毁速度上优于进程，尤其在多处理器系统中表现卓越。
我从个人感觉上来说，多线程编程是一件难度很高的事情，很容易出现并发问题。例如说最常见的多个线程访问同一个资源，就有可能出现忘记加锁或者死锁等问题。而如果涉及多个线程并发执行、同步执行，那么难度就更加高了。

------------------------------

209. kafka基于分区的方案实现延迟消息，如何解决rebalance问题？, page url: https://www.mianshi.icu/question/detail?id=672
209-672-答案：
略难的题。

回答这个问题你要刷亮点，可以先讨论在分区设置不同延迟消息的解决方案下，如何解决 rebalance 问题，而后进一步阐述常规的一般的规避 rebalance 的解决思路。
有关 rebalance 的问题，你可以先参考这个问题Kafka 中的 rebalance 是指什么？ (meoying.com)

出现这个问题的原因是消费者睡眠了，睡眠期间不会消费消息，所以 Kafka 就会判定这个消费者已经崩溃了，就会触发 rebalance。

发生 rebalance 之后，等消费者再恢复过来，就不知道又会被分配到哪个分区，那么之前的睡眠就可以认为是白睡了。
在睡眠之前，我们可以暂停消费，并在恢复后继续消费。

------------------------------

210. Kafka基于分区的方案实现延迟消息，如何解决一致性问题？, page url: https://www.mianshi.icu/question/detail?id=673
210-673-答案：
分区方案的衍生问题，在分区方案中最后睡眠完了，怎么处理消息。是先提交消息呢，还是先转发呢？

你记住一个原则，就是在 Kafka 里面，但凡涉及到什么消息丢失，一致性之类的问题，最终都是要靠消费者幂等来解决的。

先提交再转发，在转发的过程中服务器宕机，由于消息已经提交，而 Kafka不会重发会导致消息丢失 。



如果先转发再提交，提交之前服务器宕机了，后面恢复，由于没有提交 Kafka会再转发一次。业务就有可能获取多次消息，这就要求业务消费者确保自己的逻辑是幂等。
选择“先提交再转发”策略可能会面临严重的问题。如果在消息提交后，转发过程中服务器崩溃，那么消息虽然被标记为已处理，实际上业务并未执行，导致消息丢失。这种情况是不可接受的，因为会直接影响业务数据的完整性。

------------------------------

211. Kafka如何实现延迟消息？, page url: https://www.mianshi.icu/question/detail?id=674
211-674-答案：
略难的题，Kafka 中的高频面试题，校招和初级都不太常见，中级比较常见。

这个问题之所以常见，核心就在于延迟消息作为业务场景很常见，偏偏 Kafka 却不支持，然而很多公司的消息中间件都是 Kafka，在这种情况下他们就会考察你相应的解决方案。

这里提及的主要的两个解决方案，从技术含量上来说应该是随机延迟消息含量高，所以你可以用这个方案来作为亮点，并且进一步引导话题。
主要掌握两种常见的kafka支持延迟消息的方案。
方案一：通过在一个topic中设置不同的分区，每个分区代表一个延迟时间，消费者根据不同分区的延迟时间进行睡眠，睡眠完后再将消息转发到不同的业务topic

可以创建一个带有多个分区的 Kafka Topic，每个分区对应不同的延迟时间。比如说可以设置分区 0 睡眠 1 分钟，分区 1 睡眠 5 分钟。

生产者根据消息的延迟需求，发送到特定分区，例如 1 分钟的消息发送到分区 0，5 分钟的消息发送到分区 1 等。
如果要支持随机延迟时间，就要使用到第二个方案，转储。

首先，创建了一个名为 delay_topic 的消息队列，所有需要延迟处理的消息都会存入该队列。
在高并发、大量写入的场景下，MySQL 容易成为性能瓶颈。

为了优化写入性能，较为简单的方式是使用分区表，根据时间进行分区，定期清理历史数据分区。

------------------------------

212. Kafka 在高并发、大数据量下如何支持随机延迟时间？, page url: https://www.mianshi.icu/question/detail?id=675
212-675-答案：
难题，校招、初中级岗位基本不会遇到，遇到了答不出来也无所谓。

坦白来说你遇到的 1000 个面试官里面可能只有 1 个真的解决过这个问题，剩下的都是在装逼。你只要把方案说出来，以及对应的各种变种说出来就能赢得极大的竞争优势了。

你可以先看这个问题：Kafka 如何支持随机延迟时间？ (meoying.com)，里面提到了基本的解决方案。



具体来说，延迟消息发送到 delay_topic ，然后延迟消费者将消息转储到 MySQL 中，然后有一个延迟消息发送者，定期查找到时间的延迟消息，并且将其转发到对应的业务 Topic。
在 Kafka 里面，要支持随机延迟消息，可以考虑使用 MySQL 转储的方案。

具体来说，就是业务方将延迟消息都发送到 delay_topic 上，而后延迟消息消费者将 delay_topic 上的消息转储到 MySQL。而后启动一个延迟消息发送者，定时轮序 MySQL，找到已经满足延迟时间的消息发送出去。


这个方案的性能瓶颈就是 MySQL。所以在大数据高并发的场景下，就要考虑使用分库分表了。在分库分表之后，每一张表都要有一个延迟消息发送者轮询查找到期消息。
如果要是考虑消息有序性，那么就要保证要求的有序的几条消息，都落到同一张表里面。
这些方案还有很多变种。比如说在按照业务 Topic 分表的时候，也不一定非得一个 Topic 一张表。可以考虑一些复合策略，例如说数据量不多的 Topic 就可以多个合并在一起共享一张表；而如果某个业务 Topic 的数据量极大，并发极高，那么就可以一个 Topic 分好几张表，例如说 order_cancel 对应的延迟消息转储表就可以分成多个，delay_order_cancel_0，delay_order_cancel_1 这种。业务方可以通过管理界面来配置是否使用独立的表，以及是否进一步分库分表。

------------------------------

213. 什么是硬件负载均衡？, page url: https://www.mianshi.icu/question/detail?id=677
213-677-答案：
简单题，题目就是答案，不怎么常见的面试题。

其实使用硬件负载均衡，只有一个原因，就是寻求更加高的性能。但是显然我们这种研发平时比较少接触到硬件负载均衡，所以你稍微回答一下就要把话题引导过去软件负载均衡上。

毕竟要在自己熟悉的领域上才好装逼。
硬件负载均衡就是使用硬件来处理负载均衡问题，而之所以要用硬件，其实就是为了高性能。高性能源自两方面，一个是硬件是专门部署的，不需要做别的事情，而且有专门的优化；另外一个就是硬件负载均衡一般工作在 OSI 中比较低的层次，所以性能损耗更少。

而硬件的可靠性也很好，通常它们都会有很多很复杂的容错和冗余机制，可以确保在各种极端情况下依旧能够正常提供服务。
硬件负载均衡是通过专门设计的硬件设备来实现网络流量和应用请求的分发和管理。这些设备我们通常称之为负载均衡器。它们是独立的网络设备，有自己的CPU、内存和网络接口，运行着专门优化的操作系统和负载均衡软件，例如典型的 F5。

硬件负载均衡器有很多优势。首先是性能非常高，因为它们使用专用硬件和优化软件，能够处理大量的并发连接，每秒可以处理数百万次请求。其次，它们的可靠性很好，通常采用冗余设计，比如双电源、双网卡等，即使部分硬件出现问题，也能保证业务继续运行。还有，它们的安全性也很高，很多硬件负载均衡器都内置了DDoS防护、Web应用防火墙等安全功能。最后，它们的功能非常丰富，除了基本的负载均衡，还提供SSL卸载、内容缓存、压缩、健康检查等高级功能。

------------------------------

214. 什么是软件负载均衡？, page url: https://www.mianshi.icu/question/detail?id=678
214-678-答案：
简单题，在校招和初中级工程师岗位中比较常见，基本上你整个职业生涯都要频繁和软件负载均衡打交道。

在回答这个问题的时候，你可以将这个话题引导到不同的软件负载均衡实现上，例如说 DNS 负载均衡和微服务负载均衡，当然有些人单独拎出来的微服务负载均衡大多数时候也是用软件负载均衡实现的。
先看一下前置问题：
软件负载均衡是指通过在服务器上通过运行特定软件来实现网络流量和应用请求分发的方法。与需要专门硬件设备的硬件负载均衡不同，软件负载均衡可以在标准服务器或虚拟机上运行，这使得它更加灵活和成本效益高。

软件负载均衡，按照其工作的网络层次又可以分为四层负载均衡和七层负载均衡。四层负载均衡，是指根据IP地址和端口号对请求进行转发，LVS是其典型代表。七层负载均衡是指根据应用层协议内容，比如：HTTP头、URL、Cookie等对请求进行更细粒度的流量管理和路由决， Nginx是其典型代表。

首先是成本低，可以在现有硬件上运行，不需要额外的专用设备。
我们很多耳熟能详的东西本质上也可以看做是软件负载均衡。

------------------------------

215. 硬件负载均衡有什么优缺点？, page url: https://www.mianshi.icu/question/detail?id=679
215-679-答案：
简单题，基本概念题。

你可以在硬件负载均衡优点的基础上进一步总结硬件负载均衡的主要使用场景，并且搭配上使用案例来刷亮点。
你可以在什么是硬件负载均衡(meoying.com)这里找到硬件负载均衡的详细内容。
硬件负载均衡器有很多优势。

首先是性能非常高，因为它们使用专用硬件和优化软件，能够处理大量的并发连接，每秒可以处理数百万次请求。

其次，它们的可靠性很好，通常采用冗余设计，比如双电源、双网卡等，即使部分硬件出现问题，也能保证业务继续运行。
普遍来说，在寻求高性能的地方会优先考虑硬件负载均衡，甚至于可以说大部分中小公司只有在软件负载均衡搞不定的情况下才会考虑使用硬件负载均衡。

------------------------------

216. 软件负载均衡有什么优缺点？, page url: https://www.mianshi.icu/question/detail?id=680
216-680-答案：
简单题，考察基本概念。

你可以在总结软件负载均衡优缺点的基础上，进一步讨论软件负载均衡的常见使用场景。
你可以在这里找到软件负载均衡的描述：什么是软件负载均衡？ (meoying.com)
软件负载均衡有如下优点：

首先是成本低，可以在现有硬件上运行，不需要额外的专用设备。

第二是灵活，灵活源自两方面，一方面是算法可以根据场景来定制，一方面是部署灵活，只要硬件能支撑，就可以部署；

------------------------------

217. 为什么使用了负载均衡还是会出现偶发性的负载不均衡问题？, page url: https://www.mianshi.icu/question/detail?id=681
217-681-答案：
略难的题，也就是如果你第一次见到这个问题，你可能完全没有思路。

从结论上来说，那就是偶发性负载不均衡总是难以避免的，你只要把两个根源阐述清楚就足以刷出亮点了，毕竟你的竞争者没有这么深刻的认知。
偶发性负载不均有两个根本原因：无法实时准确计算服务器负载、无法准确计算请求所需资源。

无法实时准确计算服务器负载，带来的问题就是你的算法觉得这个节点上的负载应该比较低，但是实际上它的负载可能相当高。
主要有两方面的原因：

第一个是无法准确地实时地计算出服务器的负载。

第二个是无法准确地提前计算出处理请求所需资源。

------------------------------

218. 使用了负载均衡之后，负载就一定是均衡的吗？, page url: https://www.mianshi.icu/question/detail?id=682
218-682-答案：
略难的题。从直觉上来说，你很容易就能猜到，并不一定。但是要你解释为什么负载会不均衡，你就很难一言直达本质。

还是要记住核心：无法实时准确计算服务器负载、无法准确计算请求所需资源这两个问题不解决，就永远会有负载不均衡的问题，无非就是出现的频率而已。
显然，不一定，不管你用什么负载均衡算法，总是会遇到偶发性负载不均衡的问题。
不一定，因为不管使用什么负载均衡算法，总是会出现偶发性负载不均衡的情况，主要有以下两方面的原因：

第一个是无法准确地实时地计算出服务器的负载。

第二个是无法准确地提前计算出处理请求所需资源。

------------------------------

219. 你是否解决过负载不均衡的问题？, page url: https://www.mianshi.icu/question/detail?id=684
219-684-答案：
略难的题。

你要从两方面回答这个：一方面是你肯定解决过负载不均衡的问题，另外一方面则是出现负载不均衡的本质原因。最为关键的是你要准备一个足够高级的解决负载不均衡的案例。
首先，记住一个点，你不管实践中有没有解决过，你最好都回答解决过，然后背一个案例出来。

如果你有自己的案例，你就用自己的案例，然后要记得选比较高级。整个案例你需要准备一个从发现-定位-解决的全流程方案，防止面试官问你细节。

我给你一个非常非常高级的案例，你可以一直用到你的职业生涯结束，这个方案就是动态分配槽的一致性哈希负载均衡叠加本地缓存的方案。
解决过。

我曾经在使用一致性哈希负载均衡算法来提高本地缓存命中率的时候就遇到过负载不均衡的问题。

首先，为了提高本地缓存的命中率，我使用了一致性哈希的负载均衡算法，也就是同一个用户的数据会打到同一个节点上。最开始的时候，我使用的是固定分配的哈希负载均衡算法。简单来说就是我把哈希值分成 1024 个槽，而后1024 个槽平均分配到所有的节点上。例如说常规情况下，我有九台实例，也就是每个实例负责 114 个槽。
但是我这个算法也无法彻底解决负载均衡的问题，因为有两个根本原因，我始终没有办法解决。

第一个是无法准确地实时地计算出服务器的负载。

------------------------------

220. 什么是轮询算法？, page url: https://www.mianshi.icu/question/detail?id=686
220-686-答案：
简单题，在校招和初级工程师面试中，如果聊到了负载均衡，那么你大概率会被问这个算法，及其变种。

你回答的时候要先讲清楚基本原理，而后进一步讲清楚优缺点，从而将话题引导过去两个变种：基于权重的轮询和平滑加权轮询算法。

你在准备这个算法的时候要记得试着写写实现，说不定面试官就会让你手搓一个。
轮询算法一句话就能说清楚：排排坐，分果果。
轮询算法的基本思想很直观。假设我们有三台服务器A、B、C，那么第一个请求会给A，第二个给B，第三个给C，第四个又回到A，就这样循环往复。

这种算法的优点是实现简单，而且对于服务器性能相近的情况下，它能做到相对均衡的负载分配，所以轮询应该是使用最广泛的负载均衡算法了。但是它也有一些局限性。比如说，服务器之间的性能差异较大，那么单纯的轮询可能会导致负载不均衡。

------------------------------

221. 什么是加权轮询负载均衡算法？, page url: https://www.mianshi.icu/question/detail?id=689
221-689-答案：
简单题，在校招和初级工程师面试中，如果聊到了负载均衡，那么就有大概率问到这个算法，或者更加进一步的变种平滑加权轮询算法。

回答这个问题你先回答一般的加权轮询算法，而后再将平滑加权轮询算法，最终通过阐述权重的设置来进一步刷亮点。
加权轮询算法是在轮询算法的基础上更近一步，引入“权重”概念并以此来表示服务器的处理能力，也就是将服务器节点的处理能力纳入整体考量。当然，权重是根据经验人为设定的，算法要求的是在长期运行过程中，负载均衡器分配给各服务器的请求数量应与其权重成正比，权重越高的服务器节点分配到的请求应该越多。

如下图所示：


加权轮询算法（Weighted Round Robin），是轮询算法的一个改进版本。相较于普通轮询算法中，假定所有服务器的处理能力是相同的。加权轮询算法更“智能”一些，会为每个服务器预先分配一个权重值。这个权重值通常反映了服务器的处理能力，权重越高，代表服务器性能越好，可以处理更多的请求。算法会根据这些权重值，按比例将更多的请求分配给权重较高的服务器。

举个简单的例子，假设我们有三台服务器 A、B、C，它们的权重分别是 5、3、2。那么在长期运行过程中，请求的分配可能是这样的：A、A、A、A、A、B、B、B、C、C。可以看到，A 获得了5个请求，B 获得了3 个请求，C 获得了2个请求，刚好符合它们的权重比例。

这个算法的优点是实现简单，而且能够有效地根据服务器的性能分配负载。它特别适用于那些服务器性能差异较大的场景。但是，加权轮询算法也有一些限制。
在加权轮询算法中，服务器节点权重的具体值或者说绝对值并不重要。真正重要的是权重之间的相对值，因为这决定了请求是怎么分配的。在我的经验中，设置节点权重主要有三种方法。

第一种是相对比例法。比如说，如果我们有三个服务器，它们的处理能力比是2:3:5，我们就可以直接用这个比例作为权重。

第二种是性能倍数法。我们可以选择性能最低的节点作为基准，给它权重1，然后其他节点的权重就是它们相对于基准节点的性能倍数。举个例子，如果最弱的服务器性能定为1，其他两个分别是它的1.5倍和2倍，那么权重就可以设为1、1.5、2。这种方法在服务器性能差异较大时特别有用。

------------------------------

222. Redis 有哪些数据结构？, page url: https://www.mianshi.icu/question/detail?id=690
222-690-答案：
简单题，在校招和初中级岗位面试中非常常见。

回答这个问题，你一边要把 Redis 有的数据结构都列举一边，一边要指出这些数据结构对应的底层实现。而后，你要进一步举几个例子，论述不同的数据结构可以用于不同的场景，最好就是用你项目经历中有特色的 Redis 案例。

最后你通过总结一般的选择数据结构的原则来刷出最后一个亮点。
这里先列举一下 Redis 的数据结构，以及对应的底层实现。

Redis 提供了多种数据结构，每种结构都有其独特的应用场景和底层实现。以下是 Redis 的主要数据结构及其底层实现：
常用的数据结构有五种。

第一种是 String，其底层实现是简单字符串（SDS），简单来说就是一个字节数组。大多数的缓存用的都是这种数据结构。

第二种是 List，底层实现有 ziplist，quicklist。一般存储数组类的数据就很适合。我曾经用 List 结构实现过滑动窗口算法。
从实践中来说，使用 Redis 最重要的问题就是选择合适的数据结构。我个人总结了一些选择数据结构的原则。

第一个是首先要考虑的是功能性，也就是数据结构支持的操作要能满足业务场景需要，这可以说是一切的前提了。

第二个则是要考虑性能问题。性能问题包含两方面，一个是内存使用量要尽可能少，另外一个是读写操作要尽可能快。尤其是在使用一些复杂数据结构的时候，千万要小心过大的数据结构拖累 Redis 性能。比如说最典型的遍历操作，如果要是遍历巨大的 List，那么会严重拖累 Redis 的性能。

------------------------------

223. 如何解决偶发性不均衡问题？, page url: https://www.mianshi.icu/question/detail?id=692
223-692-答案：
只要无法准确地实时地计算出服务器的负载和无法准确地提前计算出处理请求所需资源两个问题解决不了，你就解决不了偶发性负载不均衡的问题。
这是不可避免的，无法完全解决，只能缓解，因为引起负载不均衡的根源有两个：

第一个是无法准确地实时地计算出服务器的负载。
缓解的话还是有可能的，这个跟具体的负载均衡算法有关。比如说在哈希类的负载均衡算法里面，要选择均匀性好的哈希函数，这样就能降低偶发性负载不均衡问题出现的概率。
我曾经在使用一致性哈希负载均衡算法来提高本地缓存命中率的时候就遇到过负载不均衡的问题。

首先，为了提高本地缓存的命中率，我使用了一致性哈希的负载均衡算法，也就是同一个用户的数据会打到同一个节点上。最开始的时候，我使用的是固定分配的哈希负载均衡算法。简单来说就是我把哈希值分成 1024 个槽，而后1024 个槽平均分配到所有的节点上。例如说常规情况下，我有九台实例，也就是每个实例负责 114 个槽。

但是很不幸的是有部分热点数据集中在了其中一个实例上，导致这个实例的负载就是比别人高。在这种情况下，我设计了一个动态分配槽的算法，并且每分钟重新分配一次。

------------------------------

224. 什么是平滑加权轮询负载均衡算法？, page url: https://www.mianshi.icu/question/detail?id=693
224-693-答案：
略难一点的题，在校招和初级工程师面试中，面试官认为你比较优秀，那么就会问这个问题，不然可能就是问基于权重的轮询或者直接问最简单的轮询算法。

回答这个问题，你能够把为什么要引入平滑过程，以及具体的算法步骤就可以了。

如果你面的是中间件研发、基础设施研发，那么你记得练习一下这个算法，有可能会让你手搓。
在平滑加权轮询负载均衡算法中每个服务器有两个权重，当前权重（current_weight）和初始权重（initial_weight）。初始权重是固定不变的，当前权重从0开始不断变化。比如，A、B、C三个服务器其初始权重分别为5、3、2。通过如下步骤选取服务节点：

平滑加权轮询（Smooth Weighted Round Robin, SWRR）是对传统加权轮询（Weighted Round Robin，WRR）算法的一种改进。它主要解决了传统算法中请求分配不均匀的问题，特别适用于后端服务器处理能力差异较大的场景。

传统的加权轮询算法虽然用权重标记了不同服务器的处理能力或性能，但是它的分配方式可能会导致请求集中在某些服务器上。比如说，如果服务器A的权重是5，服务器B的权重是3，服务器C的权重是2，那么请求分配的顺序就会是A、A、A、A、A、B、B、B、C、C，这样很容易造成局部过载。

而平滑加权轮询算法就很好地解决了这个问题。它的核心思想是通过动态调整权重，使得请求分配更加均匀。具体来说，它引入了"当前权重"的概念，每次选择权重最大的服务器来处理请求，然后立即减少刚刚被选中的服务器的当前权重。这样，就能够在保证总体请求比例符合权重的同时，避免请求过度集中。
在加权类的负载均衡算法中，服务器节点权重的具体值或者说绝对值并不重要。真正重要的是权重之间的相对值，因为这决定了请求是怎么分配的。在我的经验中，设置节点权重主要有三种方法。

第一种是相对比例法。比如说，如果我们有三个服务器，它们的处理能力比是2:3:5，我们就可以直接用这个比例作为权重。

第二种是性能倍数法。我们可以选择性能最低的节点作为基准，给它权重1，然后其他节点的权重就是它们相对于基准节点的性能倍数。举个例子，如果最弱的服务器性能定为1，其他两个分别是它的1.5倍和2倍，那么权重就可以设为1、1.5、2。这种方法在服务器性能差异较大时特别有用。

------------------------------

225. 什么是随机负载均衡算法？, page url: https://www.mianshi.icu/question/detail?id=694
225-694-答案：
简单题，有点热度，但是总体热度不如轮训，在校招和初级工程师中比较常见。

你回答的时候要指出基本步骤，优缺点就可以，而后将话题引导过去基于权重的加权随机算法。
整个算法的理论是非常简单的，就是生成一个随机数，而后根据随机数除以服务器节点个数，余数对应的服务器节点就是选中的节点。
随机负载均衡算法（Random Load Balancing ）是一种简单但有效的负载均衡策略。它的核心思想是在可用的服务器中随机选择一个来处理每一个新到达的请求。虽然这种方法看起来很简单，但在某些场景下也能达到不错的效果。

具体来说，随机算法的工作原理是这样的：每当有新的请求到达时，负载均衡器会从当前可用的服务器列表中，使用随机数生成器随机选择一个服务器，然后将请求转发给这个被选中的服务器。这个过程对于每个新的请求都会重复进行。

这种算法有几个明显的优点：首先，它实现起来非常简单。基本上只需要一个随机数生成器和一个服务器列表就可以了。这意味着它的计算开销很小，能够快速做出决策。其次，在长期运行中，如果后端服务器的处理能力大致相同，那么请求会被相对均匀地分布到各个服务器上。这是因为随机选择的长期效果会趋近于均匀分布。再者，这种算法不需要维护任何状态信息，也不需要服务器之间的通信，这让它特别适合用在无状态服务的负载均衡场景中。
相比之下，虽然随机算法和加权算法都是简单、性能好的算法，但是在实践中随机算法不如加权算法应用广泛。道理也很简单，就是随机算法的可控性更差。例如说我提到的如果要是运气不好的话，可能连续多个请求都发送到同一个节点上，导致负载不均衡。

------------------------------

226. 什么是加权随机负载均衡算法？, page url: https://www.mianshi.icu/question/detail?id=695
226-695-答案：
简单题，基本概念题，在校招和初级工程师面试中比较常见。

回答这个问题简要介绍算法的基本步骤，优缺点之后，就可以通过讨论权重的设置问题来刷亮点，并且进一步可以讨论二次随机负载均衡算法，引导话题。
加权随机，简单来说就是用权重来表达一个节点被选中的概率。权重越高，选中的概率就越高；权重越低，选中的概率就越低。

算法步骤跟随机算法差不多，也是靠一个随机数。

首先，为服务器节点分配权重：
加权随机负载均衡算法 (Weighted Random Load Balancing) 是在普通随机负载均衡算法基础上的一种改进。它引入了权重的概念，可以根据后端服务器的处理能力或资源配置的不同，为其分配不同的权重，从而实现更加灵活和合理的负载分配。

这种算法的工作原理是这样的：首先，我们为每个服务器分配一个权重值，通常是一个正整数。权重值越大，表示该服务器的处理能力越强。当有新请求到达时，算法会根据每个服务器的权重，计算出一个权重总和。然后，它会生成一个1到权重总和之间的随机数。接着，算法会遍历服务器列表，将权重累加，直到累加和大于或等于生成的随机数。此时选中的服务器就是要处理请求的服务器。

这种方法有几个明显的优点：首先，它考虑了服务器的性能差异，性能更好的服务器会被更频繁地选中。这样就避免了普通随机算法可能导致的资源利用不均衡的问题。其次，它仍然保留了随机性，避免了完全确定性算法可能带来的某些问题，比如某些服务器长期得不到使用。再者，相比简单的随机算法，它能够更好地利用系统资源，提高整体的处理效率。
在加权类的负载均衡算法中，服务器节点权重的具体值或者说绝对值并不重要。真正重要的是权重之间的相对值，因为这决定了请求是怎么分配的。在我的经验中，设置节点权重主要有三种方法。

第一种是相对比例法。比如说，如果我们有三个服务器，它们的处理能力比是2:3:5，我们就可以直接用这个比例作为权重。

第二种是性能倍数法。我们可以选择性能最低的节点作为基准，给它权重1，然后其他节点的权重就是它们相对于基准节点的性能倍数。举个例子，如果最弱的服务器性能定为1，其他两个分别是它的1.5倍和2倍，那么权重就可以设为1、1.5、2。这种方法在服务器性能差异较大时特别有用。
通常来说不会有所谓的"平滑的加权随机算法"，因为这触及到了随机算法的本质。首先，我们要理解随机性和平滑性是两个相互矛盾的概念。随机性意味着每次选择都是独立的、不可预测的。而平滑性则要求我们能够控制结果，使其在短期内不会出现极端情况。这两者本质上是冲突的。

相比之下，平滑加权轮询算法之所以可行，是因为它是一个确定性的过程。我们可以精确地控制每个选项被选中的顺序和频率。但一旦我们引入了真正的随机性，这种控制就失去了。

------------------------------

227. 什么是二次随机负载均衡算法？, page url: https://www.mianshi.icu/question/detail?id=696
227-696-答案：
略难的题，当然这不是说这个算法难，而是说你可能都没听过这个算法。这个问题在校招和初级工程师面试中不太常见，毕竟比较冷僻。

你回答这个问题就要揭示出这个算法的本质是通过两个随机过程避免选中不合适的节点。
先阐述算法核心思想是随机选两个服务节点，根据负载指标选取负载较低的分发请求，再阐述优缺点及适用场景。

二次随机负载均衡算法基本原理如下图所示：


二次随机负载均衡算法（Two-Stage Random Load Balancing）也称为两阶段随机负载均衡。或者P2C (Power of Two Choices) 算法，即“两选一”负载均衡算法。是在普通随机负载均衡算法基础上的一种改进。它的核心思想是通过两次随机选择来降低负载不均衡的概率，从而实现更加均衡的负载分配。这种算法也被称为"选择两个中的最小负载"或"两阶段随机负载均衡"。

这个算法的工作原理是这样的：当有新的请求到达时，负载均衡器会先从所有可用的服务器中随机选择两个。然后，它会比较这两个服务器的当前负载情况，选择负载较轻的那个来处理新的请求。

负载情况的比较可能是通过当前连接数、CPU使用率、响应时间等指标来进行的。

------------------------------

228. 什么是IP地址哈希负载均衡算法？, page url: https://www.mianshi.icu/question/detail?id=697
228-697-答案：
简答题，考察基础知识，这个问题其实不太常见，更加常见的是问哈希负载均衡算法。

所以实际上你就可以认为 IP 地址哈希负载均衡算法就是哈希负载均衡算法中的一个特殊实现，没啥大不了的地方。
先阐述算法核心思想，然后评价优缺点，在评价缺点的时候，引导出缓存失效、一致性哈希算法等，引导面试官追问。
IP地址哈希负载均衡算法是一种基于客户端IP地址进行负载分配的方法。这个算法的核心思想是通过对请求的源IP地址进行哈希计算，然后根据计算结果来决定将请求发送到哪台后端服务器。

具体来说，当一个请求到达负载均衡器时，算法会先提取出请求的源IP地址。然后，它会使用一个哈希函数，对这个IP地址进行哈希计算。得到哈希值后，负载均衡器会用这个值对服务器数量取模，这样就能得到一个服务器的索引。最后，负载均衡器就会把请求转发给对应的服务器。

这种算法有几个很明显的优点。首先，它能保证会话一致性。因为同一个IP的请求总是会被分配到同一台服务器，这对于需要保持用户状态的应用来说非常重要，比如在线购物网站的购物车功能。其次，这个算法的计算效率很高，哈希计算和取模操作都很快，所以能迅速做出负载均衡的决策。另外，它不需要维护复杂的状态信息，实现起来相对简单。

------------------------------

229. 什么是一致性哈希负载均衡算法？, page url: https://www.mianshi.icu/question/detail?id=698
229-698-答案：
简单题，各个级别的面试中都会出现的高频面试题。你说梦话都应该能答对的一个知识点。

回答一致性哈希负载均衡算法，可以通过讨论虚拟节点
你需要先看一下你知道哈希负载均衡算法吗？（meoying.com）

一致性哈希 (Consistent Hashing) 是一种在分布式计算系统中用于数据分片和负载均衡的算法。它提供了一种高效的数据分片和负载均衡策略，能够有效地应对系统的扩展和故障，主要目标是在动态增减节点的情况下，最小化数据重新分布的开销。 算法的框架如下图所示：




我用一个比喻来帮你加深理解。一致性哈希就仿佛你的朋友突然打电话给你说，我们下一个整点去吃海底捞，然后你看了一下时间，现在是 4:35，下一个整点是 5:00。
一致性哈希负载均衡（Consistent Hashing Load Balancing）算法是一种用于分布式系统中分配请求和数据的技术，特别适合于节点频繁变化的场景。其核心目标是在节点增加或减少时，最小化数据的重新分配，从而保证系统的稳定性和高效性。这个算法被广泛应用于分布式缓存、数据库和其他需要高可用性和扩展性的系统。

简单来说，一致性哈希算法的核心思想是将整个哈希值空间想象成一个首尾相连的环。我们把服务器节点和数据都映射到这个环上。当我们需要确定一个数据项应该存储在哪个服务器上时，我们就顺时针找到第一个遇到的服务器节点，就把数据存在那里。

这种方法有个很大的好处。当我们需要增加或减少服务器节点时，只会影响到环上相邻的一小部分数据，大部分数据不需要移动。这就大大减少了数据迁移的工作量，提高了系统的稳定性和效率。
我曾经使用过一个动态调整映射关系的一致性哈希负载均衡算法来解决本地缓存命中率过低的问题。

首先，为了提高本地缓存的命中率，我使用了一致性哈希的负载均衡算法，也就是同一个用户的数据会打到同一个节点上。最开始的时候，我使用的是固定分配的哈希负载均衡算法。简单来说就是我把哈希值分成 1024 个槽，而后1024 个槽平均分配到所有的节点上。例如说常规情况下，我有九台实例，也就是每个实例负责 114 个槽。

但是很不幸的是有部分热点数据集中在了其中一个实例上，导致这个实例的负载就是比别人高。在这种情况下，我设计了一个动态分配槽的算法，并且每分钟重新分配一次。

------------------------------

230. Kafka 不同分区设置不同延迟时间的方案是怎么运作的？, page url: https://www.mianshi.icu/question/detail?id=699
230-699-答案：
简单题，Kafka 支持延迟消息的解决方案中你必须要掌握的一种。

在这个问题之下，你要刷亮点就可以深入讨论 rebalance 的问题和一致性问题，更进一步则可以讨论极端情况下分区消息积压的问题，而消息积压本身也是一个非常热门的面试话题，所以效果会很好。
你在Kafka如何实现延迟消息？ (meoying.com)中可以看到这个部分的描述。
整体思路非常简单，通过在一个topic中设置不同的分区，每个分区代表一个延迟时间。消费者每次取出来消息之后，查看一下这个消息还要睡眠多久，睡眠完后再将消息转发到不同的业务 Topic，如下图：


我们公司用的方案是比较简单的，也就是创建了一个 delay_topic，这个 topic 有 N 个分区，每个分区设定了不同的延迟时间。然后我们创建了一个消费组去消费这个 delay_topic，每个分区有一个独立的消费者。每个消费者在读取到一条消息之后，就会根据消息里面的延迟时间来等待一段时间。等待完之后，再把消息发送到业务方真正的 topic 上。
在这个方案里面有两个常见问题：
在极端情况下，还会有消息积压的问题，而且严格来说这还是顺序消息积压。比如说，绝大部分业务都使用 30m 作为延迟时间，那么这个分区就会有大量的消息，有可能出现消息积压的问题。

而在这个场景下，解决思路也挺简单的。

第一个方案就是可以考虑多个分区设置相同的延迟时间，客户端在发送消息的时候，用随机算法或者轮询算法都可以。

------------------------------

231. 不同分区设置不同延迟时间，这个延迟时间该怎么设置（多长）？, page url: https://www.mianshi.icu/question/detail?id=700
231-700-答案：
简单题。

一句话来说，就是你的业务需要多长就多长，没别的花招。
设置延迟时间设置多久，最核心的依据就是业务方的需求。如果业务方对应消息延迟容忍度很低，就专门设置一个分区给他。其他就按照业务的延迟时间分布进行设置，这边给出两个适合大部分业务的延迟时间。
延迟时间设置多久，最核心的依据就是业务方的需求。如果业务方对应消息延迟容忍度很低，就专门设置一个分区给他。其他就按照业务的延迟时间分布进行设置。常见的可以是分为5个分区：延迟时间分别是 1min、3min、5min、10min、30min。

------------------------------

232. Kafka 如何支持随机延迟时间？, page url: https://www.mianshi.icu/question/detail?id=701
232-701-答案：
略难的题，校招和初级研发岗位不太常见，更高级别的岗位面试中会比较常见。

在这个问题之下，可以通过重复消息问题、性能优化、高并发大数据下怎么支持随机延迟时间来刷亮点。
使用转储的方案，将延迟消息发送到一个topic，然后延迟消费者将消息转储到 MySQL 中，然后有一个延迟消息发送者，定期查找到时间的延迟消息将其转发到对应的业务topic。如下图所示：



这里总结一下：
最简单的解决方案，应该是使用数据库来转储。它有几个关键步骤：

首先，创建了一个名为 delay_topic 的消息队列，所有需要延迟处理的消息都会存入该队列。
这个方案里面有可能出现重复消息的问题。

最典型的场景就是延迟发送者轮询找到了目标消息，而后转发到业务 Topic 成功，结果标记已发送状态的时候失败了。那么后续就会再次轮询出来这一条，再次转发到业务 Topic。造成业务消费者收到重复消息。

但是我这个方案是不准备解决这个问题，而是要求消费者要保证消费幂等。
很显然，在使用 MySQL 的转储方案中，最大的性能瓶颈就是 MySQL 本身。

最简单的做法是使用分区表，比如说根据并发量选择按月分、按周分、按天分，而历史分区就可以直接清理掉。这样可以保证每张表的数据的都很少，那么读写的效率都很高。

还可以考虑使用表交替方案，也就是说准备两个表 tab_0、tab_1。那么最开始的时候可以读写 tab_0，然后换成读写 tab_1。每次交替的时候，都可以把之前使用的数据 TRUNCATE 掉。这种做法也是保证了每一张表的数据量

------------------------------

233. Redis 是如何做到高可用的？, page url: https://www.mianshi.icu/question/detail?id=702
233-702-答案：
略难的题，在 Redis 面试中比较常见，各个层级的程序员面试中都很常见。

你在这个问题之下，只需要回答到 Redis Sentinel 和 Redis Cluster 就可以了，细节可以等进一步追问。而后你可以通过总结 Redis 这种对等集群和主从集群混合的模式，在 Kafka 等中间件中也很常见，从而展示你对系统设计有深刻理解。
Redis 高可用主要有两个方案一个是 Redis Sentinel (哨兵)还有一个是 Redis Cluster

Redis Sentinel 模式本质上是一个主从模式。但是 Redis Sentinel 的主从选举稍微有点特殊，它的主节点的选举不是通过从节点完成，而是通过 Sentinel 来完成。Sentinel 监控整个主从集群，对下线的主节点进行故障转移和通知。所以 Sentinel 模式会有两个集群，一个是哨兵集群，一个是数据集群。整个结构如下图：


Redis 的高可用主要是源自两套方案：Redis Sentinel 和 Redis Cluster。

Redis Sentinel模式本质上是一个主从集群，内部可以进一步细分为 Sentinel 集群和数据集群。Sentinel 集群监控数据集群，数据集群的高可用性则依赖于主从数据复制和主从选举机制。
第二种模式是 Redis Cluster。

Redis Cluster 是一个对等结构和主从结构的混合架构。Redis Cluster 由多个节点组成，这些节点之间地位是平等的，也就是说它们构成了一个对等结构。

但是从细节上来说，每一个节点都是一个主从集群，也就是说每一个节点都是类似于 Redis Sentinel 模式。
Redis Cluster 这种对等集群和主从集群的混合模式，在别的中间件里面也能看到类似的设计，甚至于可以说现代的大规模分布式软件的高可用都是通过这种设计来保证的。

------------------------------

234. Redis 如何实现字典/hashtable（哈希表） 的？, page url: https://www.mianshi.icu/question/detail?id=703
234-703-答案：
略难的题，高频题，在各个层级的面试里面都是高频题目。

回答这个问题，要把重点放在渐进式 rehash 上，这也是一个难点，并且你可以结合自己了解的别的类似结构进一步讨论，比如说语言层面上的哈希表。
Redis 的 hashtable 的结构，一句话就能说清楚，使用拉链表解决哈希冲突的双表结构。整个结构如下图：


从上图看你就懂什么双表是什么意思了，也就是 Redis 里面说的 hashtable 实际上是由两个典型的哈希表组成的，对应的就是 ht[0] 和 ht[1] 这两个。
Redis 的 hashtable 有两个核心特征：使用拉链法解决冲突，支持渐进式 rehash。
（解释渐进式 rehash）而渐进式的 rehash 则意味着在 hashtable 里面，有 ht[0] 和 ht[1] 两个标准的哈希表，它们也代表渐进式 rehash 中的旧表和新表。而不管是扩容还是缩容都是使用渐进式 rehash 来完成的。

渐进式 rehash 的过程包含三个主要步骤。

第一步是在 ht[1] 上创建一个新容量的哈希表。
Redis 这种渐进式 rehash 算是一个非常有特色的改进。这种改进主要是为了在处理正常业务请求和迁移全部数据之间取得平衡。

举个极端的例子，如果要是 hashtable 里面几千万个键值对，一次性全部迁移完的话，可能需要几十秒。而很显然，因为 Redis 本身是单线程的，所以在迁移过程中没有办法处理业务请求。而业务当然没办法等几十秒。

------------------------------

235. Redis 是如何进行主从复制的？, page url: https://www.mianshi.icu/question/detail?id=704
235-704-答案：
简单题，所有的主从复制都差不多是一个样，你会一个就会全部了。

在回答这个问题的时候，你也不太需要把所有的细节都回答出来，可以等着面试官追问。
Redis 的主从复制就两个：

综合来看，全量复制适用于初始化同步或数据丢失的情况，而部分重同步适用于从节点和主节点断线后的重新连接。

如果站在一个节点的角度，那么当一个节点加入了主从集群之后：
Redis 的主从复制分成两种模式：全量复制和增量复制。

在全量复制中，从节点从主节点那里复制整个数据集。这种策略适用于从节点初次加入或者数据丢失的情况。主节点会生成一个 RDB 快照，并将其发送给从节点，从节点加载这个快照来进行复制。全量复制的缺点是，如果数据集非常大，将会占用大量的网络带宽和时间，所以要尽可能规避全量复制。

这一个步骤的核心难点是快速生成 RDB 文件，并且不要影响 Redis 正常对外提供服务。同时 RDB 生成需要时间，从节点加载也需要时间，所以实际上从节点最终加载的数据是比较老的，还需要进一步使用增量复制来跟上节奏。
也不仅仅是 Redis 使用两种形态的复制，在所有的主从结构中，基本上都会支持两种复制模式。

------------------------------

236. 你了解 Redis Cluster 吗？, page url: https://www.mianshi.icu/question/detail?id=705
236-705-答案：
简单题，在各层级的面试中都有可能遇到，一般来说你面试的公司规模越大越有可能问到。

Redis Cluster 实际上也没啥特殊的，就是一个对等结构和主从结构的混合架构，唯一稍微特殊一点的就是引入了槽和槽分配的概念，你只要把这部分捋清楚就好了。而后，在面试中可以从跨槽问题、混合架构两个角度刷亮点。如果你在实践中有使用类似的技术，那么更加能给面试官留下深刻的印象。
这部分内容你可以在Redis 是如何做到高可用的？ (meoying.com)中找到。
Redis 本身就是一个对等结构加主从结构的混合架构，如下图：


Redis Cluster 是一个对等结构和主从结构的混合架构。Redis Cluster 由多个节点组成，这些节点之间地位是平等的，也就是说它们构成了一个对等结构。

但是从细节上来说，每一个节点都是一个主从集群，也就是说每一个节点都是类似于 Redis Sentinel 模式，并借此来保证高可用。

Redis Cluster 借鉴一致性哈希的思想，利用 CRC16 将 key 分散到 16384 个槽（哈希槽就相当于一致性哈希中的虚拟节点）上面，而后再次将这些槽分配给不同的节点。可以平均分，也可以不是平均分。
但是 Redis Cluster 并不是毫无缺点，最大的问题就是难以处理跨槽的问题。

这最典型的例子就是 pipeline。例如说在 pipeline 里面要处理分散在不同槽上的多个 key，那么pipeline 就会返回错误，这需要客户端进行处理。而有些语言的 Redis 客户端其实没有那么智能。
Redis Cluster 这种对等集群和主从集群的混合模式，在别的中间件里面也能看到类似的设计，甚至于可以说现代的大规模分布式软件的高可用都是通过这种设计来保证的。

------------------------------

237. Redis 中的槽分配是指什么？, page url: https://www.mianshi.icu/question/detail?id=706
237-706-答案：
简单题，一般如果要是聊到了 Redis Cluster，那么就会有极大的概率问这个问题。

回答这个问题，你可以谈及你借助类似的思想解决过什么问题，这样能够证明你对槽分配及对应的思想有深刻理解和丰富经验。
你可以先阅读这个问题：什么是一致性哈希负载均衡算法？ (meoying.com)。

简单来说，Redis 用 CRC16 算法计算 key 的哈希值，而后分配到 2 ^ 14 = 16384 个槽上。最后将这些槽分配到节点上。整个过程如图：

Redis Cluster 的槽分配很简单，两句话就能说清楚：CRC16算哈希值，槽分配到节点。

也就是说，针对每一个 key，Redis Cluster 会用 CRC16 计算一个哈希值。而后这个哈希值会被映射过去 16384 个槽上，Redis Cluster 再把这些槽分配到节点上。
Redis Cluster 这种槽机制，虽然很大程度上避免了数据倾斜的问题，但是还是难免会出现数据倾斜、QPS 倾斜之类的问题。

在这种情况下，最好的方案就是让 Redis Cluster 执行再平衡，重新分配一下槽。但是再平衡以及槽迁移是一个很消耗性能的问题，所以正常都不太建议这么搞。
Redis 这种槽分配的思想可以用在很多地方。

我举几个我使用过的例子。比如说我在使用一致性哈希负载均衡算法的时候，为了保证一致性哈希负载均衡算法能可能均分负载，也是引入了类似槽的概念，也就是说根据请求计算一个哈希值，而后映射过去 1024 个槽上，再结合每个槽的请求数，动态分配给节点，确保不同的服务端节点上的负载是均衡的。

------------------------------

238. Redis 为什么使用 16384 个槽？多了会怎样？少了会怎么样？, page url: https://www.mianshi.icu/question/detail?id=707
238-707-答案：
略难的题，因为绝大多数人都知道 Redis 的槽和槽分配，不少人也能记住 Redis 有 16384 个槽，但是基本没几个人会去琢磨为啥是 16384 个槽。

你在回答这个问题的时候，可以考虑使用我提供的类似槽分配思路的案例，深入讨论总结一般的槽个数的设计原则。
槽的选择主要考虑这几个因素：

所以你可以从这几个因素里面推导出来过多的问题：

也能推导出来过少的问题：
总的来说 Redis Cluster 使用 16384 个槽是为了在性能、资源消耗和可扩展性之间取得平衡。

从原则上来说，设计槽的数量要考虑三个方面。

第一个是资源消耗。维护槽是需要资源，主要包含两方面开销：内存开销和网络传输开销。例如说 Redis 节点需要用位图来维护槽分配的结果，Redis 的智能客户端需要维护每个槽到节点的映射，槽的编号在网络中传输需要多个字节来编码；
我之前参考 Redis 这种槽分配设计过一个类似的算法，一个动态分配的一致性哈希负载均衡算法，主要是为了提高缓存命中率。

基本思路就是使用了 1024 个槽，映射过去服务端节点上。但是这个映射不是静态的，而是动态的。每段时间根据每个槽的请求数量，重新分配槽到节点上，确保节点上的请求数量大致是均匀的。

1024 这个值可以说是一个经验值，也是在资源消耗、扩展性和均匀性之间取得平衡下决定的。

------------------------------

239. Redis Sentinel 中主节点崩溃之后，会发生什么？, page url: https://www.mianshi.icu/question/detail?id=708
239-708-答案：
简单题，在校招、初中级岗位面试中常见。如果目标公司强调了可用性，也会比较常见。

要在这个问题下刷亮点，可以深入讨论 Redis 的主从选举机制的特色，即是借助 Sentinel 来选举的，并且选举的从节点是按照优先级来选的，而不是选拥有最新数据的从节点。
先来回顾一下 Redis Sentinel 集群的关键点：有一个 Redis 数据集群和一个 Redis Sentinel 集群的混合集群，如下图：


你可以直接把 Redis Sentinel 看做是主从结构，而 Sentinel 就是为了解决主从选举而引入的。
首先 Sentinel 获取了主从结构的信息，而后向所有的节点发送心跳检测，如果这个时候发现某个节点没有回复，就把它标记为主观下线；如果这个节点是主节点，那么 Sentinel 就询问别的 Sentinel 节点主节点信息。如果大多数都 Sentinel 都认为主节点已经下线了，就认为主节点已经客观下线。

当主节点已经客观下线，就要步入故障转移阶段。故障转移分成两个步骤，一个是 Sentinel 要选举一个 leader，另外一个步骤是 Sentinel leader 挑一个主节点。
Redis Sentinel 和一般的主从选举有两个显著的差异。

------------------------------

240. Redis Sentinel 选举出来的新的主节点，一定会有最新的数据吗？, page url: https://www.mianshi.icu/question/detail?id=709
240-709-答案：
简单题，只有在深入讨论 Redis Sentinel 过程的时候才会提问。

你要是知道 Redis Sentinel 主从选举的过程，很容易就知道必然不是。但是你要刷亮点，就要系统分析数据延迟的各种可能。
你要先阅读这个问题：Redis Sentinel 中主节点崩溃之后，会发生什么？ (meoying.com)，这里提到一个最关键的点：Sentinel 选从节点的时候，是按照优先级-偏移量-服务器 ID 来挑选的。

所以你可以立刻想到，如果有两个从节点，一个从节点 A 有最新数据但是优先级低，一个从节点 B 优先级高但是没有最新数据，那么最终从节点 B 会被选为新的主节点。
在 Redis Sentinel 选举新主节点时，可能会因为数据延迟导致数据丢失。首先，如果配置了优先级，优先级高但与主节点存在数据延迟的从节点可能被选为新主节点，造成数据不一致。

------------------------------

241. Redis 中槽迁移的过程是什么？, page url: https://www.mianshi.icu/question/detail?id=710
241-710-答案：
略难的题，面试大厂的时候常见，尤其是聊到了 Redis Cluster 的时候。

在这个问题之下，你先回答槽迁移的基本过程，而后回答在迁移过程中如何处理读写请求，最后总结槽迁移这种渐进式的思想，刷出一波亮点。
需要先了解Redis 中的槽分配是指什么？ (meoying.com)）。Redis Cluster在触发 rebalance 的时候，就会进行槽迁移。整个过程大致如下：

整个过程如图


当 Redis 槽迁移时，首先要标记迁入和迁出的节点。迁出的源节点将负责转移部分槽的数据，而目标节点则接收这些数据。

随后，分批次从源节点获取需要迁移的键，传输到目标节点。目标节点接收到数据后，会将其反序列化并存储在自己的内存中，保持数据的一致性。

最后，源节点和目标节点更新槽的归属状态，集群完成槽的重新分配。
槽迁移的这种机制，再次体现了 Redis 的设计思想：平摊，也可以说渐进式。

------------------------------

242. 怎么解决 Redis 中数据倾斜的问题？, page url: https://www.mianshi.icu/question/detail?id=711
242-711-答案：
略难的题。一般在大厂的中高级职级面试里面会比较常见，校招和初级岗位比较少考察，

这个问题很容易回答出来，但是不容易刷出亮点。因为基本的做法如槽迁移这种很容易想到，但是你需要有一些特别的骚操作，才能赢得竞争优势。
这里我要先解释一下数据倾斜，数据倾斜一般是指某个地方的数据要比别的地方多。

具体到 Redis 这里，就是 Redis 的某些节点上的数据明显比别的节点多。而正常来说，数据倾斜了，QPS 也倾斜了。所以这些比较多数据的节点，QPS 也会更高。所以这些节点面临 CPU、内存、网络带宽等多方面的问题。

而且，如果是 QPS 很高，业内也把这个问题叫做热点问题。
最简单的处理方案有三个。

第一个是挪走部分业务数据。例如说 User 和 Order 共用一个 Redis Cluster，那么可以挪走 User；
也有一些比较复杂的方案。

第一个是重新设计 key 的结构。比如说原本的 key 是 user 前缀加上 id，那么可以改成 user_info 前缀。这个方法很简单，虽然只是修改了前缀，但是经过 CRC16 之后的哈希值是非常不一样的，所以也能起到重新分布数据的效果。不过在使用这个方案的时候要注意新的 user_info 会不会也引起数据倾斜的问题。

第二个是手动哈希一次。举个例子，比如说 user 前缀加上 id 作为原始的 key，先找一个合适的哈希函数哈希一遍，作为 key。当然为了避免冲突也可以改成类似于 user 前缀加 id 再加 id 的哈希值这种。要注意观察手动哈希之后数据分布是否均匀，以及 key 会不会冲突。

------------------------------

243. 如果 Redis Cluster 上某个节点的并发非常高，怎么办？, page url: https://www.mianshi.icu/question/detail?id=712
243-712-答案：
略难的题，在中高级岗位比较常见，校招和初级不太常见。

这个题目本质上就是问热点问题，你可以结合数据倾斜的内容来回答。实际上，你能答出重新设计 key 结构、手动哈希、拆分大 key 就能赢得竞争优势了。
这个问题和怎么解决 Redis 中数据倾斜的问题？ (meoying.com)非常类似，但是有一个区别：数据倾斜大概率造成 QPS 极高，但是 QPS 极高并不一定源自数据倾斜。举个例子来说，一个节点上可能并没有多少数据，但是因为这个节点上放了一些热点数据，导致它的 QPS 极高。

所以在这个问题之下，你就要在解决数据倾斜问题的解决思路基础上，增加额外的解决思路。

第一个思路是增加本地缓存。也就是说通过本地缓存来挡住大部分的热点流量。
（先说数据倾斜的思路）如果某个节点上的 QPS 很高，那么第一个思路就是挪走一部分数据。有三种思路：
第一个是挪走部分业务数据。例如说 User 和 Order 共用一个 Redis Cluster，那么可以挪走 User；

第二个是槽迁移，可以是让 Redis Cluster 自动 rebalance ，也可以自己手动重新分配槽;
也有一些比较复杂的方案。

第一个是重新设计 key 的结构。比如说原本的 key 是 user 前缀加上 id，那么可以改成 user_info 前缀。这个方法很简单，虽然只是修改了前缀，但是经过 CRC16 之后的哈希值是非常不一样的，所以也能起到重新分布数据的效果。不过在使用这个方案的时候要注意新的 user_info 会不会也引起数据倾斜问题或者热点问题。

第二个是手动哈希一次。举个例子，比如说 user 前缀加上 id 作为原始的 key，先找一个合适的哈希函数哈希一遍，作为 key。当然为了避免冲突也可以改成类似于 user 前缀加 id 再加 id 的哈希值这种。要注意观察手动哈希之后 QPS 是否均匀，以及 key 会不会冲突。

------------------------------

244. 如果 Redis Cluster 上的某个槽的 QPS 非常高，怎么办？, page url: https://www.mianshi.icu/question/detail?id=713
244-713-答案：
略难的题，在中高级岗位面试中会遇到，校招和初级不太可能遇到。

这个问题的难点在于特别强调了槽上的 QPS 很高，而不是常见的多个槽合在一起 QPS 很高的热点问题。解决这种问题的思路就四个字：分而治之。
你要注意和这两个问题进行对比：
先说两个最简单的做法。

第一个是槽迁移。可以把热点槽移走，也可以留下热点槽，把节点的其它槽迁走。

第二个是增加 Redis 节点，甚至于这个节点可以给这个热点槽单独使用，起到一种隔离和保护的手段；
也有一些比较复杂的方案，核心思想都是将这个槽给拆分掉。

第一个是重新设计 key 的结构。比如说原本的 key 是 user 前缀加上 id，那么可以改成 user_info 前缀。这个方法很简单，虽然只是修改了前缀，但是经过 CRC16 之后的哈希值是非常不一样的，所以大概率会把热点槽上的数据分散到别的槽上；

第二个是手动哈希一次。举个例子，比如说 user 前缀加上 id 作为原始的 key，先找一个合适的哈希函数哈希一遍，作为 key。当然为了避免冲突也可以改成类似于 user 前缀加 id 再加 id 的哈希值这种。要注意观察手动哈希之后 QPS 是否均匀，以及 key 会不会冲突。经过手动哈希之后，原本在同一个槽上的 key 大概率也被打散了；

------------------------------

245. 如果 Redis Cluster 的某个 key QPS 极高，怎么办？, page url: https://www.mianshi.icu/question/detail?id=714
245-714-答案：
略难的题，在中高级工程师面试中比较常见，校招和初级工程师比较少见。

这其实就是一个典型的热点问题，你可以在回答的时候总结一般的热点问题解决思路、原则来刷出亮点。
这就是最为极端的热点问题。一般来说这种 key 都是保存一些全局的配置、某些大客户数据等。

你要注意和这些问题进行对比：

现在一个个看过去在这些问题里面出现的解决方案：
最简单的方案就是引入本地缓存。针对这种极高 QPS 的 key，完全可以让服务端的所有节点都缓存一份，这样能够极大的削减 Redis 上的 QPS 压力。
第二种思路比较复杂，就是拆分 key，这个要看业务是否支持。这里举几个例子。

第一个例子是如果这个 key 对应的数据是一个巨大的 JSON 串，并且业务其实不是每次都要整个 JSON 串，那么可以考虑将这个 JSON 串拆分，放到好几个 key 上。

------------------------------

246. 你知道哈希负载均衡算法吗？, page url: https://www.mianshi.icu/question/detail?id=715
246-715-答案：
简单的题，在校招和初中级岗位面试中比较常见。

你回答这个问题一个是要讨论哈希算法的基本步骤，而后进一步讨论哈希类负载均衡算法的弊端：数据倾斜与对应的解决方案，从而赢得极大的竞争优势。



哈希类负载均衡算法设计框架如图所示，可以看到整体上分成三个主要步骤：


举个例子来说，假设说我们现在用 order_sn 来执行哈希负载均衡，那么先选择一个哈希函数，例如说 md5(order_sn) = 100 得出一个哈希值，这里假设是 100。而后，假设说订单有3个实例，这 3 个实例组成一个数组。那么 100% 3 = 1，也就是说数组中下标为 1 的实例就是目标实例。
设计一个哈希负载均衡算法，可以分为三个主要步骤。第一步是确定哈希键。这个步骤挺关键的，因为它决定了我们用什么来计算哈希值。通常来说，我们有几个选择：可以用客户端的IP地址，这样做的好处是能保持会话的亲和性；也可以用请求的URL，这在CDN或者缓存系统中特别有用；还有一种选择是用户ID或者会话ID，这样能确保同一用户的请求总是被一致地处理。
哈希负载均衡算法有一个很严重的问题，就是可能出现热点问题或者数据倾斜问题。比如说经过计算哈希值、服务器映射之后，某个服务器的负载要比其它服务器明显地高，这种可以通过重新选择哈希算法或者重建服务器映射来解决。
我个人在实践中还是非常喜欢使用哈希负载均衡来解决一些特定的问题。

------------------------------

247. Redis 的 ziplist 是什么？如何实现的？, page url: https://www.mianshi.icu/question/detail?id=716
247-716-答案：
略难的题，难点在你可能很难理解连锁更新的问题，一般不太常问。

注意在这个问题底下，你要先解释 ziplist 的基本结构，而后进一步解释连锁更新。目前来看，能够清晰解释连锁更新的原因、后果就差不多能赢得竞争优势了。
ziplist 是 Redis 中一种紧凑型的列表数据结构，主要用于存储元素数量少且每个元素较小的数据。它通过连续的内存空间来节省存储空间，适用于实现散列表（hash）、列表（list）等数据类型。


ziplist 的结构如上图，一句话就能说清楚 ziplist，它就是一个元素大小不等的数组。而后额外记录了总字节数，尾指针而已，没啥稀奇的。
ziplist 是 Redis 中一种紧凑型的列表数据结构，主要用于存储元素数量少且每个元素较小的数据。它通过连续的内存空间来节省存储空间，适用于实现散列表（hash）、列表（list）等数据类型。

简单来说，它可以看做是一个元素大小不定的数组结构，也就是说它占据的是一段连续的内存。它额外记录了字节数量，entry 个数和尾指针。

entry 也是一个结构体，它本身有三个字段。
但是 ziplist 有一个非常大的问题，就是所谓的连锁更新问题。

连锁更新是因为 entry 记录了前一个 entry 的长度，也就是有 prevlen 字段。但是 prevlen 字段在设计的时候，如果前一个 entry 的长度小于 254，那么用 1 个字节来记录，否则用 5 个字节来记录。在这种情况下，就会出现如果要是前一个 entry 的长度从小于 254 变成超过 254，那么当前 entry 的 prevlen 就从 1 个字节变成 5 个字节，自身长度增加了 4。万一当前 entry 的长度也因此从小于 254 变成 大于 254，就会导致下一个 entry 的 prevlen 长度从 1 个字节变成 5 个字节。如此循环往复，直到整个 ziplist 的 entry 都更新一遍。

------------------------------

248. Redis 的 quicklist 是什么？如何实现的？, page url: https://www.mianshi.icu/question/detail?id=717
248-717-答案：
简单题，相比 ziplist 的连锁更新，quicklist 还是很好理解的。

如果你是校招生，那么你可以在考察数据结构中的 List 结构的时候将话题延伸到这里，这样能够凸显你的知识积累。

在回答这个问题的时候，你可以通过解释 quicklist 为什么要设计成结合了 ziplist 来刷亮点。
一句话就可以说清楚 quicklist 的本质：一个元素是 ziplist 的双向链表。如下图：



所以，一般的双向链表有什么优缺点，它就有什么优缺点。
一句话就可以说明 quicklist 的本质：一个节点是 ziplist 的双向链表。在这个基础之上，ziplist 还额外记录了头指针，尾指针以及 quicklist 的长度。
quicklist 的高性能主要是源自它的混合结构，也就是链表元素是 ziplist 的设计。

众所周知，链表最大的性能瓶颈就是按照下标索引，但是这种混合结构很好地解决了这个问题。也就是如果我们把列表本身的节点看做是索引，那么整个结构就可以看做是一个只有一层索引的跳表。

------------------------------

249. 什么是最少链接负载均衡算法？, page url: https://www.mianshi.icu/question/detail?id=718
249-718-答案：
简单的题，不太常见。

回答这个问题，你除了要解释清楚算法的基本原理，还要进一步解释连接数并不能很好的表达负载，这会让你赢得竞争优势。
最少连接数算法是一种常用的负载均衡策略，这个算法假定，服务器上的连接数越多，表示服务器的负载越高。它的核心思想是：将新的请求或连接分配给当前活动连接数最少的服务器。让我们通过图表来逐步理解这个过程：




这个算法的弊端就是连接数不能很好地代表服务器的真实负载，主要有以下几个原因：
最少连接（Least Connections）负载均衡算法是一种动态的负载均衡策略，它的核心思想是将新的请求分配给当前活跃连接数最少的服务器。它通过实时跟踪每个服务器的连接数来进行动态调整，从而实现更均衡的负载分配。
不过，最少连接算法也存在一些重要的局限性。

首先，不同请求的资源消耗、处理时间的差异可能很大。比如说，一个简单的静态文件请求和一个复杂的数据库查询虽然都算一个连接，但它们对服务器的实际负载影响可能相差十倍甚至百倍。
如果还存在连接多路复用的情况，那么负载不均衡的问题可能变得更加严重。比如：

在HTTP/2或gRPC这样的协议中，一个TCP连接可以同时处理多个请求。这意味着，一个连接可能同时承载多个资源消耗各不相同的请求。比如，一个连接可能同时包含一个大文件下载、一个复杂的API调用和几个简单的页面请求。

多路复用还允许将多个逻辑流合并到一个物理连接中，这进一步模糊了连接数与实际负载之间的关系。同时，多路复用协议通常支持请求优先级，高优先级请求可能消耗更多资源，但这不会反映在连接数上。

------------------------------

250. 什么是最少活跃请求负载均衡算法？, page url: https://www.mianshi.icu/question/detail?id=719
250-719-答案：
简单题，你要是听过这个算法就很容易，没听过就答不上来。一般来说，在微服务负载均衡中比较常见。

要在这个问题之下刷亮点，还是可以从系统讨论活跃请求数并不能很好衡量系统节点的角度来阐述，并指出该算法用起来也难免会有偶发性负载不均衡的问题。

最少活跃请求负载均衡算法（Least Active Requests Load Balancing），也被称为最少未完成请求 （Least Outstanding Request, LOR）是一种动态负载均衡策略，其核心思想是将新到达的请求分配给当前活跃请求数最少的服务器，以实现负载的均衡分配。活跃请求是指已发送给服务器但尚未得到响应的请求。其工作原理如下图所示：



这个算法的弊端就是活跃请求数不能很好地代表服务器的真实负载，主要有以下几个原因：

下图是客户端缺乏全局信息导致负载均衡选择节点错误的示例：
最少活跃请求负载均衡算法是一种在分布式系统中广泛使用的动态负载均衡策略，算法的核心思想是把新来的请求分配给当前正在处理的请求最少的服务器。这样做的目的是为了让所有服务器的负载尽可能均衡。
不过，最少活跃请求算法也存在一些重要的局限性。

首先，不同请求的资源消耗、处理时间的差异可能很大。比如说，一个简单的静态文件请求和一个复杂的数据库查询虽然都算一个连接，但它们对服务器的实际负载影响可能相差十倍甚至百倍。同样地，不同请求需要的资源可能也不同，比如说有些需要 CPU，而有些则是极为耗费内存，所以活跃请求数其实并不能很好的体现服务器的负载。

其次，新增服务器时，因活跃请求数最少，所以可能出现短时间内接受大量请求的情况，又因为请求的资源消耗无法估计所以会造成过载。

------------------------------

251. 什么是最快响应负载均衡算法？, page url: https://www.mianshi.icu/question/detail?id=720
251-720-答案：
简单题。

一句话就能说清楚最快响应负载均衡算法：选最快的。你要想刷亮点，可以从统计响应时间以及响应时间的局部性入手，最后提及一个兜底的闲置问题，大部分人没这个实践经验，是说不出这些东西的。
最快响应负载均衡算法，也被称为最快响应时间或最短响应时间算法，是一种动态负载均衡策略，它将传入的请求路由到预期响应时间最短的服务器。其主要目标是通过利用服务器的实时性能数据来最小化延迟并改善整体用户体验。其大致原理图如下所示：



以上是算法的大致流程，需要强调的是，响应时间可以是一段时间内的平均响应时间或者99线之类的。但是不管取哪个值，在统计的时候都要考虑时间局部性原理。也就是说，应该给予近期的数据更高的权重，才能尽可能反应当下节点的负载。
最快响应负载均衡算法是一种动态负载均衡策略，算法的核心思是把新来的请求分配给响应时间最短的服务器。这样做的目的是为了优化用户体验，同时提高整个系统的性能。

说到它的工作原理，一句话就可以说清楚：选最快的。

具体来说，主要包括几个关键步骤。首先，负载均衡器会不断监控每个服务器的响应时间，可以通过发送探测请求或者分析实时流量来实现。然后，它会计算每个服务器的平均响应时间，通常是最近一段时间内的平均值。有时候，我们还会使用一些更复杂的算法，比如移动平均或指数加权移动平均，使计算得到响应时间更符合服务器性能的一般趋势。最后，当新的请求到来时，负载均衡器就会把它发送到响应时间最短的服务器。
这个算法的关键点就是统计响应时间，实践中算法效果好还是很大程度上取决于这里统计的是否准确。

从理论上来说，可以统计响应时间的中位数、平均值、99线或者 999 线，在实践中效果相差不大。但是不管用哪一个指标，都要考虑一个问题，就是响应时间的时间局部性。

举个例子来说，一个小时以前的请求的响应时间对当下的负载均衡器来说就毫无意义，因为它已经完全不能反应当下节点的负载了。

------------------------------

252. 如何选择合适的负载均衡算法？, page url: https://www.mianshi.icu/question/detail?id=721
252-721-答案：
略难的题，而且是一个很讲究实践经验的题。

你可以简单回答自己选择负载均衡算法考虑的点，而后将话题引导到自己动态调整负载均衡算法那里，从而刷一个亮点。
在实践中，选择负载均衡我只有一个建议，不需要考虑那么多，上来就是轮询。等出了问题再说。

不过在面试中回答问题，就还是要装装样子的。选择算法的时候，首先要考虑三个因素：

第一，考虑负载均衡本身的开销。因为负载均衡器是系统的关键组件，所有请求都会经过它，所以我们必须确保负载均衡算法的执行效率足够高，不会成为系统的瓶颈。简单的算法如轮询、加权轮询都能满足这一要求。
综合来说，选择负载均衡算法主要是考虑三个因素：

第一，考虑负载均衡本身的开销。因为负载均衡器是系统的关键组件，所有请求都会经过它，所以我们必须确保负载均衡算法的执行效率足够高，不会成为系统的瓶颈。简单的算法如轮询、加权轮询都能满足这一要求。

第二，考虑负载均衡的均匀性。我们希望请求能够均匀地分布到各个后端服务器，避免某些服务器过载而其他服务器闲置。这就是为什么平滑加权轮询在服务器性能不均衡的情况下更受欢迎。
那么在一些很罕见的情况下，也是要考虑自己研发负载均衡算法的。

我曾经在业务里面设计过一个比较复杂的动态调整权重的负载均衡算法，并以此来提高系统的性能和可用性。

它综合考虑超时、服务端限流、服务端降级、网络异常等多种问题，动态调整不同节点的权重，从而控制流向该节点的流量。

------------------------------

253. 你用过哪些负载均衡算法？, page url: https://www.mianshi.icu/question/detail?id=722
253-722-答案：
简单题，在各个层级的面试中都很常见。

回答这个问题，如果你只是列举什么轮询算法，那肯定寄了。这里让你引出一个高级的负载均衡算法，而后通过解决负载不均衡的问题进一步引出另外一个负载均衡实现，帮助你进一步刷亮点。
你需要先看一下你了解哪些负载均衡算法?（meoying.com)。
我自己用过很多负载均衡算法，从简单的轮询到平滑加权轮询，一致性哈希负载均衡算法都用过。
除了这些烂大街的算法以外，我曾经在业务里面设计过一个比较复杂的动态调整权重的负载均衡算法，并以此来提高系统的性能和可用性。

它综合考虑超时、服务端限流、服务端降级、网络异常等多种问题，动态调整不同节点的权重，从而控制流向该节点的流量。
我还解决过负载不均衡的问题。

我曾经使用过一个动态调整映射关系的一致性哈希负载均衡算法来解决本地缓存命中率过低的问题。

首先，为了提高本地缓存的命中率，我使用了一致性哈希的负载均衡算法，也就是同一个用户的数据会打到同一个节点上。最开始的时候，我使用的是固定分配的哈希负载均衡算法。简单来说就是我把哈希值分成 1024 个槽，而后1024 个槽平均分配到所有的节点上。例如说常规情况下，我有九台实例，也就是每个实例负责 114 个槽。

------------------------------

254. 加权类的负载均衡算法怎么设置节点的权重？, page url: https://www.mianshi.icu/question/detail?id=723
254-723-答案：
略难的题，主要难在如果你没有实践经验，你对权重的认知可能就是权重就是代表处理能力，所以没办法针对设置权重做深入探讨。

在这个问题之下，最好的刷亮点的方式就是讨论各种可以代表权重的东西，并且我给你提供了一些比较高级的案例，你用于面试中能够赢得极大的竞争优势。
权重最重要的一个点就是：权重的绝对值没那么重要，相对值很重要。比如说两个节点 A 权重 100，节点 B 权重 120，和节点 A 权重 1000，节点 B 权重 1200 没什么区别。

而后在实践中虽然一般都说权重代表的是节点的处理能力，也就是综合衡量节点的软硬件资源，但是也不绝对，还有很多种花里胡哨的做法。

第一种是用权重来代表网络通信情况。比如说如果是走专线的，会给予更高权限；
在加权类的负载均衡算法中，服务器节点权重的具体值或者说绝对值并不重要。真正重要的是权重之间的相对值，因为这决定了请求是怎么分配的。在我的经验中，设置节点权重主要有三种方法。

第一种是相对比例法。比如说，如果我们有三个服务器，它们的处理能力比是2:3:5，我们就可以直接用这个比例作为权重。

第二种是性能倍数法。我们可以选择性能最低的节点作为基准，给它权重1，然后其他节点的权重就是它们相对于基准节点的性能倍数。举个例子，如果最弱的服务器性能定为1，其他两个分别是它的1.5倍和2倍，那么权重就可以设为1、1.5、2。这种方法在服务器性能差异较大时特别有用。
在加权负载均衡算法中，权重的意义其实远不止表示服务器的处理能力。它是一个非常灵活和强大的工具，可以帮助我们实现更智能、更精细的流量调度。

首先，权重当然可以反映服务器的性能，包括CPU、内存、磁盘I/O等硬件资源，以及软件性能。但除此之外，它还可以代表很多其他因素。

第二，网络质量也是一个重要因素。我们可以根据服务器之间的网络连接质量，比如带宽、延迟、抖动等指标，动态调整权重。网络状况好的服务器就可以获得更高的权重。
举个例子来说，我在实践中就用过双权重架构，用于支持读写分离，即 CQRS 架构，并且动态调控读写流量。

具体来说，在读写分离的架构中，我们通常会在逻辑上把节点分为读节点和写节点。写操作一般会集中在主节点上，而读操作则可以分散到多个从节点上。比如为每个节点设置两个权重值：读权重和写权重。对于主要用于写操作的节点，我们会设置较高的写权重和较低的读权重。而对于那些主要用于读操作的节点，我们会设置较高的读权重和较低的写权重，通常写权重是零。

举个例子，假设我们有四个节点。我们可能会这样设置权重：一个节点的读权重是10，写权重是90，也就是写节点；其他三个节点节读权重是 90，写权重是 10，也就是读节点。这样设置的话，大部分写操作都会发送到写节点，而读操作则会根据设定的权重分配到各个节点。

------------------------------

255. 负载均衡算法中，动态调整权重可以怎么调？, page url: https://www.mianshi.icu/question/detail?id=724
255-724-答案：
你需要先了解以下内容：
在负载均衡算法中，动态调整权重是一个非常重要的优化策略。我认为需要考虑几个关键因素。

首先，我们要设置合理的权重阈值。这包括最小和最大权重阈值，目的是防止某些服务器被完全排除在负载均衡之外，或者某些服务器承担过多负载。同时，我们还要为每个服务器设置最大负载限制，这样可以避免因为权重调整导致某个服务器过载。

其次，调整频率也是一个需要仔细考虑的问题。如果调整得太频繁，可能会增加系统开销，导致系统不稳定，甚至产生负载抖动，影响服务器性能。但如果调整得太慢，又可能无法及时应对服务器负载的变化。所以，我们需要根据实际情况找到一个合适的调整频率，比如每隔几秒或几分钟调整一次。

我曾经在业务里面设计过一个比较复杂的动态调整权重的负载均衡算法，它综合考虑了超时、服务端限流、服务端降级、网络异常等多种问题，动态调整不同节点的权重，从而控制流向该节点的流量。

------------------------------

256. 负载均衡算法中，动态调整权重应该注意什么事项？, page url: https://www.mianshi.icu/question/detail?id=725
256-725-答案：
略难的题，这个东西需要实践经验才能总结出来，一般在校招和初中级岗位的面试中不常见。

也因此，你在回答中能够说清楚具体的注意事项并且结合案例来描述就可以赢得竞争优势。

负载均衡算法中，动态调整权重一般要考虑一下几个方面：
在负载均衡算法中，动态调整权重是一个非常重要的优化策略。我认为需要考虑几个关键因素。

首先，我们要设置合理的权重阈值。这包括最小和最大权重阈值，目的是防止某些服务器被完全排除在负载均衡之外，或者某些服务器承担过多负载。同时，我们还要为每个服务器设置最大负载限制，这样可以避免因为权重调整导致某个服务器过载。

其次，调整频率也是一个需要仔细考虑的问题。如果调整得太频繁，可能会增加系统开销，导致系统不稳定，甚至产生负载抖动，影响服务器性能。但如果调整得太慢，又可能无法及时应对服务器负载的变化。所以，我们需要根据实际情况找到一个合适的调整频率，比如每隔几秒或几分钟调整一次。
我曾经在业务里面设计过一个比较复杂的动态调整权重的负载均衡算法，它综合考虑超时、服务端限流、服务端降级、网络异常等多种问题，动态调整不同节点的权重，从而控制流向该节点的流量。

------------------------------

257. 为什么要使用负载均衡？, page url: https://www.mianshi.icu/question/detail?id=726
257-726-答案：
简单题。不过这个题目属于突然袭击式题目，也就是大家都默认会使用负载均衡，但是很多人没思考过为什么要使用负载均衡。

负载均衡可以提高系统的性能、可用性和可扩展性，所以要考虑从这几个角度来回答：


因为负载均衡能够提高系统的性能、可用性和可扩展性。首先就性能来说，因为负载均衡会尽可能挑选出来负载比较低的节点，负载低的节点处理请求就会更快，也就是带来更好的性能。这种提升一方面是响应时间比较短，另外一方面则是吞吐量也会比较大。
举个例子来说，我曾经在业务里面设计过一个比较复杂的负载均衡算法。

它是动态调整权重的负载均衡算法的一个变种，它综合考虑超时、服务端限流、服务端降级、网络异常等多种问题，动态调整不同节点的权重，从而控制流向该节点的流量。

------------------------------

258. 你们公司用了什么负载均衡？, page url: https://www.mianshi.icu/question/detail?id=727
258-727-答案：
略难的题，在各个级别里面都可能遇到。

回答这个问题，比较怕的是面试官沿着你提到的各种负载均衡进一步追问，所以你要注意不熟悉你就不要提了，不然就寄了。
你需要先看一下：
以我正在参与的电商平台项目为例，项目最初，我们采用了LVS+Nginx+单体应用集群的组合方案，用LVS负责四层负载均衡，通过DR（直接路由）模式将流量分发到Nginx集群。然后用Nginx负责七层负载均衡并作为反向代理，将请求转发给单体应用集群。这种架构简单可靠，易于维护，它能高效处理并发流量。

随着业务的快速增长，为了应对用户量的不断上涨、日益复杂的业务需求、频繁的发布需求、服务节点的独立部署和动态伸缩等挑战，我们在原有LVS+Nginx的基础上，引入硬件负载均衡设备F5，作为LVS之前的负载均衡进一步提高了转发效率。

还引入了Spring Cloud Gateway作为API网关，并使用Eureka进行服务注册和发现，演进从单体架构演进到微服务架构。

------------------------------

259. 你在日常工作中利用负载均衡能解决过什么问题？, page url: https://www.mianshi.icu/question/detail?id=728
259-728-答案：
简单题。

这个问题其实很累赘，用负载均衡还能是解决什么问题？当然是解决负载不均衡的问题，也就是提高性能、可用性和扩展性了。
负载均衡可以提高系统的性能、可用性和可扩展性，所以要考虑从这几个角度来回答：
我曾经在业务里面设计过一个比较复杂的负载均衡算法，并以此来提高系统的性能和可用性。

它是动态调整权重的负载均衡算法的一个变种，它综合考虑超时、服务端限流、服务端降级、网络异常等多种问题，动态调整不同节点的权重，从而控制流向该节点的流量。
我曾经使用过一个动态调整映射关系的一致性哈希负载均衡算法来解决本地缓存命中率过低的问题。

首先，为了提高本地缓存的命中率，我使用了一致性哈希的负载均衡算法，也就是同一个用户的数据会打到同一个节点上。最开始的时候，我使用的是固定分配的哈希负载均衡算法。简单来说就是我把哈希值分成 1024 个槽，而后1024 个槽平均分配到所有的节点上。例如说常规情况下，我有九台实例，也就是每个实例负责 114 个槽。

但是很不幸的是有部分热点数据集中在了其中一个实例上，导致这个实例的负载就是比别人高。在这种情况下，我设计了一个动态分配槽的算法，并且每分钟重新分配一次。

------------------------------

260. 为什么大型网站需要使用负载均衡?, page url: https://www.mianshi.icu/question/detail?id=729
260-729-答案：
简单题，很少问，大部分面试官也默认会使用负载均衡，很少额外提问为啥要搞负载均衡。

本质上就是为了提高可用性、性能和扩展性，只不过你在回答的时候要多说几句话，水一水时间秀秀谈吐。

下面是个大型网站在各个架构层次上的负载均衡示意图：
首先，大型网站面临的最大挑战之一就是海量的并发访问。随着用户数量的增加，单台服务器很快就会达到其处理能力的极限。这时，我们就需要引入多台服务器来分担负载。负载均衡器的作用就是将这些访问请求合理地分配到多台服务器上，确保每台服务器都能高效工作，而不会出现某些服务器过载而其他服务器闲置的情况。

其次，负载均衡能够提高系统的可用性和可靠性。在我们的项目中，我们经常遇到服务器需要进行维护或者升级的情况。如果没有负载均衡，这可能会导致整个网站的服务中断。但有了负载均衡，我们可以轻松地将流量从需要维护的服务器转移到其他正常运行的服务器上，用户甚至不会察觉到有服务器下线了。

再者，负载均衡为系统提供了更好的扩展性。随着业务的增长，我们经常需要增加新的服务器来处理增加的流量。有了负载均衡，我们可以很容易地将新服务器加入到现有的服务器集群中，负载均衡器会自动将部分流量分配到新服务器上，实现了系统的平滑扩展。

------------------------------

261. ziplist 的连锁更新是什么？怎么解决？, page url: https://www.mianshi.icu/question/detail?id=731
261-731-答案：
略难的题，难在你可能比较难理解连锁更新的过程，这个问题在问到了 ziplist 之后差不多可以说是必面了。

回答这个问题，你只需要清晰解释出来连锁更新就可以，并且提及 Redis 在后面引入了 listpack 来规避 ziplist 的连锁更新问题。
下图展示了连锁更新的问题：


连锁更新是因为 entry 记录了前一个 entry 的长度，也就是有 prevlen 字段。但是 prevlen 字段在设计的时候，如果前一个 entry 的长度小于 254，那么用 1 个字节来记录，否则用 5 个字节来记录。在这种情况下，就会出现如果要是前一个 entry 的长度从小于 254 变成超过 254，那么当前 entry 的 prevlen 就从 1 个字节变成 5 个字节，自身长度增加了 4。万一当前 entry 的长度也因此从小于 254 变成 大于 254，就会导致下一个 entry 的 prevlen 长度从 1 个字节变成 5 个字节。如此循环往复，直到整个 ziplist 的 entry 都更新一遍。

每一次更新， ziplist 都要重新分配内存，并且把原本的数据复制过来，连锁更新多次，那么就复制多次，性能极差。

------------------------------

262. Redis 中的 listpack 是什么？, page url: https://www.mianshi.icu/question/detail?id=732
262-732-答案：
略难的题，难在你看一般的八股文，它们可能根本没有相关的内容，以至于你是在这里第一次听说到。

回答这个问题，只需要重点强调 listpack 和 ziplist 比起来，改进点在哪里。当然，如果你要是胆子大，就可以顺便喷一下 ziplist 的垃圾设计。
在这之前，你需要先了解 ziplist
listpack 说穿了也是一种紧凑型的设计，而且它的设计和 ziplist 比起来，就改了一个东西：

两者的结构对比如下图：
listpack 说穿了也是一种紧凑型的设计，每个元素有三个字段。

第一个字段是 encoding，用来描述 content 字段的类型和长度。

第二个字段就是 content，它就是一个字节数组，具体是什么含义取决于 encoding 字段。
listpack 查找元素和遍历都比较简单。

如果是从后往前遍历，从 listpack 的最后一个字节开始，先找到最后一个元素的长度。而后跳转对应的长度，就到了上一个 entry 的位置。

------------------------------

263. Redis 的 listpack 是如何从后往前遍历的？, page url: https://www.mianshi.icu/question/detail?id=733
263-733-答案：
略难的题，从 listpack 中衍生出来的，一般来说只有你和面试官深入讨论 Redis 的数据结构，并且这个面试官还对 Redis 比较了解，才会遇到这个问题。

listpack 中唯一的理解难点就是这个从后往前遍历，你能回答出来就可以赢得竞争优势了。
你可以先看 listpack 这个问题：Redis 中的 listpack 是什么？ (meoying.com)
如果是从后往前遍历，从 listpack 的最后一个字节开始，先找到最后一个元素的长度。

这里最关键的就是如何读取 back-len 的值。简单来说就是从后往前，如果字节的第一个比特是1，那么就说明这个字节是 back-len 的一部分；如果是 0，就说明 back-len 已经结束。

------------------------------

264. 在将 Redis 用作缓存中，先更新 Redis 还是先更新数据库？, page url: https://www.mianshi.icu/question/detail?id=734
264-734-答案：
简单题，高频题目，不管在什么层次的面试里面，如何解决数据一致性都是一个热点问题，长盛不衰。

你在回答这个问题的时候，可以先说标准答案先更新 DB，而后就要批判这个方案，从而引出其它解决方案，最终则是总结这些所有方案，都只是追求最终一致性而已。
你先读一下这个问题在分布式系统中，为什么说数据一致性问题难以解决？ (meoying.com)，而后再看这个问题在使用 Redis 作为缓存时，如何解决数据一致性问题？ (meoying.com)

如果面试官已经限定了是更新 DB 和更新 Redis，那么你标准答案肯定是先更新缓存，后更新 Redis。道理很简单，如果要是先更新 Redis，后更新 DB，那么如果还没更新 DB 程序就崩了，那么数据就没了。但是这个时候 Redis 还有，就会出现业务刚开始看到了最新的数据，过了一会你说最新数据没了，交代不过去。

先更新 DB 显然既有并发更新又有部分失败的问题，下图是并发更新的问题：
优先考虑先更新 DB。因为先更新 Redis 有一个很棘手的场景，就是更新 Redis 成功了，但是更新了 DB 失败。那么别的业务就有可能读取到 Redis 上的新数据，执行业务。过了一会 Redis 数据过期，业务再次查询的时候发现这个数据没了，那么就出现了严重的不一致问题。

但是先更新 DB 也会有很多问题，最主要的就是并发更新问题和部分失败的问题。就并发更新来说，如果有一个线程1 先于线程 2 更新DB 但是后更新 Redis，而另线程2 后于线程 1 更新 DB 却先更新 Redis，这就出现了不一致的问题。

当然部分更新的问题可以考虑引入版本号来解决。相当于每次更新 DB 的时候拿到一个版本号，而后在更新的 Redis 的时候，只有版本号更大的才能更新成功。这可以借助 lua 脚本来完成，因为要保证检查版本号、更新两个步骤中间不能插入别的操作。这种版本号的做法的缺点就是引入版本号会让实现变得复杂，并且 lua 脚本的性能相比普通的 Redis 操作来说，还是慢很多的。
当然，除了先更新 DB 后更新 Redis 这种方案，也可以考虑先更新 DB 后删除 Redis，或者延迟双删，或者使用 write-back 这种缓存模式。相比来说，我个人倾向于使用先更新 DB 后删除 Redis 的方案，因此从数据一致性的角度来说，这种方案不需要额外的措施都很难出现不一致的情况。
总的来说，可以说在分布式系统下，但凡涉及到多个中间件的数据一致性，就难以保证强一致性，只能是保证最终一致性，并且尽可能把不一致的时间缩短。不一致的根源有两个：并发更新和部分失败。

------------------------------

265. 在分布式系统中，为什么说数据一致性问题难以解决？, page url: https://www.mianshi.icu/question/detail?id=735
265-735-答案：
难题，在校招、初中级岗位的面试中，很少会问这么抽象的问题，一般都是具体问某个场景下怎么解决数据一致性的问题。

你在这个问题之下，要把握住：并发问题和部分失败是数据一致性问题难以解决的两个根源。而部分失败，在当下的分布式环境下是一个无法解决的问题。你在面试中能够阐释清楚这些理论就足以证明你对分布式系统有非常深刻的理解了。
分布式系统下，数据一致性问题难以解决的两个根源就是并发更新和部分失败。这里我用 Redis 和 MySQL 这个典型的例子来说明这两个根因的影响，并且进一步假设我们的操作是先更新 MySQL，再更新 Redis。

并发更新很好理解，就是在分布式环境下，有多个线程同时更新同一个数据。这些线程可以在同一个机器上，也可以不在一个机器上。并发更新的问题很容易理解，下面是一个例子：

分布式环境下，数据一致性难以保证主要是由两方面造成的。

一方面是并发更新，也就是有多个线程并发更新同一个数据。举个例子，比如说先更新 MySQL 再更新 Redis 这种缓存更新方案。那么并发更新容易出现的就是线程1 先更新 MySQL，但是后更新 Redis；而线程 2 后更新 MySQL 反而先更新 Redis。这就会导致 MySQL 和 Redis 的数据不一致。
这里值得讨论的就是分布式事务能不能解决部分失败的问题。目前来看，所有的分布式事务解决方案寻求的都是最终一致性。并且如果进一步思考的话会发现分布式事务连隔离性都没解决，这就会导致同一时刻不同线程可能看到完全不同的数据。

------------------------------

266. 在使用 Redis 作为缓存时，如何解决数据一致性问题？, page url: https://www.mianshi.icu/question/detail?id=736
266-736-答案：
简单题，这个应该说是最基础的数据一致性的问题了。

回答这个问题，不能仅仅回答先更新数据库这种，而是详尽分析各种可行的策略，最终可以引出结论——不管怎么做，都没有办法做到强一致性，只能追求最终一致性。
你可以先看这个问题在分布式系统中，为什么说数据一致性问题难以解决？ (meoying.com)
假设说系统架构就是 Redis 用作缓存，而后 MySQL 作为持久化存储。

那么在更新的数据的时候，有很多种做法。
如果不是寻求强一致性的话，还是有一些办法来解决的。
第三种方式就是所谓的延迟双删了，也就是前一个解决方案的基础上多了一次删除。简单来说就是更新 DB 之后立刻删除缓存，而后睡眠一段时间再次删除缓存，这样就能避开前一种方式造成的不一致。但是极端情况下，也有可能业务在读取了 DB  的老数据之后，隔了好久才回写缓存，刚好绕开了两次删除，那么也会出现数据不一致的问题。真的出现了，就认命，反正缓存是会过期的，到时候重新加载出来就是正确的数据了。
总的来说，可以说在分布式系统下，但凡涉及到多个中间件的数据一致性，就难以保证强一致性，只能是保证最终一致性，并且尽可能把不一致的时间缩短。不一致的根源有两个：并发更新和部分失败。

------------------------------

267. Redis 是如何删除过期数据的？, page url: https://www.mianshi.icu/question/detail?id=737
267-737-答案：
简单题，在校招和初中级岗位中比较常见。

本来 Redis 的过期删除其实没啥好说的，但是因为 Redis 有过一个 BUG 导致后面面试题就经常问了。你在回答的时候可以通过抽象总结一般的缓存如何解决过期问题来赢得竞争优势。
Redis 的过期删除策略非常简单：定时轮询 + 延迟删除。

延迟删除最好理解，就是你在读取 key 的时候，Redis 会检查一下这个 key 有没有过期。如果过期了，Redis 就会删除这个数据，并且给你返回 key 不存在的响应。
Redis 删除数据采取的是定时删除和延迟删除两种策略。
Redis 的这种策略，在大部分缓存实现里面都能见到，比如说本地缓存的实现差不多也是借助定时删除和延迟删除的策略。

------------------------------

268. Redis 在什么情况下会触发渐进式 rehash?, page url: https://www.mianshi.icu/question/detail?id=738
268-738-答案：
简单题，不太常问。

回答这个问题比较容易出现的误区就是只回答了扩容，而实际上扩容和缩容都会出现。要想在这里刷出亮点，就要仔细回答触发扩容和缩容的条件，并且进一步讨论总结缩容和扩容因子选择要注意的问题。
Redis 不管是扩容还是缩容，都会触发渐进式 rehash。
Redis 不管是扩容还是缩容，都会触发渐进式 rehash。
实际上，在实践中比较关键的就是这个扩容和缩容的阈值。

就扩容来说，如果要是负载因子比较小就开始扩容，那么会浪费内存；如果要是负载因子比较大才开始扩容，那么哈希冲突会比较严重。

------------------------------

269. Redis hashtable 扩容/缩容之后的容量如何计算？, page url: https://www.mianshi.icu/question/detail?id=739
269-739-答案：
简单题。

不管是扩容还是缩容，新的容量都是 2 的幂，你可以从两个角度来刷亮点。第一个是为什么使用 2 的幂，第二是一直使用 2 的幂来扩容有什么缺点。


不管是扩容还是缩容，都是一个固定的算法：假设现在有 N 个元素（不是容量），那么就新容量就是第一个比 N 大的 2 的幂。
不管是扩容还是缩容，都是一个固定的算法：假设现在有 N 个元素（不是容量），那么就新容量就是第一个比 N 大的 2 的幂。
之所以使用 2 的幂，是因为在求余数的时候，可以直接使用位运算，性能更高。

------------------------------

270. 在 Redis 的渐进式 rehash 过程中，如果我要读取一个数据，Redis 怎么读？, page url: https://www.mianshi.icu/question/detail?id=740
270-740-答案：
简单题，在讨论了 rehash 的时候，就会深入讨论。在校招和初级工程师中比较常见。
你可以在Redis 如何实现字典/hashtable（哈希表） 的？ (mianshi.icu)中找到 hashtable 的详细内容。
Redis 会先查找 ht[0]，也就是旧哈希表。如果找到了，直接返回。

------------------------------

271. 在 Redis 的渐进式 rehash 过程中，如果我执行 SET 命令，Redis 会怎么处理？, page url: https://www.mianshi.icu/question/detail?id=741
271-741-答案：
简单题，在讨论了 rehash 的时候，就会深入讨论。在校招和初级工程师中比较常见。
你可以在Redis 如何实现字典/hashtable（哈希表） 的？ (meoying.com)中找到 hashtable 的详细内容。
Redis 会先找ht[0] 再找 ht[1]，看看有没有同样的 key，如果找到了就执行 SET 操作，使用新值覆盖旧值。

------------------------------

272. Redis 的持久化机制是如何运作的？, page url: https://www.mianshi.icu/question/detail?id=742
272-742-答案：
略难的题，Redis 面试一般比较少考察持久化机制，因为在实践中就不推荐开启持久化，只有在一些很罕见的情况下才会考虑开启持久化功能。

你在这个问题下，可以深入阐述讨论 AOF 的刷盘机制，以及 COW 机制来赢得竞争优势。
Redis 的持久化机制主要有两个：
Redis有两种主要的持久化机制：RDB（Redis DataBase）和AOF（Append Only File）。

RDB通过创建数据库快照，将内存中的数据以二进制形式保存到磁盘上的RDB文件中。触发方式包括手动（SAVE或BGSAVE命令）和自动（配置文件设置）。优点是文件体积小、恢复速度快，适合灾难恢复。缺点是可能存在数据丢失风险（取决于快照间隔），且生成快照时会有很大的性能开销。

RDB 本身也是进行全量主从复制的核心结构。
这里值得一提的是 AOF 的刷盘机制，它支持三种策略：always 每次写操作后立即同步、everysec 每秒同步一次、no 由操作系统决定。
另外一个很有意思的点是RDB持久化中采用的COW（Copy-On-Write）思路是指在创建数据库快照时，并不直接复制整个数据集，而是仅在数据被修改时才复制相应的页面。这种机制通过延迟复制，减少了不必要的写操作和内存消耗，提高了效率。

------------------------------

273. 什么是限流？, page url: https://www.mianshi.icu/question/detail?id=743
273-743-答案：
简单题，在各个层级里面都很常见，在面试微服务架构的时候，差不多是必面的题。

在这个问题之下，你要尽可能提及自己知道的，和限流有关的东西，作为引导。而后结合实践来谈自己的限流案例，并进一步总结限流和别的治理措施的共同点，赢得竞争优势。

限流（Rate Limiting）是一种重要的系统保护机制，用于控制系统接收和处理请求的速率。其主要目的是在面对高并发或资源受限的情况下，通过限制请求流量，防止系统过载，保障服务的稳定性和可用性。限流可以看作是一种主动防御机制，通过牺牲部分请求来确保系统的整体性能和可用性。



限流算法可以分为“静态”算法：


和“动态”算法 ——通常是基于反馈的、自适应的、优先级的限流，即根据系统的实时负载状况、关键指标、优先级等动态调整限流策略。通常是基于上述“静态”限流算法改造来实现。
（综述）限流是一种非常重要的系统保护机制。它的主要作用是控制系统处理请求的速度，特别是在面对高并发或者资源紧张的情况下。通过限制请求流量，可以有效防止系统过载，保证服务的稳定性和可用性。我个人认为，限流就像是系统的一道防护墙，通过牺牲一部分请求来确保整个系统的正常运行。

（限流目的）说到限流的目的，主要有几个方面：首先是防止系统过载，控制请求速率可以避免突发流量导致系统崩溃。其次是保护系统资源，防止某些请求占用太多资源影响其他请求。还有就是防止恶意攻击，比如DDoS攻击，通过限制请求频率可以起到一定的防御作用。另外，限流还能保障服务质量，确保每个请求都能得到稳定、可靠的服务。在访问高峰期，限流可以帮助我们平滑请求曲线，把超出系统承载能力的请求延后处理或者拒绝掉。对于一些按量计费的云服务，限流还能帮助我们控制成本。最后，限流还能确保不同用户或应用程序之间公平地使用系统资源。

（算法与优缺点）常见的限流算法主要静态和动态算法两类。静态算法主要有四种：固定窗口计数器、滑动窗口计数器、漏桶算法和令牌桶算法。每种算法都有自己的特点和适用场景。比如固定窗口计数器实现简单，性能高，但可能在窗口边界出现突发流量。滑动窗口计数器则能提供更平滑的限流效果，但实现起来稍微复杂一些。漏桶算法适合平滑输出流量，但对突发请求的处理不太友好。令牌桶算法则更灵活，允许一定的突发请求，但需要精确的时间控制。动态算法通常是在静态算法的基础上，根据系统的实时负载状况、关键指标或者优先级来动态调整限流策略。这种方法更加灵活，能够更好地适应系统的实际情况。
在我的实际工作经验中，我使用过以下限流策略：

比如说在 Redis 作为缓存的架构中，当请求达到限流阈值时，可以让这些被限流的请求只访问 Redis，而不再访问后端的主数据库或其他较重的计算逻辑。当然，正常的请求还是走 Redis、数据库这条路。

这种策略的优点是可以大大减轻后端系统的压力，同时还能保证用户快速获得响应。只有恰好 Redis 中没有缓存数据，并且这个请求又是触发了限流的情况，用户会拿不到数据。但是可以预期没有被限流的请求会把缓存重新加载好，保证用户体验。
从服务治理的角度来说，熔断、限流和降级之间其实并没有什么本质区别。

------------------------------

274. 为什么需要限流？, page url: https://www.mianshi.icu/question/detail?id=746
274-746-答案：
简单题，在校招、初中级岗位中比较常见。

回答这个问题的时候，可以使用一个具体例子来证明你使用限流解决了问题，提高了系统可用性。如果这个例子很不错的话，就能够刷出闪瞎眼的亮点。
你需要先看一下什么是限流（meoying.com）

限流主要有以下好处：


上面这些点太多了，你随便记几个来回答就可以了，最关键的保护系统你回答出来了就差不多了。
首先，限流可以保护系统资源，能够防止单个用户或者客户端过度消耗CPU、内存和网络带宽等系统资源，避免服务器性能下降或者崩溃的问题。

第二，限流可以保障服务稳定，在高负载或突发流量的情况下，限流能够有效地平滑流量，防止系统过载并能够为每个请求提供稳定、可靠的服务体验，避免因系统过载而导致的延迟响应。

第三，限流可以防止恶意攻击，当一些恶意用户频繁发起请求，限流能够在一定程度上减少恶意请求对系统带来的影响。通过限制请求速率，系统可以有效抵御恶意流量，保护系统的安全性。
举个例子来说，我以前就利用限流设计过一个比较有意思的缓存方案。

在这个方案里面，先利用 gRPC 的 interceptor 机制，接入了令牌桶限流算法。设置得是单机限流 5000/s。如果触发限流了，那么不是直接拒绝请求，而是给请求打上一个标记，代表它已经是被限流了。

而后在查询接口里面，先查询 Redis。如果 Redis 有数据，则直接返回。

------------------------------

275. 常见的限流算法有哪些？, page url: https://www.mianshi.icu/question/detail?id=749
275-749-答案：
简单题，在校招和初中级岗位面试中比较常见。

在这个问题之下，你除了回答常见的限流算法，还可以进一步谈及自己设计过的一些花里胡哨的限流算法。
常见的限流算法主要包括以下几种：
常见的限流算法主要有以下几种：

首先是固定窗口算法，它的特点是实现起来比较简单，但在窗口边界时可能会出现问题，可能在短时间内放出阀值两倍的请求。

其次是滑动窗口算法，它解决了固定窗口算法的“窗口边界”问题，非常适合在特定的时间段内限制请求数量。具体来说优可以分为滑动窗口计数算法和滑动窗口日志算法，它们只是实现细节不同；
当然，在实践中我还设计过独特的限流算法，它本质上就是动态限流的一种实现。

比如说我曾经维护过一个手动关联数据的服务，它需要的是内存，并不是传统意义上的 IO 密集型服务。而有些请求消耗很少的内存，有些请求消耗很多的内存，所以难以设置限流的阈值。

因此我每秒查询一次机器的内存使用量，如果内存使用量超过 80%，那么就会限流 1s，之所以保持 1s 是因为消耗很多内存的请求其实处理很快，因此保持 1s 的限流状态，大概率内存使用量就降下去了。在这限流的期间，返回一个请求就允许接收新请求。

------------------------------

276. 什么是固定窗口限流算法？, page url: https://www.mianshi.icu/question/detail?id=750
276-750-答案：
简单题，在校招和初中级岗位面试中比较常见。

在这个问题之下可以深入讨论拒绝策略，以及限流也无法根治过载来刷亮点。

固定窗口（Fixed Window）限流算法，也叫做固定窗口计数器（Fixed Window Counter）是一种简单而常用的限流方法，核心思想是控制在固定时间窗口内的请求数量，即基于固定的时间窗口来统计请求数量，并根据预设的阈值来决定是否允许请求通过。

该算法一般可用于控制客户端访问服务的速率，保护系统资源不被过度消耗。


固定窗口限流算法的工作原理大致如下：
固定窗口限流算法。这是一种简单而常用的限流方法。它的核心思想是控制固定时间窗口内的请求数量。简单来说，就是通过固定的时间窗口来统计请求数量，然后根据设定的阈值来决定是否允许请求通过。这种算法主要用于控制客户端访问服务的速率，从而保护系统资源不被过度消耗。

算法的优点是实现起来非常简单，逻辑也很直观，易于理解。而且性能高效，因为计数器的操作速度很快，对系统性能的影响很小，内存占用也低，只需维护一个计数器。

不过，它也有一些缺点。首先，它缺乏流量整形机制，不能确保请求能够连续、平滑地传递到下游服务。这是因为固定窗口计数器算法的放行速率和流量的涌入速率是相同的，所以在处理不规则或者突发流量时，它的效果就不佳。
然而，固定窗口限流算法只是说了怎么限流，但是并没有规定拒绝策略应该是怎样的，而这个也是我们搞服务治理根据业务可以灵活发挥的地方。我举几个例子：

首先，针对被限流的请求，重试机制是一种常用的方法。这意味着对于被拒绝的请求，我们可以设置一个重试策略，比如使用指数退避算法。在这种情况下，用户的请求会在一定时间后自动尝试重新发送，这样可以让用户在流量恢复时继续享用服务。

第二，对于非实时性要求高的请求,可以采用异步处理的方式，被限流的请求可以放入队列中，等到系统负载降低后逐一处理。这方法能平滑请求流，减少用户直接被拒的情况，从而改善体验。
然而， 不管使用什么限流算法，不管使用怎样的拒绝策略，系统偶发性过载不可避免。因为限流算法要么是限制请求的数量，要么是限制请求的速率，这两种方式都没有考虑下游服务器的真实负载情况，即便调整限流算法基于下游服务器的负载状况动态调整限流策略还是会出现偶发性负载。

------------------------------

277. 什么是滑动窗口计数器限流算法？, page url: https://www.mianshi.icu/question/detail?id=752
277-752-答案：
简单题，在校招和初中级岗位的面试中非常常见。一方面，你要掌握理论，另外一方面，你也要练习练习手写代码，毕竟有些时候面试官可能会让你手写代码。

注意一点的是，滑动窗口计数器限流算法是滑动窗口算法的一种实现方式。你在这个问题之下，可以通过讨论拒绝策略和限流的局限性——即无法彻底解决偶发性的负载不均衡的问题。

PS：暂时不要发布，因为正常滑动窗口限流算法不会讨论这个实现。这个实现复杂而且没啥特别的优点。
如果面试官问你滑动窗口限流算法，你也可以用这个问题的内容来回答。滑动窗口日志限流算法和滑动窗口计数器限流算法就是滑动窗口限流算法的不同实现方式而已。


滑动窗口计数器限流算法（Sliding Window Counter Rate Limiting）是一种基于时间窗口的限流策略，核心思想是动态跟踪最近一段时间内的请求数量，以适应不均匀的流量，从而有效避免了固定窗口算法所存在的窗口边界问题，同时保持较低的内存开销。


滑动窗口计数器限流算法的工作原理大致如下：
滑动窗口计数器限流算法其实是一个基于时间窗口的限流策略。它的核心思想是动态地跟踪最近一段时间内的请求数量，这样可以更好地应对不均匀的流量。同时，它还有效避免了固定窗口算法常见的窗口边界问题，而且保持了比较低的内存开销。

该算法会预先设定一个阈值，这个阈值表示在每个时间窗口内允许通过的最大请求数量。比方说，我们可以将阈值设定为5。然后它会把时间轴划分为固定长度的窗口，比如说1秒、5秒或者1分钟。需要注意的是，这个窗口不是静止的，而是随着时间向前移动，这样就能对最近一段时间内的请求进行计数。

在这个过程中，我们还设置了一个计数器，用来统计窗口内的请求数量。一旦新的请求到达，算法会先移除那些过期的请求。然后，算法会检查当前时间窗口内有效的请求数量。如果这个数量小于我们设定的阈值，请求就会被允许通过，并且会更新计数器。而如果达到了阈值，就拒绝这个请求。
滑动窗口计数器（Sliding Window Counter ）限流算法只是说了怎么限流，但是并没有规定拒绝策略应该是怎样的，而这个也是我们搞服务治理根据业务可以灵活发挥的地方。我举几个例子：

首先，针对被限流的请求，重试机制是一种常用的方法。这意味着对于被拒绝的请求，我们可以设置一个重试策略，比如使用指数退避算法。在这种情况下，用户的请求会在一定时间后自动尝试重新发送，这样可以让用户在流量恢复时继续享用服务。

第二，对于非实时性要求高的请求,可以采用异步处理的方式，被限流的请求可以放入队列中，等到系统负载降低后逐一处理。这方法能平滑请求流，减少用户直接被拒的情况，从而改善体验。
然而， 不管使用什么限流算法，不管使用怎样的拒绝策略，系统偶发性过载不可避免。因为限流算法要么是限制请求的数量，要么是限制请求的速率，这两种方式都没有考虑下游服务器的真实负载情况，即便调整限流算法基于下游服务器的负载状况动态调整限流策略还是会出现偶发性负载。

------------------------------

278. 什么是漏桶限流算法？, page url: https://www.mianshi.icu/question/detail?id=753
278-753-答案：
简单题，在校招和初中级岗位面试中很常见。

你除了要掌握基本的理论，最好还要试着写一下，万一面试官让你手搓一个出来，你也能写出来。

在这个问题之下，你可以通过讨论拒绝策略，以及限流无法彻底解决偶发性负载不均衡问题来刷亮点，赢得竞争优势。

漏桶（Leaky Bucket）限流算法是一种基于固定容量桶模型的算法，特点是如果倒入水的平均速率超过桶漏水的速率，或者如果一次倒入的水超过桶的容量，则持续泄漏的桶将溢出。


漏桶算法很简单：想象一个有固定容量的漏水桶，桶里存储的是水（请求）。每当有新请求到达时，它们会被加入到桶中。但桶底有一个孔，水（请求）会以恒定的速率漏出，不管桶中有多少水。以下是该算法的示意图和工作原理：


漏桶限流算法，它是基于一个固定容量的桶来控制流量的。可以想象一个漏水的桶，桶底有一个孔，水（也就是请求）会以固定的速度漏出。每当新的请求到达，如果桶没有满，就把请求放进去。反之，如果桶已满，那么新到的请求就会被拒绝，直到有空间可以容纳新的请求。在这个过程中，桶是持续漏水的，也就是请求会以固定的速率发送给服务，这样就能避免由于流量过大造成的系统崩溃。

这个算法的优点有两个。首先，它简单直观，基于桶的模型，容易理解和实现。其次，它能平滑处理突发的高峰流量，避免对下游系统的瞬时冲击。

不过，漏桶算法也有缺点。在高流量的情况下，请求往往需要排队等待，这可能导致用户体验下降。此外，设置恰当的限流阀值也挺困难的。如果阀值设得过高，桶的容量就会变大，这样排队的请求可能超时或延迟响应；而如果设得过小，那系统就会频繁拒绝用户的请求，影响体验。还有，设置合适的漏水速率同样不容易，当速率设置得过高时，可能会导致后端服务过载。但如果设置得太低，桶里的请求会增加，一旦满了，就又会频繁拒绝请求，影响用户体验。

漏桶限流算法只是说了怎么限流，但是并没有规定拒绝策略应该是怎样的，而这个也是我们搞服务治理根据业务可以灵活发挥的地方。我举几个例子：

首先，针对被限流的请求，重试机制是一种常用的方法。这意味着对于被拒绝的请求，我们可以设置一个重试策略，比如使用指数退避算法。在这种情况下，用户的请求会在一定时间后自动尝试重新发送，这样可以让用户在流量恢复时继续享用服务。

第二，对于非实时性要求高的请求,可以采用异步处理的方式，被限流的请求可以放入队列中，等到系统负载降低后逐一处理。这方法能平滑请求流，减少用户直接被拒的情况，从而改善体验。
然而， 不管使用什么限流算法，不管使用怎样的拒绝策略，系统偶发性过载不可避免。因为限流算法要么是限制请求的数量，要么是限制请求的速率，这两种方式都没有考虑下游服务器的真实负载情况，即便调整限流算法基于下游服务器的负载状况动态调整限流策略还是会出现偶发性负载。

------------------------------

279. 什么是令牌桶限流算法？, page url: https://www.mianshi.icu/question/detail?id=754
279-754-答案：
简单题，在校招和初中级岗位面试中比较常见。你一方面要能说出来理论，一方面也要稍微写一下代码，防止面试官让你手写实现。

除此之外，你可以通过讨论拒绝策略，以及令牌桶限流算法无法彻底解决偶发性负载不均衡的问题来刷亮点。

令牌桶（Token Bucket）限流算法是一种基于固定容量桶模型的算法，特点是如果请求进入系统的速率超过令牌的生成速率，或者如果一次进入的请求数量超过桶中的令牌数，则请求被限流。它通过动态限制请求进入系统的速率来实现限流，比如，令牌的生成速率是2个/s，桶的容量是10，那么请求进入系统进入的最大速率是10个/s，平均速率2个/s。


令牌桶算法的基本构思是使用一个“桶”，其中存放了“令牌”，每个令牌允许处理一个请求。系统以固定的速率向这个桶中生成令牌，直到达到最大的桶容量。如果桶已满，新生成的令牌将会被丢弃。由于令牌可以在桶中累积，这使得算法在遭遇短时间内的请求高峰时，依旧能够保持一定的处理能力。


令牌桶限流算法是一种基于固定容量桶模型的算法。它的主要特点是，如果请求进入系统的速率超过了令牌的生成速率，或者一次进入的请求数量超过了桶中的令牌数，那么请求就会被限流。通过这样的方式，算法动态限制请求的进入速率。例如，假设令牌的生成速率是2个每秒，桶的容量是10个，那么请求进入系统的最大速率可以是10个每秒，平均速率则是2个每秒。当桶空时，每秒超过2个就会被拒绝，当桶满的时候，每秒超过10个请求就会被拒绝。

该算法的优点是能够容忍一定程度的突发流量。当请求量骤增时，系统会利用桶中积累的令牌进行处理。然而，它也有一些缺点，比如实现较复杂，需要占用一定的内存和CPU资源。由于缺乏流量整形机制，如果桶容量设置不当，高峰流量会对下游系统造成瞬时冲击。同时，设置合适的限流阀值也是一大挑战。如果阀值过高，桶的容量过大，可能导致下游系统过载；如果容量过小，用户请求会频繁被拒绝，影响用户体验。再比如，如果令牌生成速率设置过高，也可能导致下游服务过载；设置过低则会频繁拒绝用户请求，造成体验不佳。
令牌桶限流算法只是说了怎么限流，但是并没有规定拒绝策略应该是怎样的，而这个也是我们搞服务治理根据业务可以灵活发挥的地方。我举几个例子：

首先，针对被限流的请求，重试机制是一种常用的方法。这意味着对于被拒绝的请求，我们可以设置一个重试策略，比如使用指数退避算法。在这种情况下，用户的请求会在一定时间后自动尝试重新发送，这样可以让用户在流量恢复时继续享用服务。

第二，对于非实时性要求高的请求,可以采用异步处理的方式，被限流的请求可以放入队列中，等到系统负载降低后逐一处理。这方法能平滑请求流，减少用户直接被拒的情况，从而改善体验。
然而， 不管使用什么限流算法，不管使用怎样的拒绝策略，系统偶发性过载不可避免。因为限流算法要么是限制请求的数量，要么是限制请求的速率，这两种方式都没有考虑下游服务器的真实负载情况，即便调整限流算法基于下游服务器的负载状况动态调整限流策略还是会出现偶发性负载。

------------------------------

280. 什么是滑动窗口/滑动窗口日志限流算法？, page url: https://www.mianshi.icu/question/detail?id=755
280-755-答案：
简单题，在校招、初中级岗位面试中非常常见。

注意一点的是，滑动窗口日志限流算法是滑动窗口算法的一种实现方式。你在这个问题之下，可以通过讨论滑动窗口日志限流算法如何处理被限流请求——也就是拒绝策略，以及讨论限流的局限性来刷亮点，赢得竞争优势。
一般来说，如果面试官说滑动窗口限流算法，那么就是指这个算法。但是严格来说，它是滑动窗口日志限流算法，还有一个滑动窗口计数器算法，但是实践中几乎不用，并且面试也不会面了。


你经常看到的滑动窗口算法的实现其实就是滑动窗口日志的实现方式，因为使用简单的计数器是无法区分哪些请求是过期的，所以算法需要维护一个请求时间戳列表，同时通过动态更新来监测在时间窗口内的请求数量，有一点需要注意，这个过程中会先将过期的（即不再窗口范围内的）请求先去掉再统计。





优点：
滑动窗口日志（Sliding Window Log）限流算法是一个基于时间窗口的限流策略。它的核心思想是动态地跟踪最近一段时间内的请求数量，这样可以更好地应对不均匀的流量。该算法实际上是滑动窗口限流算法的一种实现方式，另一种是滑动窗口计数器限流算法。

（算法基本思路）该算法会预先设定一个阈值，这个阈值表示每个时间窗口内允许通过的最大请求数量。然后它会把时间轴划分为固定长度的窗口，比如1秒、5秒、1分钟或者5分钟。需要注意的是，这个窗口不是静止的，而是随着时间向前移动，这样就能对最近一段时间内的请求进行统计。

在这个过程中，我们还设置了一个时间戳日志列表，用来记录请求的时间戳。一旦新的请求到达，算法会先将时间戳日志列表中过期的请求移除，然后统计当前时间窗口内有效的请求数量。如果这个数量小于我们设定的阈值，请求就会被允许通过，并且记录请求的时间戳。而如果达到了阈值，那么就拒绝这个请求。
滑动窗口日志限流算法只是说了怎么限流，但是并没有规定拒绝策略应该是怎样的，而这个也是我们搞服务治理根据业务可以灵活发挥的地方。我举几个例子：

首先，针对被限流的请求，重试机制是一种常用的方法。这意味着对于被拒绝的请求，我们可以设置一个重试策略，比如使用指数退避算法。在这种情况下，用户的请求会在一定时间后自动尝试重新发送，这样可以让用户在流量恢复时继续享用服务。

第二，对于非实时性要求高的请求,可以采用异步处理的方式，被限流的请求可以放入队列中，等到系统负载降低后逐一处理。这方法能平滑请求流，减少用户直接被拒的情况，从而改善体验。
然而， 不管使用什么限流算法，不管使用怎样的拒绝策略，系统偶发性过载不可避免。因为限流算法要么是限制请求的数量，要么是限制请求的速率，这两种方式都没有考虑下游服务器的真实负载情况，即便调整限流算法基于下游服务器的负载状况动态调整限流策略还是会出现偶发性负载。

------------------------------

281. 什么是本地限流？, page url: https://www.mianshi.icu/question/detail?id=756
281-756-答案：
简单题。在校招和初中级岗位面试中很常见。

你在讨论到本地限流的时候提及在负载均衡中设计的动态调整权重的负载均衡算法，以及针对限流会如何处理，赢得竞争优势。
你需要先看一下什么是限流（meoying.com）


本地限流（Local Rate Limiting）是一种在单个服务节点或实例内部实现的限流机制，用于控制该节点或实例接收和处理请求的总量或速率，防止API过度调用、避免性能下降和资源耗尽等情形。它不需要依赖于外部的限流服务或组件，而是直接在应用服务器内部完成流量控制。


优点：
本地限流是一种实现于单个服务节点或实例内的限流机制。它的主要作用是控制单个节点或实例所接收和处理请求的总量或者速率，防止被过度调用、避免性能下降以及资源的耗尽。

本地限流有以下优点，首先，它非常简单易行，不需要复杂的配置和部署，你可以很直接地在代码中实现它。其次，由于限流是在本地节点上进行的，所以处理速度很快，几乎不会增加额外的网络通信开销。此外，它的灵活性也很强，开发者可以根据具体的业务需求灵活调整限流策略，并且针对不同的用户、服务或API设定不同的限流规则。

当然，本地限流也有一些局限性，比如：当系统的规模扩大时，你就需要在每个应用服务器上配置和维护限流规则，这样维护成本会相对较高。另外，如果系统中包含多个服务的话，本地限流就无法有效地控制跨服务之间的调用流量，这点需要特别注意。
使用本地限流的时候，其实有一个隐含的假设，即负载均衡器运作良好。也就是说，如果要是负载均衡效果不好，很容易出现某个节点已经触发了限流，但是别的节点还是很清闲的情况。

------------------------------

282. 什么是分布式限流？, page url: https://www.mianshi.icu/question/detail?id=757
282-757-答案：
简单题，在校招和初级岗位中很常见。

在这个问题下，你可以深入讨论分布式限流之后出现的流量倾斜的问题，分布式限流需要和本地限流结合起来以保护单一节点的必要性，以及分布式限流无法使用换节点的容错机制，进一步赢得竞争优势。

分布式限流是实现全局限流的一种方式。它通过在多个服务实例或节点上协同实施灵活的限流策略来管理局部流量，最终实现对整个系统总体流量的控制和管理。分布式限流通常需要监控和协调多个服务节点间的请求量，以实现全局一致的流量控制。为此，需要在服务实例或节点之间共享限流状态，维护限流信息的一致性，确保所有实例都遵守既定限制。

分布式限流支持灵活多变的限流策略，如常规的令牌桶、漏桶和滑动窗口，以及根据用户角色、API接口等设定的策略。分布式限流还能够随着系统规模的扩大而持续有效地应用，在防止系统过载的同时，确保资源的公平分配和服务质量的稳定。然而，实现分布式限流也面临着限流状态一致性维护、网络延迟带来的数据同步等挑战。通常，可以通过集中式限流服务、分布式缓存或一致性哈希等技术来解决这些问题。


分布式环境下如何实现限流？（meoying.com）
分布式限流是一种在多个服务实例或节点上协同实现全局限流的方法。通过在这些实例或节点间协调灵活的限流策略，我们能够同时管理各个节点的局部流量，最终实现对整个系统流量的控制和管理。

分布式限流的优点总的来说就是提高系统可用性，保障用户体验。具体来说，首先，它能够提供全局的流量控制，确保总流量不会超出系统的整体承载能力。其次，它提高了系统的稳定性和可用性，有效避免高并发和突发流量导致的系统过载、响应延迟等问题，保障系统的稳定运行。并且分布式限流可以防止资源被独占或过度消耗，平滑流量，避免系统延迟和超时问题，为所有用户提供一致且优质的服务，有效提升用户体验与服务质量。

当然，分布式限流也存在一些缺点，复杂度较高，并且性能损耗比较大。比如说，实现起来比本地限流更为复杂，涉及数据一致性和高可用性的管理。在高并发的情况下，可能会引入性能瓶颈，影响系统的响应速度。此外，维护限流状态的一致性在高并发环境中是个挑战，可能需要额外的机制来处理。同时，我们还需要考虑限流系统自身的高可用性，防止限流状态丢失或不可用的情况发生。
但是，使用分布式限流的时候也有一些注意事项。

分布式限流还需要进一步考虑单个节点的负载问题，也就是我们常说的流量倾斜（traffic skew）。在实际应用中，不管怎么搞负载均衡，都还是有可能出现某些节点上的负载就是比别的节点高的情况。如果仅在全局层面进行分布式限流，而不关注单个节点的负载差异，可能会导致个别节点过载，影响整体系统的性能和稳定性。

为了解决这个问题，分布式限流通常需要结合良好的负载均衡策略，将流量尽可能均匀地分配到各个节点上。常用的负载均衡算法有轮询、最小连接数、一致性哈希等，选择合适的算法可以有效减少流量倾斜的发生。

------------------------------

283. 为什么要进行本地限流？, page url: https://www.mianshi.icu/question/detail?id=758
283-758-答案：
简单题，在校招和初中级工程师面试中很常见。

你可以通过深入阐述为什么在使用了集群限流（分布式限流）的情况下，还要使用本地限流。而后你就可以将话题引申到本地限流带来的好处，以及和负载均衡算法结合带来的优势。
你需要先看一下

其实这个问题可以看做是：有了集群限流（分布式限流），为什么你还要使用本地限流？

说白了就是集群限流保护的是整个集群，但是不能完全保护住单个节点。
本地限流可以控制单个节点或实例所接收和处理请求的总量或者速率，防止被过度调用、避免性能下降以及资源的耗尽。

相比其它限流做法，它有实现简单、低延迟、易扩展的好处。

首先，它的实现简单，直接在应用代码中集成限流逻辑，而不需要处理那些复杂的配置或者依赖外部服务。这样就大大降低了开发和维护的复杂性。
即便是在使用了集群限流的情况下，依旧要考虑使用本地限流。

举个例子来说，假设一个集群有 3 个服务端节点，限流 1000/s，那么在最差的情况下，这 1000/s 都落在同一个节点上了，瞬间就打爆这个节点。

因此，本地限流可以有效防范这种极端情况。

------------------------------

284. 为什么要进行分布式限流？, page url: https://www.mianshi.icu/question/detail?id=759
284-759-答案：
简单题，在校招和初中级岗位面试中比较常见。
你需要先看一下什么是分布式限流？（meoying.com），分布式限流也叫做集群限流。
分布式限流在分布式架构和微服务架构中发挥着关键作用，最突出的是两个：提高系统可用性和保障用户体验。

------------------------------

285. 你们公司用了哪些限流策略？, page url: https://www.mianshi.icu/question/detail?id=760
285-760-答案：
略难的题。在社招、中级以上岗位中比较常见。

在这个问题之下，你只要能够将公司从前端到后端的各个环节使用的限流措施列举出来就可以赢得竞争优势了。
先来看一个大型网站的限流方案，如下所示：

总之，针对不同层次的限流策略组合使用，能够有效保障系统的稳定性和可用性。不同层采取的合适限流措施结合在一起，可以更好地应对高并发场景、流量攻击以及资源竞争，提升系统的整体抗压能力。
在我参与的项目中，确实制定了一套相对全面的限流方案，这些策略分布在多个层面上，确保的系统能够稳定运行。

首先，在客户端层面，用户发起请求的地方，我们会设置请求频率限制，也就是限制用户操作的速度，比如在按钮点击上设定最小时间间隔。这种做法在秒杀、抢优惠券这种地方非常常见，能有效减少正常用户发送的请求数量；

接着是接入层，比如内容分发网络，分布在全球的边缘节点，我们会设置请求速率限制，限制每个IP的访问频率。此外，还会对同时连接数进行限制，甚至根据用户地理位置来控制访问，以确保我们能够平衡负载。不过这个主要是借助云服务商来实现的，他们一般都提供了类似的服务；

------------------------------

286. 固定窗口限流算法有什么优缺点？, page url: https://www.mianshi.icu/question/detail?id=761
286-761-答案：
简单题，在校招和初中级岗位面试中常见。


你需要先查看一下什么是固定窗口限流算法？（meoying.com)。
固定窗口限流算法的优点是实现起来非常简单，逻辑也很直观，易于理解。而且性能高效，因为计数器的操作速度很快，对系统性能的影响很小，内存占用也低，只需维护一个计数器。

不过，它也有一些缺点。首先，它缺乏流量整形机制，不能确保请求能够连续、平滑地传递到下游服务。这是因为滑动窗口计数器算法的放行速率和流量的涌入速率是相同的，所以在处理不规则或者突发流量时，它的效果就不佳。流量整形是为了对突发流量进行管理，确保输出的请求在可接受的范围内，从而避免对后端造成冲击。

另外，固定窗口算法对突发流量的容忍性也比较差。如果设置的时间窗口是10秒，阈值是100个请求，那在前1秒内涌入100个请求后，接下来的9s不会再放行请求。即使系统可能在第5秒已经处于空闲状态，具备处理新请求的能力，也只能等到下一个限流周期才行。相比之下漏桶、令牌桶则灵活的多，更能容忍突发流量。

------------------------------

287. 滑动窗口日志限流算法有什么优缺点？, page url: https://www.mianshi.icu/question/detail?id=763
287-763-答案：
简单题，在校招和初中级岗位面试中比较常见。
你需要先查看一下什么是滑动窗口限流算法？（meoying.com)
滑动窗口日志（Sliding Window Log）限流算法的优点是，相比固定窗口限流算法，它确实解决了固定窗口算法的窗口边界问题。

不过，它也有一些缺点。比如，它缺乏流量整形机制，不能确保请求能够连续、平滑地传递到下游服务。这是因为滑动窗口日志算法的放行速率和流量的涌入速率是相同的，所以在处理不规则或者突发流量时，它的效果就不佳。

------------------------------

288. 令牌桶限流算法有什么优缺点？, page url: https://www.mianshi.icu/question/detail?id=764
288-764-答案：
简单题，在校招和初中级面试中比较常见。

令牌桶和漏桶是比较像的，你要注意区别。
你需要先查看一下什么是令牌桶限流算法？（meoying.com)
令牌桶限流算法的优点是能够容忍一定程度的突发流量。当请求量骤增时，系统会利用桶中积累的令牌进行处理。然而，它也有一些缺点，比如实现较复杂，需要占用一定的内存和CPU资源。由于缺乏流量整形机制，如果桶容量设置不当，高峰流量会对下游系统造成瞬时冲击。同时，设置合适的限流阀值也是一大挑战。如果阀值过高，桶的容量过大，可能导致下游系统过载；如果容量过小，用户请求会频繁被拒绝，影响用户体验。再比如，如果令牌生成速率设置过高，也可能导致下游服务过载；设置过低则会频繁拒绝用户请求，造成体验不佳。

------------------------------

289. 漏桶限流算法有什么优缺点？, page url: https://www.mianshi.icu/question/detail?id=765
289-765-答案：
简单题，在校招和初中级岗位面试中比较常见。
你需要先查看一下什么是漏桶限流算法？（meoying.com)
漏桶（Leaky Bucket）限流这个算法的优点是，首先，它简单直观，基于桶的模型，容易理解和实现。其次，它能平滑处理突发的高峰流量，避免对下游系统的瞬时冲击。

不过，漏桶算法也有缺点。在高流量的情况下，请求往往需要排队等待，这可能导致用户体验下降。此外，设置恰当的限流阀值也挺困难的。如果阈值设得过高，桶的容量就会变大，这样排队的请求可能超时或延迟响应；而如果设得过小，那系统就会频繁拒绝用户的请求，影响体验。还有，设置合适的漏水速率同样不容易，当速率设置得过高时，可能会导致后端服务过载。但如果设置得太低，桶里的请求会增加，一旦满了，就又会频繁拒绝请求，影响用户体验。

------------------------------

290. 本地限流和分布式限流有什么区别？, page url: https://www.mianshi.icu/question/detail?id=766
290-766-答案：
简单题，在校招和初级岗位面试中比较常见。

你理解本地限流和分布式限流之后，再来看这个问题就很简单的。
你需要先看一下
两者之间最为关键的区别就在于两点。

第一个点是关注点不同。本地限流重点在于保护单一节点，而分布式限流强调的是保护整个集群。

------------------------------

291. 本地限流适用什么场景？, page url: https://www.mianshi.icu/question/detail?id=767
291-767-答案：
简单题，在校招和初中级岗位面试中比较常见。

在这个问题之下，你可以深入讨论本地限流和负载均衡的配合来刷亮点，赢得竞争优势。
你需要先看一下
本地限流适用于所有需要对服务提供方的单一节点或实例进行保护的场景，防止因过请求量过大或请求速率过快导致服务崩溃或性能下降。其特点是实现简单、性能好、灵活性高可以基于业务定制。

比如：对于前端应用来说，它作为后端服务调用方可以通过本地限流防止用户频繁刷新页面或提交表单内容，减轻后端服务的压力，从而提升系统的稳定性和响应速度。
使用本地限流的时候，其实有一个隐含的假设，即负载均衡器运作良好。也就是说，如果要是负载均衡效果不好，很容易出现某个节点已经触发了限流，但是别的节点还是很清闲的情况。

------------------------------

292. 分布式限流适用什么场景？, page url: https://www.mianshi.icu/question/detail?id=768
292-768-答案：
简单题，在校招和初级岗位中比较常见。

类似地，在分布式限流有关的场景下，你只需要深入讨论分布式限流有可能出现流量倾斜的问题，以及分布式限流需要和本地限流结合就足以赢得竞争优势。
你需要先看一下：
我认为分布式限流主要适用于需要在全局范围内对请求流量进行精确控制的场景，一句话就是分布式系统最好都接入分布式限流。比如说：

首先，分布式限流适用于有高并发和高可用性要求的系统。在这种系统中，我们需要控制整体的请求量，以确保系统稳定性和用户体验。比如，大型电商平台在促销活动期间可能会遇到海量用户访问。通过分布式限流，我们可以在全局范围内控制请求数量，防止服务器被突然涌入的请求压垮，从而确保系统的稳定运行。

其次，分布式限流适用于多实例、多节点的环境。在这种环境中，我们需要协调不同节点之间的负载，防止单点过载。举个例子，在采用微服务架构的系统中，服务可能部署在多台服务器甚至跨多个数据中心。通过分布式限流，我们可以在各个服务实例之间协调请求流量，均衡负载，避免某个节点因负载过高而成为系统瓶颈。
但是，不管在什么场景下使用了分布式限流，都要进一步考虑分布式限流的固有缺陷，即难以避免会有流量倾斜的问题，造成部分节点的负载极高。

------------------------------

293. 分布式环境下如何实现限流?, page url: https://www.mianshi.icu/question/detail?id=769
293-769-答案：
简单题。

如果要想回答出来，其实很简单回答借助 Redis 来限流就可以。但是如果要想刷出亮点，就要回答一些奇诡的做法，比如说借助分布式协调算法来实现的限流。只不过这种限流方式在平常几乎见不到而已。
你需要先看一下：

后面所有的限流方式，你记不住都没有关系，但是这个是肯定要记住的。

基于共享存储的限流方案是通过在多个服务实例之间共享限流状态，实现全局的一致性限流控制。所有的服务实例都访问同一个共享存储，以获取和更新限流状态数据，从而实现对整体请求流量的控制。一种可能的实现方式如下图所示：
在分布式环境下，有多种限流方案。
除了前面两种，还有别的限流做法。

比如说基于负载均衡的限流也是一种常见的方法。通过在负载均衡器层面实现限流策略，对进入系统的请求进行统一的流量控制。负载均衡器作为系统的入口，可以集中地监控和管理所有流入的请求，防止异常流量对后端服务造成影响。同时，结合在每个应用服务器上进行本地限流，提供了双层保护机制。优点是策略统一，灵活性高，能根据不同的服务器处理能力设置限流策略，简单易实现，本地限流可以快速做出决策，减少延迟。但缺点是在分布式环境中，保持全局限流和本地限流的一致性具有挑战，配置和管理也较复杂，可能造成资源浪费，限流的精度可能较低，扩展性有限。这种方案适用于中小规模的分布式系统，或者不同服务器处理能力差异较大的异构系统。

当然，还可以进一步考虑基于可观测性和负载均衡的动态限流。通过实时监控系统的性能指标和负载状况，结合负载均衡策略，动态调整限流策略和流量分配方式。这样可以根据系统的实时状态，智能地控制流量，既保证系统的稳定性，又最大化利用系统资源。它的优点是自适应性强，资源利用最大化，精细化控制，智能决策。但缺点是实现复杂度高，需要完善的监控、分析、自动化和负载均衡体系，数据依赖性强，可能存在响应滞后风险。这种方案适用于大型分布式系统，如电商、视频直播、社交平台等，对资源利用率和性能有高要求的应用场景。

------------------------------

294. 如何处理突发流量？, page url: https://www.mianshi.icu/question/detail?id=771
294-771-答案：
略难的题，一般出现在社招的中高级岗位面试中。

在这个问题之下，要仔细区分突发流量是瞬时的，还是会持续一段时间；是可预期的，还是不可预期的，要从限流等各个角度全方位做好系统可用性的保障工作。



在互联网应用中，突发流量是不可避免的现象。无论是由于热门事件引发的流量激增，还是恶意攻击导致的异常请求，系统都需要具备应对突发流量的能力。合理的限流策略可以帮助系统在高并发情况下保持稳定，同时保障核心功能的可用性。


在制定限流方案之前，首先需要明确突发流量的性质和规模。这有助于我们选择合适的应对措施。


对于短暂且可控的流量激增，系统通常可以通过调整限流算法和策略来应对：
在处理突发流量下的限流问题时，需要区分正常的突发流量和异常流量，以便采取针对性的措施：

首先，对于小规模、短时间的突发流量，这类流量通常在短时间内激增，但峰值并没有大幅超出系统的最大承载能力，持续时间也相对较短，可能是因为某个热门话题或短期活动引起的。针对这种情况，需要调整限流算法和限流策略来应对。具体来说，我可能会选择令牌桶算法限流，因为它的优势在于允许一定程度的突发请求，同时控制整体的请求速率。同时，我会制定合理的限流策略。当请求超过限流阈值时，可以采用等待-超时机制，让请求在队列中等待处理。如果等待超时，就返回错误信息。这在一定程度上缓解了瞬时压力，但需要控制队列的长度，以防资源耗尽。也可以将请求放入Kafka进行异步处理，客户端无需等待处理结果，从而削峰填谷，提高系统的吞吐量。如果系统资源非常紧张，还可以采用快速失败的策略，超过限流阈值后立即拒绝请求并返回友好的错误信息，这样可以保护系统资源，但需要权衡用户体验。
对于预期内的流量高峰，比如秒杀、促销活动等，我们可以提前规划容量和准备应对方案。首先，要优化系统配置并进行容量预估。通过优化线程池、数据库连接池和缓存等配置，可以提升系统的处理能力。基于历史数据和业务增长的预期，适当预留系统容量，确保在短时间的流量高峰期，系统仍能稳定运行。

其次，进行压力测试也很重要，通过模拟高并发场景，测试系统在高负载下的稳定性和性能表现。根据活动的预期，提前增加系统资源和带宽，确保有足够的处理能力。
由于无法预知突发流量何时到来，因此必须确保系统具备良好的可用性和伸缩性。当流量突然增加时，系统必须能够迅速响应并自动调整资源，从而保持稳定的性能，确保用户获得良好的服务体验。关键在于有效利用监控、告警以及自动扩容和缩容机制，例如：

首先，需要建立一个完善的监控体系，实时收集系统运行的各项指标。这包括CPU使用率、内存使用率、网络带宽、请求响应时间以及错误率等。通过这些关键数据，我们能够快速了解系统的当前状态，并及早发现潜在的问题。

在监控的基础上，需要设计合理的告警机制。当任何关键指标超过设定的阈值时，系统应自动发出告警。这些告警可以通过多种渠道（比如电子邮件、短信或办公软件的通知）发送给相关团队，以确保他们能够及时响应并迅速采取行动。

------------------------------

295. Redis 是单线程的吗？, page url: https://www.mianshi.icu/question/detail?id=772
295-772-答案：
简单题，但是是一个陷阱题。在校招、初中级岗位面试中非常常见。

回答这个问题的关键点就是指出 Redis 的线程模型的演进，而后强调一下多线程的模式不到逼不得已不要使用。
这个题目之所以说是陷阱题，是因为 Redis 的线程模型有过一次比较重大的变化：

下图是 IO 多线程 + 单线程的模型：


Redis 并非完全意义上的单线程。虽然命令处理是单线程的，但它的持久化、异步删除等操作是使用额外的线程或进程来执行的。这种设计使得 Redis 能够充分利用多核 CPU 的性能，同时保持单线程模型带来的简单性和可预测性。

------------------------------

296. Redis 的 IO 模型是怎样的？, page url: https://www.mianshi.icu/question/detail?id=773
296-773-答案：
简单题。

所有问到中间件 IO 模型的，你不要怀疑就直接答 IO 多路复用，这个话题很有可能将问题导向 IO 多路复用，你要有一个心理准备。
你可以在这里找到 IO 多路复用的内容：什么是 IO 多路复用？ (meoying.com)

以 6.0 开启了多线程模型，并且允许多线程读请求为例，IO 模型如下图：


Redis 的 IO 模型就是一个典型的 IO 多路复用。
总的来说，也不仅仅是 Redis，IO 多路复用已经是中间件的标配了。

------------------------------

297. Redis 为什么那么快？, page url: https://www.mianshi.icu/question/detail?id=774
297-774-答案：
略难的题，之所以略难是因为这个题目很容易错，在校招和初中级岗位中比较常见。

Redis 之所以那么快，最关键就是两个：纯内存操作和IO多路复用，剩下的都不值一提。很容易犯的错误就是说 Redis 那么快是因为用了单线程模型，这个有点扯淡，要是单线程是原因，那么你的 Kafka 干嘛不用单线程？
Redis 之所以那么快，最关键就是两个：纯内存操作和IO多路复用。

Redis 是一个基于内存的数据库，所以说它的绝大部分操作都是内存操作，也就是 CPU 转一下的问题。而作为对比，Kafka 之类的中间件其实是 IO 密集型的应用，动不动就咔咔写磁盘，天王老子来了性能也比不过 Redis。

此外就是 Redis 用的是 IO 多路复用，这个算是高性能中间件的标配了。
Redis 高性能的最主要因素有两个：高效的 IO 多路复用和纯内存操作。

首先，IO 多路复用是高性能中间件的标配，Redis 基于 IO 多路复用可以支撑非常多的连接，并且高效读写数据。

其次 Redis 是基于内存的数据库，大部分操作都是内存操作，天然就非常快。作为对比，Kafka 这种 IO 密集型的应用，性能始终不如 Redis。
但是我要澄清一个问题。就是我们业界里面有一种错误的说法，说 Redis 的单线程模型是 Redis 高性能的原因。

这句话其实不对，倒果为因了。虽然 Redis 的主线程是单线程的，确实在一定程度上提高了 Redis 的性能。

------------------------------

298. Redis 的过期时间应该怎么确定？, page url: https://www.mianshi.icu/question/detail?id=775
298-775-答案：
简单题。

虽然大部分情况下你可能都是凭借经验值随便搞一个过期时间，但是在面试中你就要说自己是认真思考过的，不是随便设置的。而后提供一两个案例证明自己确实很会搞过期时间。
你可以先参考这个问题缓存的过期时间应该设置多长？怎么确定？ (meoying.com)
设置缓存的过期时间，从原则上来说是接下来这个数据多长时间内都会被使用，就设置多长时间。

但是这个很难知道，所以设置缓存的过期时间主要就是考虑三个因素。

第一个是平衡内存使用量和缓存命中率。也就是如果要减少内存使用量，就缩短过期时间；如果要提高命中率，就延长过期时间。
当然，还有一些比较奇诡的用法。

比如说我曾经设计过一个业务相关预加载的缓存方案。它的原理就是根据用户的行为来预测用户接下来会访问什么数据，提前加载起来。

我的用法是当时有一个批量接口，如果用户访问第一页的 10 条数据的时候，我这个批量接口会顺手把前 3 条数据缓存起来，即我预测用户看到这一批数据之后大概会查看这三条数据的详细内容。

------------------------------

299. 缓存的过期时间应该设置多长？怎么确定？, page url: https://www.mianshi.icu/question/detail?id=776
299-776-答案：
简单题，在各个层级里面都能见到。

如果你没思考过这个问题，我猜你只能回答说凭借感觉，凭借经验来设置一个过期时间。这里我就要教你确定过期时间的根本因素，以及一些花里胡哨的过期时间的玩法。
从根源上来说，你缓存的数据大概率会在多久内会被再次访问，就缓存多长时间。举个例子，你认为某个数据在半小时内会被频繁访问，你就设置为半小时。

从实践上来说，缓存过期时间真的是根据经验来确定的。就算面试官喷你，他也就是装逼。

虽然都是经验主义，但是要综合考虑一些因素：
设置缓存的过期时间，从原则上来说是接下来这个数据多长时间内都会被使用，就设置多长时间。

但是这个很难知道，所以设置缓存的过期时间主要就是考虑三个因素。

第一个是平衡内存使用量和缓存命中率。也就是如果要减少内存使用量，就缩短过期时间；如果要提高命中率，就延长过期时间。
当然，还有一些比较奇诡的用法。

比如说我曾经设计过一个业务相关预加载的缓存方案。它的原理就是根据用户的行为来预测用户接下来会访问什么数据，提前加载起来。

我的用法是当时有一个批量接口，如果用户访问第一页的 10 条数据的时候，我这个批量接口会顺手把前 3 条数据缓存起来，即我预测用户看到这一批数据之后大概会查看这三条数据的详细内容。

------------------------------

300. 在 Kafka 异步消费-批量提交的方案中，如果要是部分消费失败了，怎么办？, page url: https://www.mianshi.icu/question/detail?id=777
300-777-答案：
略难的题。校招不常见，在社招中如果回答消息积压之类的问题提到了异步消费之后，就很容易遇到这个问题。

在这个问题之下，你可以想到，大部分人只能回答出来重试。所以如果你要是能够回答出来部分提交、转储、以及在重试的时候去重就可以刷出亮点，赢得竞争优势。
这里要先解释一下批量提交这个概念。在 Kafka 的 Broker 看来，其实并没有什么批量提交啥的，因为它其实只维护一个偏移量。看下面这些例子：

也就是它只认最大的那个已提交偏移量。

那么假设现在我们批量消费 100, 101, 102, 103, 104，但是很不幸的是 102 这一条消息处理失败了。
解决问题的思路有很多种。

最简单的一种就是全部不提交，那么再次拉取消息，还是拉到这一批，也就是进行重试了。
很显然，就是三种思路实际上都会涉及到重试的问题。那么有一个改进的方案，可以减少不必要的重试。

也就是在异步消费的时候，使用 Redis 或者本地缓存之类的记录一下这一批消费成功的消息，例如说用 Redis 的 zset 来记录，。而后在重试消费这一批消息的时候，检查一些哪些已经消费成功了。如果要是消费成功了，就不要再次重试了。不然就直接转交给业务方进行处理。

------------------------------

301. 你用 Redis 遇到过什么问题？, page url: https://www.mianshi.icu/question/detail?id=778
301-778-答案：
略难的题。在社招中常见，校招基本不会这么问，毕竟你没什么工作经验。

这个题目如果你随便回答当然可以，但是最好是精心设计一个回答，引导过去你准备的复杂案例上。这里我会列举一些案例，你根据实际情况来挑选。
注意审题，这个题目问的是你遇到过什么问题，不是你用 Redis 解决过什么问题。

你有两种回答策略。

第一种回答策略就是直接回答自己遇到的什么问题。
我用 Redis 的时候遇到过很多问题。
我还遇到过大 key 的问题，优化了查询性能。

也遇到过数据倾斜问题，包括节点级别数据倾斜、槽级别数据倾斜。

------------------------------

302. 怎么监控 Redis 的性能？, page url: https://www.mianshi.icu/question/detail?id=779
302-779-答案：
略难的题，主要是很多人没机会接触到监控 Redis 的事情，所以不知道怎么做。一般在社招里面有可能遇到，校招不常见。

在这个回答之下，如果你是在中小型公司里面工作的，那么你就可以装逼自己在公司内部接入了非常多的监控工具，改善了对 Redis 的监控情况。并且还可以进一步将这个东西纳入到你的可观测性面试方案中。
监控 Redis 基本就是两条思路：

两者之间的差异如图：


一般来说，可以通过两种方式来监控 Redis 的性能。
从实践上来说，最重要的指标有很多个。

对于业务方来说，主要是关注缓存命中率、慢查询、连接池三个。

而对于 Redis 运维团队来说，要关注的内容更多。

------------------------------

303. 怎么监控 Redis 慢查询？, page url: https://www.mianshi.icu/question/detail?id=780
303-780-答案：
简单题，你要是听过就是简单题，你要是没听过就是难题。一般在社招里面会有，校招不常见。

回答这个问题，你在指出了 Redis 慢查询监控的方法之后，可以深入讨论慢查询的阈值应该是多少，从而赢得竞争优势。
监控慢查询有两种方式：
监控慢查询有两种方式。
其实监控慢查询这个东西好说，难说的是慢查询的阈值究竟应该是多少。

如果是 slowlog 统一监控，就容易出现发现一个慢查询报过去业务那边，业务说我们就是能接受这个结果。如果要是公司有明确规定，就还可以强制要求他们优化性能，如果没有，那就是扯淡了。

所以业务方自己监控也很好，但是业务方监控是在客户端监控，一般都是包含了网络传输，就有可能出现因为网络传输引发的慢查询问题。这也导致了业务方在排查的时候要进一步分辨究竟是什么因素引起的。

------------------------------

304. HTTP POST、PUT 和 PATCH 有什么区别？, page url: https://www.mianshi.icu/question/detail?id=781
304-781-答案：
简单题，在校招和初级工程师面试中有极小概率遇到。

在这个问题之下，你可以讨论一下 RESTful 中是如何使用这三个方法的。
最重要的就是两个区别：
POST，PUT 和 PATCH 的区别主要体现在两方面。
在 RESTful 架构中，POST 是非幂等地更新或者创建资源；PUT 是幂等地更新或者创建资源；PATCH 是幂等地更新资源的部分属性。

------------------------------

305. 令牌桶限流算法和漏桶限流算法的区别是什么？, page url: https://www.mianshi.icu/question/detail?id=782
305-782-答案：
简单题，在校招和社招的初级工程师面试中有可能遇到，一般在讨论到了限流算法之后，就有很大概率会问这个问题。

你回答这个问题，就要深入讨论两者应对突发流量的能力。
你需要先看一下：

漏桶算法（meoying.com）：

令牌桶算法（meoying.com）：
我认为它们的本质区别主要体现在输出流量的速率特性上。

在令牌桶限流算法中，令牌的填充速率其实就决定了输出流量的平均速率。而令牌桶的容量，也就是桶满时的情况，则定义了输出流量的最大速率。当桶里有令牌的时候，输出流量的速率是由输入流量的速率决定的，基本上就等于输入流量的速率，所以输出流量的速率会在一定范围内波动。

而在漏桶限流算法中，输出流量的速率就是漏桶漏水的速率，而且这个速率是固定不变的。
网上对令牌桶算法评价很高，一看到“能应对突发流量”的描述就想当然地认为令牌桶算法更适合瞬时高并发场景，其实不然。这两种算法最早是应用在底层网络流量控制的而不是上层业务的，所以令牌桶算法中的“应对突发流量”其实指的是应对网络抖动，即在一定范围内允许发包速率突变，但长期平均速率不变，而不是我们通常说的秒杀、准点签到等业务场景。

------------------------------

306. 如何提高缓存命中率？, page url: https://www.mianshi.icu/question/detail?id=786
306-786-答案：
略难的题。校招比较少遇到，在社招讨论到缓存的时候就比较常见。

在这个问题之下，可以通过阐述自己使用过的各种奇诡的提高缓存命中率的案例来刷亮点。
提高缓存命中率的手段有很多，最主要的有：
提高缓存命中率有很多种手段。

第一种手段是缓存预热。也就是在系统启动的时候主动加载缓存，这样可以规避因为系统刚刚启动的时候缓存中没有数据引发的命中率降低；

第二种手段是缓存预加载，它类似于缓存预热，但是强调的是在缓存快要过期的时候就提前刷新。一般来说，用在热点数据上的效果会非常好，但是用在非热点数据上就比较浪费宝贵的缓存内存；
如果用的是本地缓存，那么还有一些别的方法。
我还曾经借助过灰度过程来避免缓存命中率降低。当时我设计了一个 Redis 互为备份的容错方案，也就是准备一个 main 集群，一个 backup 集群。

------------------------------

307. 本地缓存有什么缺点？, page url: https://www.mianshi.icu/question/detail?id=787
307-787-答案：
简单题，在校招和初中级岗位面试中很常见。

在这个问题之下，你的竞争者大概率只能回答出来本地缓存会加重数据一致性问题，但是难以深入讨论。而实际上，你可以提及本地缓存的命中率低、内存浪费两个点，引出一个哈希类负载均衡算法 + 本地缓存的案例，刷上满满的亮点。
本地缓存的缺点，具体来说就是三个。

第一个是严重的数据一致性问题。也就是说多个节点的情况下，每个节点上都有本地缓存，那么在更新数据的时候就很难保证这些节点上本地缓存数据一致。如下图所示，更新的时候如果策略不当就可能出现别的节点上缓存的还是老数据。所以使用本地缓存的时候要额外注意本地缓存的数据更新问题。


本地缓存的缺点有三个。

第一个是数据一致性问题很严重。这个很容易理解，在分布式系统下，同一个数据在不同的节点上都会有一份本地缓存，那么在数据更新的时候就很难解决一致性的问题，这远比使用 Redis 来作为缓存严重。

第二个是内存使用量大。道理也是类似的，多个节点都可能缓存一份数据，浪费严重。
我以前也多次用过本地缓存，并且使用过一个很骚气的解决方案缓解这三个问题。
当然，我的这个方案也有一点小缺点。就是哈希类的负载均衡算法，在节点数量发生变化的时候，同一个业务的请求可能落到别的节点上。比如说，在扩容前用户 ID 为 1 的数据在节点 A  上；扩容后，经过负载均衡之后用户 ID 为 1 的请求落到了新节点 B 上，那么 B 上缓存就会出现未命中，从而回查数据库。

------------------------------

308. Redis 中如何排查大 Key 问题？, page url: https://www.mianshi.icu/question/detail?id=789
308-789-答案：
略难的题，在社招中会遇到，一般在中高级岗位中考察较多。

这个问题是包含两层意思：怎么知道有大 key，知道了怎么解决。

回答这个问题，最好就是能引出一个具体的案例，然后借助这个案例来刷亮点。
所谓的大 key，不是指 key 本身很长，而是指值很大，比如说一个巨大的 JSON 串，一个巨大的 zset，list 等。大 key 一般会引起数据倾斜、QPS 倾斜等问题。或者说大 key 有极大的可能是热点 key，并发极高。

在这之前，你可以先看这些问题，它们有些类似，但是又很不同：

大 key 其实就有一个解决方案，分而治之。也就是拆分 key。比如说原本一个 key 对应的 list，放了 100w 数据，那么可以拆成 100 个 key，每个 key 只有 1w 数据。
一般来说，大 key 问题基本上都是通过监控慢查询和 Redis 内存使用量发现的，尤其是慢查询监控。举个例子来说，之前我就排查过一个 Redis 的 list 有很多元素，导致查询、遍历非常慢的问题。

当然，发现大 key 问题并不难，难的是如何解决大 key 问题。

大 key 问题如果要解决，就只有一个思路：分而治之。假定这里都是分成 10 个key。基本的拆分思路有两个：
这里还有一些比较关键的问题。
我就曾经解决过一个大 key 问题，是为了解决大数据、高可用、高性能的榜单问题。

一开始我们业务的数据量不是很大，所以直接用了最简单的 Redis zset 来计算榜单。

后面随着业务的发展，计算这个榜单的数据越来越多，并发量越来越高。这个时候，一个 zset 里面要放几百万个数据，存储这个 zset 的 Redis 并发极高，压力极大。并且 zset 中元素数量太多，导致更新的时候越来越慢。

------------------------------

309. Redis 中如何排查热点（Hot Key）问题？, page url: https://www.mianshi.icu/question/detail?id=790
309-790-答案：
略难的题，在社招中会遇到，一般在中高级岗位中考察较多。

这个问题是包含两层意思：怎么知道有热点问题，知道了怎么解决。

回答这个问题，最好的装逼方式就是讨论拆分 key，并且进一步给出具体的案例作为佐证。
在这之前，你可以先看这些问题，它们有些类似，但是又很不同：

这个问题咋一看很像大 key 问题，但是这个问题有更多的解决方案。你可以认为这个问题等价于某个 key 的 QPS 非常高，怎么办？

具体来说，热点问题有三个解法：
一般来说可以通过监控来发现 Redis 中的热点，但是这个监控一般是业务层面上监控的，比如说某个业务访问 Redis 的频率非常高，那么进一步去这个业务里面排查就能发现哪些 key 的 QPS 非常高，也就是所谓的热点 key。

热点问题主要有三个解决方案。

第一个方案是引入本地缓存。引入本地缓存之后热点大概率命中本地缓存，就能极大的减轻 Redis 的压力。本地缓存会进一步加剧数据一致性的问题；
这里还有一些比较关键的问题。
我就曾经解决过一个大 key 问题，是为了解决大数据、高可用、高性能的榜单问题。

一开始我们业务的数据量不是很大，所以直接用了最简单的 Redis zset 来计算榜单。

后面随着业务的发展，计算这个榜单的数据越来越多，并发量越来越高。这个时候，一个 zset 里面要放几百万个数据，存储这个 zset 的 Redis 并发极高，压力极大。并且 zset 中元素数量太多，导致更新的时候越来越慢。

------------------------------

310. 如何处理死锁？, page url: https://www.mianshi.icu/question/detail?id=794
310-794-答案：
简单题，不过记忆的内容有点多，所以也不太容易答好。在校招和初级工程师面试中，如果要是提及了死锁，那么差不多肯定会问这个问题。

常用的处理死锁的方法有：死锁预防、死锁避免、死锁检测与恢复、死锁忽略（也叫做鸵鸟策略）。

第一种，死锁预防。基本思想就是确保死锁发生的四个必要条件中至少有一个不成立：

第二种，死锁避免。在每次资源请求时，系统会检查此次分配是否会导致系统进入不安全状态。如果会，则拒绝此次资源请求；如果不会，则允许分配。这里最有名的就是使用银行家算法来检测资源分配后是否安全。
常用的处理死锁的方法有：死锁预防、死锁避免、死锁检测与恢复、死锁忽略（也叫做鸵鸟策略）。

第一种，死锁预防。基本思想就是确保死锁发生的四个必要条件中至少有一个不成立：

第二种，死锁避免。在每次资源请求时，系统会检查此次分配是否会导致系统进入不安全状态。如果会，则拒绝此次资源请求；如果不会，则允许分配。这里最有名的就是使用银行家算法来检测资源分配后是否安全。

------------------------------

311. 如何确定限流阈值？, page url: https://www.mianshi.icu/question/detail?id=797
311-797-答案：
略难的题，理论上来说校招是不应该问这个问题，因为他们没有经验。但是实际校招和初中级工程师面试中都有可能遇到。

大部人人在实践中其实都只是依据经验来随便设置一个阈值，但是在面试中就不能这么回答。

你可以通过深入讨论各种场景和限制下，如何设置一个合理的阈值，从而赢得竞争优势。
一般确定限流阈值有如下几种方法：
限流阈值一般受到服务质量和用户体验、业务需求和特点、阈值的类型（动态或静态）、选取的系统指标、系统的伸缩性等因素影响，通常确定限流阈值有以下几种方法。

首先是观测法，通过监控系统或者日志分析工具，长期观察流量数据，分析历史数据，找出流量的峰值、平均值、波动情况和周期性模式。这种方法适合已经稳定运行一段时间，并且有完善监控体系的系统。这个方法的优点在于，基于真实的业务数据，能够直接反映实际流量特征。缺点是依赖于历史数据的完整性和准确性，无法预测未来的流量变化，对于新系统或者流量剧烈变化的情况适用性较差，并且需要较长的时间观察才能获得可靠的数据。

还有压测法，通过使用压力测试工具，比如 JMeter、k6、wrk 等，模拟各种负载场景，逐步增加请求量，同时监控系统的关键指标，找到系统性能下降的临界点。它的优点是可以有效了解系统的性能上限，为阈值设置提供支持，同时测试系统在极端情况下的表现。但准备测试环境和工具的成本较高，测试结果会受到环境和测试方法的影响，需要专业人士来操作和分析，同时也难以完全模拟真实复杂场景。
在实践中还有一种做法，就是根据不同的情况动态调整限流阈值。

------------------------------

312. 怎么同时更新所有节点上的本地缓存？, page url: https://www.mianshi.icu/question/detail?id=801
312-801-答案：
略难的问题，在校招不常见，在社招并且你提及使用了本地缓存之后，差不多就是必问的题目了。

在回答这个问题的时候，在罗列了各种解决方案之后，有两个刷亮点的方向：一个方向就是讨论一些注意事项和细节，另外一个方向就是结合你自己的实践，阐述你在什么场景下用过什么方案。
这个问题问的是，当我要更新本地缓存数据的时候，我怎么更新所有节点上的本地缓存呢？如图：


从实践上说，有很多种做法。
有很多种方法。

第一种方法是借助 RPC 等框架的广播特性，直接通知所有的节点刷新本地缓存。

第二种方式是监听 binlog，一般的架构是通过引入 Canal 来监听 binlog，中间可能会进一步引入消息队列来解耦削峰。
在更新本地缓存的时候，还有一些细节问题要注意。

第一个是大部分方式其实总结起来就是发一个通知，节点收到通知之后就更新本地缓存。那么通知里面如果直接包含了新的缓存数据，那么就要注意顺序问题；
我用得最多的应该是定时刷新这个解决方案。举个例子来说，我曾经设计过一个高并发的榜单方案，最关键的一部就是定时从 Redis 里面捞出最热门的 top 100 数据，放到本地缓存里面，确保每次读取榜单数据的时候，直接命中本地缓存，同时定时刷新本地缓存，也能在一定长度上保证榜单数据的实时性。

------------------------------

313. 如何实现限流策略的动态调整与优化?, page url: https://www.mianshi.icu/question/detail?id=802
313-802-答案：



总是，通过优化限流指标的选取、数据处理、综合评分计算方法及动态调整策略，限流机制不仅能有效保护系统稳定性，还能提升用户体验。实现高效且灵活的限流策略，需结合持续的监控和反馈机制，以确保策略的有效性和响应能力。
限流策略中有以下几个方面可以优化：

首先，限流指标的选取，需要根据实际的业务场景来选取多个关键性能指标，比如请求率（也就是QPS）、并发数、平均响应时间、错误率，还有CPU利用率。这是因为单一指标可能无法全面反映系统的真实状态，所以使用多维度的指标会让我们能更全面地理解系统的负载和健康状况。

接下来是限流数据的收集与预处理。我们要采用高效的数据收集机制来实现不同限流指标的实时采集，收集到的数据还需要经过清洗和预处理，以确保数据的准确性和有效性。常见的预处理方法包括平滑处理，比如滑动平均法、处理异常值，以及数据的标准化，这些都能帮助我们降低噪声的影响，使后续的分析更加准确。
然而， 不管使用什么样的动态限流调整策略，系统偶发性过载不可避免。这个问题类似于负载均衡算法，不管选什么算法都会出现偶发性负载均衡。根源都是一样的：无法准确地实时地计算出服务器的负载和无法准确地提前计算出处理请求所需资源。

------------------------------

314. 你怎么知道你的限流效果好还是不好？, page url: https://www.mianshi.icu/question/detail?id=804
314-804-答案：
略难的题，一般出现在社招中，校招中很少遇到。

如果你没做过限流或者你做了限流但是没有仔细评估过限流效果，那么你肯定不知道。很多时候，你会在面试中吹自己的限流用得有多好。这个时候面试官就可能突然来一个提问，你怎么知道你的限流效果很好？

实际上，在实践中评估限流效果也是一个难题，因为如果要是选择基础
这个问题，其实就是评估限流策略的有效性。

评估限流策略的有效性是确保系统在处理高并发请求时能够维持稳定性和良好用户体验的关键步骤。需要从多个维度进行综合考量，不能仅仅依靠单一指标。一个有效的限流策略应该在保护系统稳定性的同时，尽可能减少对用户体验的影响。以下是一些评估限流有效性的指标：


简单来说，就是看各种指标是否符合你的预期。
评估限流有效性的常用方法有如下几种：

首先是基线对比，即将限流前后的指标进行比较，以评估限流的效果。特别是在不同时间段，如高峰期和非高峰期进行对比，可以获得更准确的信息。此外，确保拥有足够的数据样本至关重要，以提升对比结果的有效性和可信度。

常用的指标可以分为两类：系统内部指标，这些指标能够反映系统的性能，包括：TPS、QPS、资源利用率（CPU 利用率、内存利用率、磁盘 I/O 利用率和网络带宽利用率等）、响应时间、错误率（502 和 503 错误等是否偏高）等。

------------------------------

315. 什么是预热限流？, page url: https://www.mianshi.icu/question/detail?id=805
315-805-答案：
略难的题，因为你很可能都没听过这个概念，一般出现在社招中，校招很少会遇到。

在系统冷启动的情况下,会出现以下几个问题:

预热限流是一种限流策略，主要用于解决”系统冷启动问题“，它旨在平滑地增加系统负载，避免系统在短时间内承受过大的压力，从而导致性能下降或崩溃。 想象一下，你突然打开一个水龙头，水流过大，管道可能承受不住。预热限流就像慢慢打开水龙头，让管道逐渐适应水流。

具体来说，预热限流在开始时会设置一个较低的限流阈值，然后在一段时间内，逐渐增加限流阈值，最终达到预设的峰值。 这个过程就像一个“预热”阶段，让系统有时间逐渐适应增加的流量。
预热限流主要用于解决“系统冷启动问题”。它的目标是在负载增加的过程中平滑且渐进地引入流量，防止系统在短时间内承受过大的压力，避免性能下降或崩溃。想象一下，预热限流就像是慢慢打开水龙头，让管道逐渐适应水流的增加。

具体来说，预热限流在开始时会设置一个较低的限流阈值，然后在一段时间内逐步增加，最终达到预设的峰值。这一过程类似于一个“预热”阶段，给系统留出时间逐渐适应增加的流量。

这种策略有一些优点，比如平滑过渡，避免系统在短时间内承受过大的压力，同时保护系统的稳定性，提高可靠性，从而提升用户体验，减少请求失败或响应时间过长的情况。然而，它也有一些缺点，比如实施过程中算法复杂性增加，初期性能可能会受到影响，且在流量逐步增加时，响应时间可能会出现波动。

------------------------------

316. 在高并发场景下，限流器本身的性能及可用性如何保证？, page url: https://www.mianshi.icu/question/detail?id=806
316-806-答案：
简单题，校招几乎不会问，毕竟校招没实践经验。只有在一些高可用的社招中高级工程面试中有可能会遇到。

你需要看一下：
本地限流器一般基于内存实现，性能非常好，自身的可用性与系统的可用性一致，可以通过限流、熔断、降级等手段保障。

------------------------------

317. 如何处理被限流的请求？, page url: https://www.mianshi.icu/question/detail?id=807
317-807-答案：
简单题，在校招都有可能遇到。

然而这个问题要刷亮点就得用点小花招了，如果你只会被限流的请求返回错误响应，那么肯定是没有办法赢得竞争优势的。你还要进一步考虑使用诸如转异步、降级、failover 等策略。
对于被限流的请求，可以采取以下几种常见的处理方式：
关于被限流的请求，我们可以采取几种常见的处理方式来妥善应对。最简单的处理机制有两种：返回友好响应和排队等待处理。

首先，返回友好的错误响应是一个简单且有效的选项。当请求被限流时，直接返回一个明确的错误响应，比如 HTTP 状态码 429（Too Many Requests）。同时，在响应中加入“Retry-After”头部的信息，告知客户端什么时候可以重试。这种方式的好处是简单高效，不消耗系统资源，但缺点是用户体验较差，可能会导致请求丢失。这种方式适用于非关键性请求以及容错性要求低的情况。
还有两种比较高级的处理方式：降级和异步处理。
最后还有一种需要客户端配合的高端的限流处理方案，也就是所谓的 failover。本质上它由两个关键步骤构成：更换节点重试。

当请求被限流时，服务提供方向服务调用方发送一个特定的错误码，指示请求无法处理。客户端收到这个错误后，会检查是否还有其他可用的节点。如果有可用的节点，服务调用方会切换到这些节点重新发送请求，从而继续服务。

------------------------------

318. 如何动态调整限流阈值？, page url: https://www.mianshi.icu/question/detail?id=808
318-808-答案：
略难的题，一般校招和初级工程师面试中几乎不会遇到，只有在中高级工程师面试中有可能遇到这些题目。

在这个问题之下，可以深入讨论动态调整限流阈值的敏感度问题，也可以考虑列举一个自己用过的动态调整阈值策略，从而赢得竞争优势。
在限流算法中，动态调整限流阈值需要考虑以下几个关键方面：
在限流算法中，动态调整限流阈值，每次调整的幅度不宜过大，如果一次性调整太大，可能会导致流量在服务器之间剧烈波动，从而影响服务的稳定性。

我建议采用渐进式调整策略，每次只调整一个小部分的阈值，比如说不超过10%。这样可以确保限流阈值的平滑过渡，避免剧烈变化。
动态调整阈值其实最大的难点是敏感度问题，说白了就是决定动态调整阈值的时候，反应够不够快，实时性好不好。这涉及到两个点。

第一个点，采集指标的时间间隔。理论上来说，时间间隔越短越好，但是代价就是会消耗更多的 CPU 和内存。

------------------------------

319. 如何选取指标？, page url: https://www.mianshi.icu/question/detail?id=809
319-809-答案：
略难的题，校招不太可能遇到，在社招的中级工程师以上的面试岗位中会有可能遇到。

在回答这个问题的时候，可以通过强调选取指标要避免使用单一指标，应该使用复合指标来获得竞争优势。而后可以深入讨论选择指标本质上是为了回答如何衡量服务是否健康这一个问题，而这个问题可以认为是服务治理中的一个核心问题
选取合适的指标，是能够准确判定服务是否健康的重要前提条件。注意一点，这个是一个通用的理论，你可以同时用在熔断、限流和降级的场景下。


选择指标需要结合具体的业务场景、系统架构和服务质量等因素进行综合考量，没有单一的最佳方案适用于所有场景。以下是选择指标的一般流程：



在选择指标的时候，也要遵循一些原则：
从原则上来说，指标应该能够全面反映系统的负载状态和性能。

常见的指标包括请求数、响应时间、错误率、并发连接数等。在选择指标时，需要结合具体的业务场景、系统架构、服务质量、用户体验、可操作性、成本等因素，进行综合考量。

选择指标的一般流程如下：首先要明确目标，清楚我们的目的是什么，比如保护数据库、提升用户体验、防止恶意攻击或者保障核心业务的可用性。明确了目标后，我们就能选择与之最相关的指标。
在实践中，选择指标应该遵循一些原则。

首先是组合使用多个指标，因为单一指标无法全面反映后端服务健康状况，组合使用可提供更全面视图，如同时监控成功率、平均响应时间和错误率。可设置综合评分机制，将多个指标加权平均，获取全面评估。

第二根据服务特性选择。不同类型服务的需要不同的指标，例如，I/O密集型服务更关注平均响应时间，而CPU密集型服务需关注CPU利用率等指标，内存密集型服务则需关注内存使用率。
更进一步来说，选择这些指标，本质上就是为了回答一个问题：如何衡量服务是否还能正常运行。

从分类上来说，也就是选择和服务有关的指标，以及服务运行环境所在的指标。前者是指响应时间，错误率等指标。而后者则是指诸如 CPU 利用率等指标。

而在实践中的问题是，现在并没有一套放之四海而皆准的指标。所以不管选什么样的指标，不管怎么组合使用这些指标，都难以避免两个问题：没有及时系统有问题，误判系统有问题。

------------------------------

320. 常用的指标有哪些？, page url: https://www.mianshi.icu/question/detail?id=811
320-811-答案：
略难的题。在社招中有可能遇到，尤其是中级工程师以上的岗位。

你可以先罗列单一的指标，而后强调单一的指标的局限性，从而得出最终的结论：复合指标更加能够反应系统的负载。最终装逼还是要注意指出不管什么指标都难准确衡量系统状态，这也是为啥我们各种服务治理搞来搞去都会出现服务崩溃的情况。
常见的指标：
常见的指标有很多，比如：请求数（Request Count），它指的是单位时间内，比如每秒或每分钟，收到的请求数量。这个指标是非常常用的，简单直接，适用于大多数的应用场景。

还有并发数（Concurrency），这个指标是指同时处理的请求数量。并发数更加关注系统的资源占用情况，比如线程池大小和数据库连接数等。这个指标适合那些需要控制系统资源占用的场景，特别是在资源有限的环境中。我们可以通过监控系统的资源使用情况来获取这个指标。

再来是响应时间（Response Time），它是用来衡量处理单个请求所需的时间。当响应时间超过某个阈值时，就可能表明系统性能出现问题。这个指标尤其关注用户体验，适合那些对实时性要求比较高的场景，比如在线游戏和视频直播等。通过应用程序的监控或链路追踪系统，我们可以统计单个请求的处理时间。而具体的指标，可以是平均响应时间，也可以是 99线或者 999 线。

------------------------------

321. 选择指标时要考虑哪些因素？, page url: https://www.mianshi.icu/question/detail?id=812
321-812-答案：
略难的题，难在你得有很丰富的实战经验才能总结出来这些因素，市面上没啥八股文或者博客会讨论这个问题。

回答这个问题的时候，可以先罗列选择指标的核心因素，而后深入讨论如何综合使用这些指标，设计一个能够综合判定服务状态的算法。
选择指标时要考虑以下因素：
在选择指标时，我们需要综合考虑以下几个关键因素：

首先，目标是核心导向。我们的目标要么是保障系统的可用性，要么是提升用户体验。例如，如果目标是保障用户体验，我们会优先选择那些能够反映响应时间的指标，确保用户在使用过程中感受到流畅的服务。

其次，业务特征是重要依据。不同业务对资源的依赖不同，流量特点也各异。例如，对于内存敏感型业务，我们会重点考虑内存使用率；而对于CPU敏感型业务，CPU使用率则成为关键指标。通过深入理解业务需求，我们可以更有针对性地选择合适的指标。
正常来说，单一的指标很难全方面反应系统的负载，所以一般都要选用多个指标。

那么带来的挑战就是如何设计高效的算法，以全面考虑各种影响因素。为此，我采用了一种行之有效的策略，该策略包含两个核心要点：

首先，特定指标的一票否决权。这意味着当某些关键指标超过预设阈值时，就认为系统已经陷入一种不健康的状态。例如，对于大多数业务而言，内存是至关重要的资源。一旦内存使用率达到临界点，就应果断启动限流，以防止系统崩溃。
更进一步来说，选择这些指标，本质上就是为了回答一个问题：如何衡量服务是否还能正常运行。

从分类上来说，也就是选择和服务有关的指标，以及服务运行环境所在的指标。前者是指响应时间，错误率等指标。而后者则是指诸如 CPU 利用率等指标。

而在实践中的问题是，现在并没有一套放之四海而皆准的指标。所以不管选什么样的指标，不管怎么组合使用这些指标，都难以避免两个问题：没有及时系统有问题，误判系统有问题。

------------------------------

322. 为什么动态限流阈值会设置一个上限？, page url: https://www.mianshi.icu/question/detail?id=813
322-813-答案：
简单题，几乎不会遇到的题目。你理解了什么是动态限流，就理解了这个问题。
你需要看一下：
动态限流阈值上限通常代表系统能够承受的最大负载，虽然动态调整阈值可以根据实际负载情况进行优化，但上限作为最后的安全防线，确保即使动态算法出现误判也能保护系统。

------------------------------

323. 为什么动态限流阈值会设置一个下限？, page url: https://www.mianshi.icu/question/detail?id=814
323-814-答案：
简单题，你理解了动态限流算法之后，就可以回答出来这个问题。
你需要看一下：
设置动态限流阈值下限的目的是保障系统的最低服务能力并能应对突发情况。

------------------------------

324. 如何设计动态限流算法？, page url: https://www.mianshi.icu/question/detail?id=815
324-815-答案：
难题，这个难是指如果要真的设计一个现实中很好用的算法，是比较难的。你在面试中几乎不太会遇到这个问题，只有在面试很高端、偏基础中间件或者架构的时候才会遇到这个问题。

在这个问题之下，你可以深入讨论算法设计的要点，以及最新的一种借助 AI 来控制系统的新趋势，从而赢得竞争优势。



限流算法本质上是在讨论一个问题：如何判定服务是否健康，基本思想是通过指标来评价服务的健康状况，然后根据评价结果来确定是否执行限流。如上图上半部分所示，评价指标一般分为两类：



选定评价指标后还需要确定“评价函数（评价算法）”，此过程涉及指标数据的预处理方式、表达式、评分阈值范围、评分与评分阈值之间的比较方式等要素的确定。依然以BMI作为类比：
可以采用以下流程设计动态限流算法。

首先，我们需要确定限流指标与采集方式。这意味着我们要根据实际需求选定一些系统指标，比如 CPU 使用率、响应时间和内存使用等，同时还要考虑一些业务指标，比如请求量和转化率。

从采集方式上来说，基本都是依赖各种检测工具，如 Linux 工具，Prometheus 等。而采集频率则要在实时性和系统开销两方面取得一个平衡。此外，采集的数据还要注意时间局部性以及归一化处理，方便后续计算分数。
这只一些粗略的框架，这里面还有很多的细节要处理。最复杂的部分就是根据指标、业务目标等因素综合设计一个算法。

算法要解决的第一个是指标的预处理问题。预处理要解决两个核心问题：异常值处理，归一化处理。

异常值处理就是要想办法辨别采集的数据中有问题的部分，而后可以考虑纠正或者直接丢弃。而归一化处理则是要考虑如何处理不同量纲的指标。举个例子来说，响应时间和单位是 ms，并且值会在几十到几百之间。而对应的内存使用率，是一个百分比，数值在 0- 100 之间。所以需要归一化处理之后才能使用。
在当下，还有一种思潮是借助 AI 来控制系统，它本质上就是用 AI 取代人的作用。

------------------------------

325. 如何确定动态限流阈值的下限？, page url: https://www.mianshi.icu/question/detail?id=816
325-816-答案：
简单题，校招中几乎不会遇到，社招中只有在很深入讨论了限流之后，才有可能遇到。
确定动态限流阈值下限与确定静态限流阈值的方法相同，你需要看一下如何设置限流阈值？
确定动态限流阈值下限与确定静态限流阈值的方法相同。

限流阈值一般受到服务质量和用户体验、业务需求和特点、阈值的类型（动态或静态）、选取的系统指标、系统的伸缩性等因素影响，通常确定限流阈值有以下几种方法。

首先是观测法，通过监控系统或者日志分析工具，长期观察流量数据，分析历史数据，找出流量的峰值、平均值、波动情况和周期性模式。这种方法适合已经稳定运行一段时间，并且有完善监控体系的系统。这个方法的优点在于，基于真实的业务数据，能够直接反映实际流量特征。缺点是依赖于历史数据的完整性和准确性，无法预测未来的流量变化，对于新系统或者流量剧烈变化的情况适用性较差，并且需要较长的时间观察才能获得可靠的数据。

------------------------------

326. 如何确定动态限流阈值的上限？, page url: https://www.mianshi.icu/question/detail?id=817
326-817-答案：
简单题，校招中几乎不会遇到，社招中只有在很深入讨论了限流之后，才有可能遇到。
确定动态限流阈值上限与确定静态限流阈值的方法相同，你需要看一下如何设置限流阈值？
确定动态限流阈值上限与确定静态限流阈值的方法相同。

限流阈值一般受到服务质量和用户体验、业务需求和特点、阈值的类型（动态或静态）、选取的系统指标、系统的伸缩性等因素影响，通常确定限流阈值有以下几种方法。

首先是观测法，通过监控系统或者日志分析工具，长期观察流量数据，分析历史数据，找出流量的峰值、平均值、波动情况和周期性模式。这种方法适合已经稳定运行一段时间，并且有完善监控体系的系统。这个方法的优点在于，基于真实的业务数据，能够直接反映实际流量特征。缺点是依赖于历史数据的完整性和准确性，无法预测未来的流量变化，对于新系统或者流量剧烈变化的情况适用性较差，并且需要较长的时间观察才能获得可靠的数据。

------------------------------

327. 熔断器模式的三个基本状态是什么？, page url: https://www.mianshi.icu/question/detail?id=818
327-818-答案：
简单题，在校招和初级工程师面试中比较常见。

在这个问题之下，你可以深入讨论引入半开启状态的原因，从而赢得竞争优势。
前置知识：
熔断器模式的三个基本状态分别是：闭合状态、开启状态和半开启状态。

首先，闭合状态是熔断器的起始状态。在这个状态下，所有的请求都会正常处理。熔断器会持续监控一段时间内的熔断指标，比如成功率、错误率和响应时间等，并以此来评估服务的健康状况。如果这些熔断指标超出了预设的阈值，比如在过去一分钟内观察到成功率低于90%或请求响应时间超过500ms，熔断器就会切换到开启状态。

然后是开启状态。在这个状态下，熔断器会先启动一个定时器，然后开始等待，一旦定时器到期就切换到半开启状态。等待期间，会根据配置的熔断策略处理请求，比如拒绝请求并返回友好的错误提示，一般是503 Service Unavailable 或者服务降级。这不仅能有效减轻后端故障服务的压力使其有机会恢复，还可以保护调用方避免级联故障。
熔断器引入半开启状态是为了在系统恢复过程中实现平滑过渡，避免因过度保护或瞬时故障导致的频繁状态切换。

从抖动角度来看，半开启状态允许熔断器在完全开启和完全关闭之间进行平滑过渡，通过逐步放行少量请求，试探服务的恢复情况，避免因瞬时故障引起的频繁切换，减少系统抖动。

------------------------------

328. 请解释一下熔断器的三种状态及其转换机制, page url: https://www.mianshi.icu/question/detail?id=819
328-819-答案：
简单题，在校招和初级工程师面试中比较常见。

在这个问题之下，你可以深入讨论引入半开启状态的原因，从而赢得竞争优势。

你可以先看什么是熔断器？请简要解释其设计初衷和目的。，这个问题等价于熔断器模式的三个基本状态是什么？，所以相关回答都是一样的。
熔断器模式的三个基本状态分别是：闭合状态、开启状态和半开启状态。

首先，闭合状态是熔断器的起始状态。在这个状态下，所有的请求都会正常处理。熔断器会持续监控一段时间内的熔断指标，比如成功率、错误率和响应时间等，并以此来评估服务的健康状况。如果这些熔断指标超出了预设的阈值，比如在过去一分钟内观察到成功率低于90%或请求响应时间超过500ms，熔断器就会切换到开启状态。

然后是开启状态。在这个状态下，熔断器会先启动一个定时器，然后开始等待，一旦定时器到期就切换到半开启状态。等待期间，会根据配置的熔断策略处理请求，比如拒绝请求并返回友好的错误提示，一般是503 Service Unavailable 或者服务降级。这不仅能有效减轻后端故障服务的压力使其有机会恢复，还可以保护调用方避免级联故障。
熔断器引入半开启状态是为了在系统恢复过程中实现平滑过渡，避免因过度保护或瞬时故障导致的频繁状态切换。

从抖动角度来看，半开启状态允许熔断器在完全开启和完全关闭之间进行平滑过渡，通过逐步放行少量请求，试探服务的恢复情况，避免因瞬时故障引起的频繁切换，减少系统抖动。

------------------------------

329. 请解释熔断器的开、关、半开状态的转换条件，以及需要注意的问题。, page url: https://www.mianshi.icu/question/detail?id=821
329-821-答案：
略难的题，一般来说只会在社招中出现，而且是看你有一定服务治理基础才会考察。不然的话，没有什么服务治理经验的肯定回答不上来。

这个题倒不是难在状态转换以及转换条件，而是难在注意事项。你差不多能把各种注意事项说清楚，而后总结拔高这些所有的注意事项，其实都源自一个问题：我怎么知道这个服务健康还是不健康，一般的面试官是没这个见识的。
前置知识：

熔断器的三种状态（闭合、开启、半开启）之间的转换依赖于对后端服务健康状况的监控。转换条件基于一段时间内收集的熔断指标及其预设阈值，以反映最近一段时间内后端服务的健康状况。转换条件和需要注意的问题如下：

1. 闭合状态 (Closed) 到 开启状态 (Open) 的转换:
在整个熔断机制中，有三个关键状态转换。

第一个是熔断器从闭合变为开启。触发条件是熔断器认为当下监控的指标证明此时系统已经处于一种不健康的状态，从而开始熔断，此时一般也会开启一个定时器。此时所有的请求都会按照预先指定的策略被处理，比如说返回一个特定错误。

在这个转换状态中，关键点是要熔断算法使用的指标以及对应的阈值。这一方面要确保这些指标能够及时反应系统的状态，一方面又要避免漏判或者误判系统处于不健康状态。
如果抽象地来说，所有的状态转换都是在回答一个问题：服务是否处于一种健康的状态。

第一个转换，则是对应于服务从健康变为不健康。

第二个转换，则是对应于服务从不健康，开始恢复。

------------------------------

330. 什么是半开状态（Half-Open），它在熔断器模式中的作用是什么？, page url: https://www.mianshi.icu/question/detail?id=823
330-823-答案：
简单题，在深入讨论熔断的时候就有可能遇到，在校招和初级工程师中有可能遇到。

在这个问题之下，你要抽象拔高这种半开设计的技术手段，它可以被看做是一种通用的规避系统震荡和系统过载的手段，只要是有流量切换过程，都可以考虑采用类似的设计。
前置知识：
熔断器的半开状态是熔断器从打开状态向关闭状态过渡时的中间状态，其核心作用其实就是提供更精准、更可靠的状态切换决策。

在半开状态下，熔断器会放行小部分请求，并持续监控这些请求的熔断指标，以此判断服务是否已经恢复。这样一来，熔断器就可以有效决定是继续保持半开状态，还是切换到打开状态或者关闭状态。
半开启的这种状态，可以看做是一个从关闭到开启状态的灰度。我也利用过很多这种思想。

比如说在我设计的 Redis 集群互为备份的容错方案中，也就是当 Redis 集群崩溃的时候，会切换到使用备份 Redis 集群。而当 Redis 集群再次恢复的时候，会把流量从备份 Redis 集群切回来正常的 Redis 集群，但是此刻并不是立刻切换 100% 流量，而是引入了一个灰度过程。

------------------------------

331. 熔断机制在系统中的角色是什么？有哪些应用场景？, page url: https://www.mianshi.icu/question/detail?id=826
331-826-答案：
简单题，在微服务话题下很常见。在校招和初级工程中有可能遇到，一般高端的面试不太会讨论这种简单的问题。

在这个问题之下，最佳装逼策略就是列举一些自己使用过熔断的具体场景，最好是有一些花里胡哨的熔断场景，显得你对熔断有很深刻的理解。

前置知识：

熔断机制在分布式系统中扮演着关键的保护者角色。它如同电路中的熔断器，在检测到下游服务出现持续故障或性能严重下降时，迅速切断对该服务的调用，防止故障蔓延，从而保护整个系统的稳定性和可用性。熔断机制的核心在于快速故障检测和隔离，而不是故障的根因诊断和修复。

熔断机制通常应用于服务调用方通过远程调用与服务提供方通信的场景。无论服务提供方的可靠性、可用性如何，熔断机制都能够帮助调用方在服务提供方出现故障时快速响应，降低对整体业务的影响。其使用场景可以按照依赖服务的类型做如下归纳：
熔断机制是分布式系统中一种重要的容错机制，它扮演着保护角色。我们可以把它想象成电路中的熔断器，当它检测到下游服务出现持续故障或者性能严重下降时，就会迅速切断对那个服务的调用。

具体的使用场景可以分成三类：针对外部服务的熔断，针对内部服务的熔断，和针对内部资源的熔断。

第一类是针对外部服务的熔断。一般来说业务系统常依赖如支付网关、短信服务等外部服务，其稳定性不受控，可能导致系统资源耗尽和级联故障。例如，电商支付服务依赖银行网关，网关故障时熔断器可防止支付服务瘫痪。
我在实践中也用过很多次熔断来保护我的系统。

------------------------------

332. 什么是熔断机制？它解决了什么问题？, page url: https://www.mianshi.icu/question/detail?id=827
332-827-答案：
简单题，在微服务话题下很常见。在校招和初级工程中有可能遇到，一般高端的面试不太会讨论这种简单的问题。

在这个问题之下，刷亮点有两个策略。第一个策略是谈及自己使用过的熔断案例，最好是项目经历中的案例，而后将话题引导过去你准备面试项目上；第二个策略是讨论熔断、降级、限流三者的本质，也就是它们其实是一回事。

前置知识：
熔断是一种重要的分布式系统容错机制，一般通过有限状态机实现。常用于处理不可靠的远程服务或资源调用，防止单个服务故障导致整个系统的级联故障（也称为雪崩效应）。它借鉴了电力系统中熔断器的原理，当系统检测到某个服务不可用或性能严重下降时，会自动“熔断”对该服务的调用，防止故障进一步扩散到其他服务，从而保证整个系统的稳定性和可用性。

可以将熔断机制想象成一个开关。当某个服务出现故障或不可用时，它就“断开”与该服务的连接，避免其他服务因持续调用该故障服务而受到影响，从而保障整个系统的稳定运行。一旦故障服务恢复，熔断机制会自动“闭合”，重新建立与该服务的连接。
我在实践中也用过很多次熔断来保护我的系统。
熔断经常和限流、降级联系在一起讨论，而且在实践中三者的界限也不是那么分明。

举个例子来说，当我们说触发熔断之后可以返回一个默认返回值，这本身也可以看做是一种降级措施。又或者在实践中可以将三者混合使用，例如说在限流策略中，被限流的请求可以直接熔断。

------------------------------

333. 什么是熔断器（Circuit Breaker）模式，它的主要作用是什么？, page url: https://www.mianshi.icu/question/detail?id=829
333-829-答案：
简单题，在校招和初级工程师中会遇到，高端岗位不太可能出现这种基础题。

在这个问题之下，最好的刷亮点方式就是举例子，阐述自己曾经使用过的熔断技术。
前置知识：

熔断器模式是一种重要的分布式系统容错机制，用于处理不可靠的远程服务或资源调用，防止单个服务故障导致整个系统级联故障（也称为雪崩效应）。它借鉴了电力系统中熔断器的原理，当系统检测到某个服务不可用或性能严重下降时，会自动“熔断”对该服务的调用，防止故障进一步扩散到其他服务，保证整个系统的稳定性和可用性。其主要作用是提高系统稳定性和可用性，具体体现在下几个方面：


你只需要记住最关键的提高系统稳定性和可用性，其余都可以看做是从这里衍生出来的。
熔断器模式对于分布式系统来说是一种重要的容错机制，一般通过有限状态机来实现。主要是用来处理那些不可靠的远程服务或者资源调用。核心目的是防止单个服务出问题引发整个系统的级联故障，这种现象也常被称为雪崩效应。这个模式是借鉴了电力系统中的熔断器原理。当系统发现某个服务不可用，或者它的性能严重下降时，熔断器会自动“熔断”对这个服务的调用，避免故障扩散到其他服务，从而保证整个系统的稳定性和可用性。

具体来说，它的几个关键点包括：首先，熔断器模式能够有效隔离系统故障。它通过快速识别并中断对故障服务的请求，阻止故障在服务间蔓延，这样就能够防止出现雪崩效应，保障系统整体的稳定性。

其次，熔断器模式能够保护系统资源。通过快速返回一些错误响应或降级响应，它能够释放被熔断请求占用的系统资源，降低资源的浪费，防止因资源耗尽导致系统整体崩溃。
我在实践中也用过很多次熔断来保护我的系统。

------------------------------

334. 什么是熔断器？请简要解释其设计初衷和目的。, page url: https://www.mianshi.icu/question/detail?id=830
334-830-答案：
简单题。在微服务相关的面试下非常高频，校招和初中级工程师面试中很常见。

在这个问题之下，你可以深入讨论为什么会引入半开启状态，并且进一步揭示出熔断、限流、降级三者的联系，指出它们本质上是一样的，从而赢得竞争优势。
前置知识：

熔断器（Circuit Breaker）是微服务架构中一种常见的设计模式，旨在提高服务的稳定性和可用性，防止级联故障的发生。这种故障通常是因为后端服务（下游服务）出现问题，而导致调用方服务也受到影响，可能引发更严重的雪崩效应甚至系统崩溃。熔断器通过监控下游服务的健康状况，在发现故障时“熔断”一段时间，阻止对故障服务的进一步请求，从而保护调用方及整个系统。熔断器可以通过有限状态机的设计来管理请求，有效地隔离故障服务。熔断器主要有三种状态：闭合状态（Closed）、开启状态（Open）和半开启状态（Half-Open）。以下是状态机示意图：


熔断器是微服务架构中一种常见的设计模式，旨在提高服务的稳定性和可用性，从而防止级联故障的发生。熔断器主要有三种状态：闭合状态、开启状态和半开启状态。

（状态变迁）在闭合状态下，熔断器接收所有请求并正常传递到后端服务，并持续监测熔断指标，比如成功率、错误率和响应时间。一旦这些指标超过了设定的阈值，熔断器将会进入开启状态。例如，如果在过去一分钟内观察到成功率低于90%或请求响应时间超过500ms，熔断器会判定后端服务不可用并作出状态转移。

开启状态则是熔断器在判断后端服务不可用后所处的状态。在此状态下，熔断器会启动一个定时器，定时器到期后，熔断器就会转入半开启状态。期间，熔断器会拒绝所有请求，并返回友好的错误提示，例如说 503 Service Unavailable。这个状态能有效减轻故障服务的压力，并保护调用方。
（深入讨论半开启状态）熔断器引入半开启状态是为了在系统恢复过程中实现平滑过渡，避免因过度保护或瞬时故障导致的频繁状态切换。

从抖动角度来看，半开启状态允许熔断器在完全开启和完全关闭之间进行平滑过渡，通过逐步放行少量请求，试探服务的恢复情况，避免因瞬时故障引起的频繁切换，减少系统抖动。
熔断经常和限流、降级联系在一起讨论，而且在实践中三者的界限也不是那么分明。

举个例子来说，当我们说触发熔断之后可以返回一个默认返回值，这本身也可以看做是一种降级措施。又或者在实践中可以将三者混合使用，例如说在限流策略中，被限流的请求可以直接熔断。

------------------------------

335. HTTP 协议是什么？, page url: https://www.mianshi.icu/question/detail?id=832
335-832-答案：
简单题。在校招和初级工程师的面试中有可能遇到，但是这个问题实在太简单了，所以差不多就是送分题了。
HTTP（超文本传输协议，HyperText Transfer Protocol）是互联网上应用最为广泛的网络传输协议之一。它定义了客户端（如用户的Web浏览器）和服务器之间的通信规则，以便于客户端能够从服务器请求超文本数据，以及服务器能够将超文本数据传输给客户端。
HTTP 是互联网上应用最为广泛的网络传输协议之一。它定义了客户端和服务器之间的通信规则，以便于客户端能够从服务器请求超文本数据，以及服务器能够将超文本数据传输给客户端。

HTTP 协议简单、无状态、可扩展性好，本质上来说是一个请求-响应模式。

客户端向服务器发送一个HTTP请求，服务器接收请求并用包含数据的HTTP响应进行回复。这个过程涉及以下步骤：
HTTP 不仅仅是在 Web 应用中广泛使用，在当下微服务架构为主的分布式系统中也大量使用。

具体体现有两个。

------------------------------

336. 什么是 TCP 协议？, page url: https://www.mianshi.icu/question/detail?id=833
336-833-答案：
简单题，在校招和初级工程师面试中比较常见，差不多可以说是必考了。

在这个问题之下，你要提及三次握手、四次挥手、网络编程，以及 TCP 在当下的局限性，进一步提到可靠传输的 UDP 等内容。
TCP（传输控制协议） 是一种面向连接的、可靠的、基于字节流的传输层通信协议，广泛用于互联网中的数据传输。以下是 TCP 协议的核心特点：
（定义）TCP（传输控制协议）是一种面向连接的、可靠的传输层协议，广泛应用于互联网数据传输。它通过三次握手建立连接，确保通信双方的可靠性。当使用完毕之后，通过四次挥手确保客户端和服务端能够正常结束数据传输。
但是 TCP 也有很多缺点，最大的缺点就是创建和销毁的开销都很大，也就是对应于三次握手和四次挥手消耗很多的资源。因此正常来说我们使用 TCP 都是会配合池化技术，并且精心设计连接池的各种参数，如空闲时间、最小连接数、最大连接数。

此外，在高并发的场景下，TCP 还有一些缺点。

第一个是头部开销大，也就是每一个报文都要传输一大堆的头部，导致系统的吞吐量下降；
因此现在还有一种说法，认为早期之所以 TCP 使用非常广泛，是因为早期的网络设备可靠性比较差，数据在传输过程中很有可能会出错，因此需要引入 TCP 这种强可靠性的机制。

------------------------------

337. 在熔断器实现中，哪些指标对于触发熔断非常重要？, page url: https://www.mianshi.icu/question/detail?id=837
337-837-答案：
略难的题，这也是一个需要实践经验才能回答好的问题，在校招和初级工程师的面试中不太可能遇到。

在这个问题之下，要注意强调因为业务场景不同，所以在设计熔断机制的时候，要针对业务特性来筛选出来对业务很重要的指标。
你需要先阅读常用的指标有哪些？

这种开放性的题目，可以算是半客观。也就是说你回答的什么比较重要，只要不是故意跟业界唱反调，那就可以。因为这个问题本身就是要求你带上自己的主观评价。

你可以根据自己对业务的理解，来陈述自己认为的比较重要的指标。
我个人认为比较重要的指标有三个。

第一个是 QPS/TPS，它直观体现了系统承担的并发压力。

第二个是响应时间，它反映了处理请求所需的时间。响应时间过长通常意味着性能下降，这时候有必要触发熔断，以维护服务质量。
此外还有一些和具体业务特征有关的指标。

比如说在一些强调 CPU 的应用中，CPU 使用率就会是关键指标。类似地，如果是内存敏感性应用，那么空闲内存就是关键指标。

------------------------------

338. 如何确定熔断阈值？, page url: https://www.mianshi.icu/question/detail?id=838
338-838-答案：
略难的题，理论上来说校招是不应该问这个问题，因为他们没有经验。但是实际校招和初中级工程师面试中都有可能遇到。

大部人人在实践中其实都只是依据经验来随便设置一个阈值，但是在面试中就不能这么回答。

你可以通过深入讨论各种场景和限制下，如何设置一个合理的阈值，从而赢得竞争优势。
这部分 和确定限流阈值的思路是一样的，但是限流里面回答中的例子不适合用在这里如何确定限流阈值？
熔断阈值一般受到服务质量和用户体验、业务需求和特点、、依赖服务特性、阈值的类型（动态或静态）等因素影响，通常确定熔断阈值有以下几种方法。

首先是观测法，通过监控系统或者日志分析工具，长期观察流量数据，分析历史数据，找出流量的峰值、平均值、波动情况和周期性模式。这种方法适合已经稳定运行一段时间，并且有完善监控体系的系统。这个方法的优点在于，基于真实的业务数据，能够直接反映实际流量特征。缺点是依赖于历史数据的完整性和准确性，无法预测未来的流量变化，对于新系统或者流量剧烈变化的情况适用性较差，并且需要较长的时间观察才能获得可靠的数据。

还有压测法，通过使用压力测试工具，比如 JMeter、k6、wrk 等，模拟各种负载场景，逐步增加请求量，同时监控系统的关键指标，找到系统性能下降的临界点。它的优点是可以有效了解系统的性能上限，为阈值设置提供支持，同时测试系统在极端情况下的表现。但准备测试环境和工具的成本较高，测试结果会受到环境和测试方法的影响，需要专业人士来操作和分析，同时也难以完全模拟真实复杂场景。

------------------------------

339. 熔断器处于开启状态后如何处理请求？, page url: https://www.mianshi.icu/question/detail?id=839
339-839-答案：
简单题，在讨论熔断的时候有可能问到，在校招和初中级工程师面试中有可能问到。

坦白说，触发熔断之后能做的事情，和被限流的请求比起来，能玩的花活还是要少一些的。毕竟熔断讲究的是快速失败，防止雪崩。

在这个问题之下，你可以用动态调整权重的负载均衡算法来刷亮点，因为这个算法可以针对触发熔断的服务节点进行特殊处理。
前置知识：
当熔断器处于开启状态时，它不会进一步处理请求。这样做的目的是为了防止对已经故障的后端服务施加更大的压力，同时也避免当前服务出现级联故障。这现了熔断器的核心策略快速失败（Fail-fast）。

在这种开启状态下，熔断器处理请求的方式通常是直接拒绝。也就是说，熔断器会立即返回一个错误响应给客户端，告知服务暂时不可用。例如，它可能会返回HTTP 503错误码，这表示服务不可用。快速返回这样的错误响应，不仅可以避免客户端长时间等待，还能提升用户体验，同时减少系统资源的消耗。错误响应的具体内容可以根据不同的应用场景进行定制。

此外，在某些情况下，熔断器还可以选择返回降级的响应，而不是直接拒绝请求。这种降级响应可能包括一些默认值、缓存的响应，或者是一些简化功能的结果，从而保持用户体验。比如，如果某个服务无法提供实时数据，而之前又缓存了请求的结果，熔断器就可以直接返回这些缓存的结果，而不需要去访问已经故障的后端服务。
使用熔断器的时候，其实有一个隐含的假设，即负载均衡器运作良好。也就是说，如果要是负载均衡效果不好，很容易出现某个节点已经触发了熔断，但是别的节点还是很清闲的情况。

------------------------------

340. 熔断器处于半开启状态后如何处理请求？, page url: https://www.mianshi.icu/question/detail?id=840
340-840-答案：
简单题，在讨论到熔断的时候有可能遇到，校招或者初中级岗位面试有可能遇到。

这个问题和开启状态的比起来，就是请求分成了两类，正常处理的和拒绝的。因此可以通过讨论怎么平滑控制两类请求的比率来刷亮点，大部分的熔断器的设计和实现都没有考虑这一点。
总的来说，半开启就是：
当熔断器处于半开启状态时，请求被分成两部分，一部分是正常处理的请求，熔断器需要根据处理结果来调整自身的状态。

另外一部分是被拒绝的请求，这些请求主要有三种处理方式：

首先，熔断器会直接拒绝这些请求，并立即返回一个错误响应给客户端，告知服务暂时不可用。这可能是HTTP 503错误码（服务不可用）。快速返回错误响应不仅避免客户端长时间等待，提升用户体验，还能减少资源消耗。错误响应的具体内容可以根据应用场景进行定制。
半开启状态中，比较重要的一个点是如何调整正常请求和拒绝请求的比例，也就是所谓的灰度究竟怎么灰的问题。

从原则上来说，我们希望尽快从半开启状态进入到关闭状态，也就是正常处理所有的请求。但是这里面又有一个问题，如果正常处理的请求比例增长太快，很容易引出熔断的抖动问题。

我就用过两种策略。

------------------------------

341. 什么是 UDP 协议？, page url: https://www.mianshi.icu/question/detail?id=841
341-841-答案：
简单题，至少比 TCP 简单很多。在校招和初中级岗位面试中很常见，高级岗位一般是聊可靠 UDP 传输了。

在这个问题之下，装逼点就一个：可靠 UDP 传输。
UDP（用户数据报协议）是一种无连接的网络协议，工作在传输层，为应用程序提供了一种简单的、不可靠的数据传输服务。与TCP（传输控制协议）不同，UDP不提供数据包的顺序保证、重传机制或拥塞控制，因此具有较低的延迟和较高的传输速率，但牺牲了可靠性。

UDP的特点：

UDP的应用场景，UDP适用于那些对实时性要求高、可以容忍一定程度丢包损失的应用，例如：
UDP（用户数据报协议）是一种无连接的传输层协议，它提供了一种简单、高效但不可靠的数据传输服务。

UDP的主要特点包括无连接性、面向数据报、低开销和不可靠性，即它不保证数据包的顺序、可靠到达或错误纠正。
在高并发环境下，传统的TCP协议由于其复杂的拥塞控制、重传机制和三次握手过程，可能导致较高的延迟和较大的开销，难以满足实时性和高性能的需求。为此，引入了可靠UDP传输，如 QUIC 来解决这些问题。
并且进以来来说，RPC 框架的设计也同样受到了可靠 UDP 传输的影响。这主要体现在两方面，一方面是类似于 gRPC 这种，是建立在 HTTP 上，而 HTTP 是建立在 QUIC 上；另外一方面来说，是 RPC 框架也可以直接建立在可靠 UDP 传输上。

------------------------------

342. 为什么分布式锁要设置过期时间？, page url: https://www.mianshi.icu/question/detail?id=842
342-842-答案：
简单题，校招不太常问，除非你装逼说自己会写分布式锁。社招有使用分布式锁经验的候选人容易遇到这个问题。

反正记住在跟分布式锁有关的问题下，装逼就不要忘了提分布式锁的续约问题。
其实这个问题后面还有半段：为什么分布式锁要设置过期时间？我等自然结束释放锁不可以吗？

答案是不可以的，因为你的业务在用锁的时候，可能会出现释放锁失败，也可能还没释放锁自己就崩溃了。如果你没过期机制，那么这个锁后续就可能没有任何人能拿到了。如图：
设置过期时间主要是为了防止业务在拿到锁之后，没有正确释放锁。

这主要是两种可能，一种可能是业务方确实尝试释放锁了，但是没成功，比如说网络出现了问题之类的。

另外一种可能是，业务方还在执行业务的过程中就崩溃了，根本没来得及释放锁。
但是引入了过期时间之后，就会有另外一个问题：过期时间不好设置。

如果设置短了，那么业务方可能还没结束业务，锁就过期了。如果设置长了，那么万一要是业务出问题了，这个锁别人也很难拿到。

------------------------------

343. 如果分布式锁过期了，业务还没完成，有什么问题？, page url: https://www.mianshi.icu/question/detail?id=843
343-843-答案：
简单题，校招不常见，因为校招生没有实践经验。社招比较常见。

很多人在使用分布式锁的时候其实都不会去考虑怎么解决这个问题，只是说设置一个足够长的过期时间，寄希望于不会出现这种情况。

而你要装逼，就要指出最好就是借助续约机制，避免出现分布式锁过期而业务还没结束的情况。
造成的问题，说到底就是数据不一致。而从数据不一致，可以引申出来业务重复执行、用户体验差、系统性能下降、死锁等问题。
如果要是出现业务还没结束，而分布式锁就已经过期了的情况，那么有很大概率引发数据不一致的问题。并进一步造成业务重复执行，系统性能下降，甚至造成死锁，从而影响到用户体验。
如果要想解决这个问题，可行的办法并不多。

第一个是搞好监控和告警。这样一旦出现了类似的情况，就及时处理，避免问题和影响面扩大。

------------------------------

344. 分布式锁如何续约？, page url: https://www.mianshi.icu/question/detail?id=844
344-844-答案：
简单题，社招可能遇到，校招不太可能，因为校招没经验。

你在回答这个问题的时候，要深入讨论续约中可能出现的问题，以及对应的解决方案。
我在 github 上有一个开源的 Go 语言分布式锁实现：gotomicro/redis-lock: 基于 Redis 实现的分布式锁 (github.com)，里面有手动续约和自动续约两个方法，你可以参考。
续约从本质上来说就是延长锁的过期时间。假如说分布式锁是基于 Redis 实现的，那么就是再次执行 EXPIRE 命令。
续约的难点不在于如何续约，而是一些细节问题。

第一个问题是续约失败了怎么办？这时候一般会考虑重试，但是重试多次之后都有可能失败，这时候就只能返回错误给业务方，让业务方决定怎么办。大多数时候业务方应该尽量中断自己的业务，如果不能中断则是要告警。

------------------------------

345. 分布式锁续约失败怎么办？, page url: https://www.mianshi.icu/question/detail?id=845
345-845-答案：
简单题，一般出现在社招，校招不会问——毕竟你没写过分布式锁。

在这个问题之下，可以深入讨论业务方中断业务的问题。
续约失败了，首先考虑的是，但是重试也会失败，对于分布式锁的设计者来说，只能是返回一个续约失败的错误。

而真正的麻烦事，是业务方需要能够正确处理续约失败的问题。业务方能选的也不多：

你可能不太理解什么叫做能中断，什么叫做不能中断，我来给你举两个例子。
在分布式锁的使用中，如果要是续约失败了，那么正常是可以进行重试的。但是重试最终都有可能失败，这种情况下业务方就要想好怎么处理了。

一般来说，业务方有两个选择。

第一个选择就是中断业务，也是一个比较好的选择。但是使用这个解决方案，对业务代码有要求。也就是业务要在一些特定的地方插入检测续约失败的代码，一旦发现了就可以中断业务。如果这个业务恰好是一个事务，那就可以回滚事务。如果不是事务，那么可以进一步引入一下反向补偿的机制，将数据还原回去。

------------------------------

346. 什么是 BASE 原理？, page url: https://www.mianshi.icu/question/detail?id=847
346-847-答案：
简单题。这种纯理论，有些面试官很喜欢在校招的时候问，虽然我觉得问校招的人没任何意义。社招问得更多。

在这里，你可以根据自己的理解，讲一下 BASE 在自己的系统里面的具体体现，以及使用 BASE 的时候的注意事项。
BASE 是一个缩写，表达了三个特性：

基本上所有的分布式中间件，业务都是使用 BASE 来作为设计指导思想。比如说：
BASE 是一个缩写，是基本可用性 Basically Available，软状态 Soft State 和 最终一致性 Eventual Consistency三个特性的缩写。

基本可用性是指系统在大多数情况下都能正常响应请求，即使在出现故障或网络分区的情况下，系统仍然能够提供基本的服务。这意味着系统不会因为某个节点的故障而完全不可用。而当下在分布式系统中，我们扩展了它的含义，变成即便分布式系统出现各种问题，但是也能返回一些基本的响应。最直观的体现就降级这种治理措施。
BASE 理论可以看作是 CAP 理论的延伸。CAP 理论指出在分布式系统中，一致性（C）、可用性（A）和分区容错性（P）三者最多只能同时满足两个。而 BASE 理论则专注于在满足可用性和分区容错性的前提下，如何通过最终一致性来逐步达到数据的一致。这种理论为分布式系统设计提供了更多的灵活性和实用性。

在我之前的项目中，我们使用了 Kafka 作为消息队列系统。通过应用 BASE 理论，我们设计了一套异步处理机制，确保了系统的高可用性和最终一致性。在这个过程中，我深刻体会到了 BASE 理论在解决分布式系统问题中的重要性。

------------------------------

347. BASE 和 CAP 的区别是什么？, page url: https://www.mianshi.icu/question/detail?id=848
347-848-答案：
简单题。校招和社招都有可能问到，属于难者不会会者不难的题。

你要先看这两个问题：
CAP 理论是分布式系统中的一个重要理论，它指出在分布式系统中，一致性（Consistency）、可用性（Availability）和分区容错性（Partition Tolerance）三者最多只能同时满足两个。而 BASE 理论是 CAP 理论的延伸，专注于在满足可用性和分区容错性的前提下，如何通过最终一致性来逐步达到数据的一致。

------------------------------

348. 什么是分布式事务？, page url: https://www.mianshi.icu/question/detail?id=849
348-849-答案：
简单题，这就是问个概念，在任何层级的面试中都有可能遇到。你可以把这个问题看做一个信号，面试官准备跟你深入讨论分布式事务的信号。

类似地，在这种起点问题下你就要尽可能提及多的知识点，而后引导面试官提问。而后你也可以进一步提及自己在实践中解决过的分布式事务问题。
分布式事务是指在分布式系统中，涉及多个节点（如不同的数据库、服务或应用）的事务。它需要保证这些节点上的操作要么全部成功，要么全部失败，以确保数据的一致性和完整性（当然在，这是妄想）。

分布式事务和单体事务还是很不一样的，单体事务只涉及数据库一个组件，ACID 之类的特性还是能满足的，虽然也很复杂。

此外，有些人会说分布式事务也追求 ACID，但是我只能说这个想 P 吃。任何的已有的分布式事务解决方案，差不多都是垃圾，不要说 ACID 全满足，连满足单一的一致性都是要了老命。
分布式事务是指在分布式系统中，涉及多个节点如不同的数据库、服务或应用的事务。它需要保证这些节点上的操作要么全部成功，要么全部失败，以确保数据的一致性和完整性（当然在，这是妄想）。

分布式事务和单体事务还是很不一样的，单体事务只涉及数据库一个组件，ACID 之类的特性还是能满足的，虽然也很复杂。

分布式事务有很多解决方案。
从事务的定义上来说，是要满足 ACID 的。但是很可惜，分布式事务基本不可能满足 ACID，甚至于连一致性都很难满足。

举个例子来说，TCC 是微服务架构下非常常见的分布式事务解决方案。但是 TCC 有一个很大的问题，就是它没解释如果要是在 Confirm 的时候，部分节点成功了，部分节点失败了，应该怎么办。

例如说分布式事务，先调用订单服务，在调用支付服务，如果在 Confirm 阶段，订单服务返回了 OK，但是支付服务 Confirm 超时了，怎么办？至少在当下这一刻，数据就是不一致的。

------------------------------

349. 分布式事务有哪些解决方案？, page url: https://www.mianshi.icu/question/detail?id=850
349-850-答案：
难题，热题，热过迪丽热巴的题。难在两方面：解决方案多，每一种方案都有点难，尤其是它们在各种异常情况下的处理方案。

当然在这个问题之下，你可以不用提及非常多的细节，只需要给出结论，等面试官追问就可以。而后你可以谈谈自己使用过的方案，刷一个经验丰富的人设。


先来看通用的分布式事务解决方案：
分布式事务有非常多的解决方案。

首先是著名的两阶段提交协议和三阶段提交协议，它们可以看做是其它很多分布式事务解决方案的理论基石。比如说 XA 事务规范，它就是一个基于两阶段提交协议的具体的落地规范。
就我个人使用经验来说，用的最多的就是在微服务架构中使用 TCC 来作为分布式事务的解决框架。具体来说，TCC 三个都分别对应了一个微服务调用，又事务框架 Seata 进行协调。
我还设计过一个更加复杂的，基于 SAGA 理论的事件驱动分布式事务框架。它本质上使用的是 SAGA 事务的理论，即拆解成一个个非常细小的本地事务，而后提供对应的反向补偿措施。

而基于事件驱动，则是不管是正向的本地事务，还是事务失败之后的反向补偿操作，都是一个个处理事件的消费者。举个简单的例子，假如说一个业务有 A B 和 C 三个步骤，那么 A 步骤是一个本地事务，成功之后会直接发送一条消息，而后消费者执行 B 步骤，也是一个本地事务。如此类推。

------------------------------

350. 在熔断机制中，如何决定何时触发熔断？有哪些常用的策略？, page url: https://www.mianshi.icu/question/detail?id=852
350-852-答案：
简单题，在校招和初中级工程师面试中都有可能遇到。

在这个问题之下，你可以结合实践谈谈自己用过的熔断策略，引导话题，刷出亮点。
前置知识：

熔断机制的核心在于保护系统的稳定性和可用性，以防止因后端服务故障而导致的连锁反应。

从本质上来说，什么时候判定服务已经不健康了，就可以触发熔断了。因此不同的熔断策略，就是在判定服务是否健康上，采用了不同的判定方式。
熔断机制的核心目标是保护系统的稳定性和可用性，防止后端服务故障引发连锁反应。其关键在于判定服务何时处于不健康状态，从而触发熔断。不同的熔断策略采用了不同的判定方式。

总体来看，判定方式主要有两类：一是单一指标一票否决，即当某个关键指标超过预设阈值时触发熔断，例如内存不足时触发熔断；二是复合指标计算分数，通过多个指标加权计算得出分数，超过阈值时触发熔断。
而具体采用什么熔断策略，也是要看业务的。

比如说在优先保护用户体验的场景下，就应该采用针对响应时间的熔断策略。而如果对响应时间不敏感，但是对数据一致性、系统稳定性比较看重的场景，就应该优先考虑采用跟异常有关的熔断策略。

实践中，也可以根据自己的需要，设计独特的熔断策略。

------------------------------

351. 你了解两阶段提交协议吗？, page url: https://www.mianshi.icu/question/detail?id=853
351-853-答案：
简单题，在各个层级的面试中容易遇到，尤其是校招。

那在这个问题之下，你可以从两个角度刷亮点：一个角度是错误处理，也就是任何一个步骤、任何一个参与者失败了怎么办；另外一个角度是深入讨论 2PC 和 ACID，结论自然是 2PC 一点都没实现 ACID。
两阶段提交协议（Two-Phase Commit Protocol，简称2PC）是一种用于确保分布式事务原子性的协议。它的核心目标是在多个分布式节点上执行的事务要么全部成功，要么全部失败，从而保证数据的一致性。

2PC协议广泛应用于分布式数据库管理系统、分布式事务处理系统等需要强一致性的场景。

2PC 协议涉及两个主要角色：
（基本原理）两阶段提交协议是分布式系统中用于确保事务原子性的一种重要协议。它的核心目标是在多个分布式节点上执行的事务要么全部成功，要么全部失败，从而保证数据的一致性。

（参与方）两阶段提交协议涉及两个主要角色：协调者和参与者。

协调者是事务的发起者和决策者，负责向各个参与者发送指令并收集它们的反馈，最终决定事务的提交或回滚。
然而，两阶段提交只是提了有两个阶段，但是对于如何处理各种异常情况并没有深入讨论。

比如说准备阶段，部分参与者超时了，怎么办？

提交阶段，部分参与者超时了，又怎么办？协调者崩溃了又该怎么办？
而更加进一步，可以分析两阶段提交和 ACID 的关系，尤其是隔离性。

很多的人都会忽略可见性的问题，而实际上如果不能解决隔离性的问题，就不能称为解决了数据一致性的问题。

在 两阶段提交协议里面，丝毫不涉及隔离性的问题。我这里直接举出一个例子来，就能看出来。

------------------------------

352. 如何避免熔断机制的误判？, page url: https://www.mianshi.icu/question/detail?id=854
352-854-答案：
略难的题。这个难是指面试不好回答，也是指实践中就很难解决。一般出现在社招中，校招不太会问，因为校招生没经验。

在这个问题之下，要抽象出一个最终的结论：即不管怎么设计、优化熔断策略，最终都难以避免出现漏判、误判的问题。而这和负载均衡难以彻底解决偶发性负载不均衡问题是一样的。
前置知识：
为了避免熔断机制的误判，关键在于提高熔断机制判断服务健康状况的准确性，以及能及时发现并处理误判的情况。我觉得可以从以下几个方面入手。

首先，要考虑优化熔断算法，也就是判定熔断的算法。有几种优化手段：

第一种优化是阈值设置要精确，可以考虑根据监控数据来设置阈值，也可以考虑使用压测得出的阈值，避免使用经验阈值。
然而类似于负载均衡上面临的两个问题：

第一个是无法准确地实时地计算出服务器的负载。

------------------------------

353. 你了解三阶段提交协议吗？, page url: https://www.mianshi.icu/question/detail?id=855
353-855-答案：
简单题，在各个层级的面试中容易遇到，尤其是校招。

那在这个问题之下，你可以从两个角度刷亮点：一个角度是错误处理，也就是任何一个步骤、任何一个参与者失败了怎么办；另外一个角度是深入讨论 3PC 和 ACID，结论自然是 2PC 一点都没实现 ACID。
你需要先看两阶段提交协议的内容：你了解两阶段提交协议吗？

三阶段提交协议（Three-Phase Commit Protocol，简称3PC）是分布式系统中用于确保事务原子性的一种改进协议，旨在解决两阶段提交协议（2PC）的一些缺点，特别是其阻塞性和单点故障问题。

3PC在2PC的基础上增加了额外的询问阶段，将事务的提交过程分为三个阶段：询问阶段（CanCommit）、预提交阶段（PreCommit）和提交/回滚阶段（DoCommit/Abort）。通过引入预提交阶段，3PC减少了系统在不确定状态下的时间，降低了阻塞的可能性。
三阶段提交协议是分布式系统中用于确保事务原子性的一种改进协议，旨在解决两阶段提交协议（2PC）的阻塞性和单点故障问题。3PC在2PC的基础上增加了询问阶段，将事务提交过程分为询问阶段（CanCommit）、预提交阶段（PreCommit）和提交/回滚阶段（DoCommit/Abort）。

在准备阶段，协调者向所有参与者发送询问请求，询问是否可以执行事务。参与者检查自身状态并反馈是否可以继续。

如果所有参与者同意，进入预提交阶段，协调者发送预提交请求，参与者执行预提交操作并确认预提交成功。
然而，三阶段提交协议和两阶段提交协议一样，对于如何处理各种异常情况并没有深入讨论。

（下面内容和两阶段提交协议一样）比如说预提交阶段，部分参与者超时了，怎么办？

提交阶段，部分参与者超时了，又怎么办？协调者崩溃了又该怎么办？
（这部分和两阶段提交协议一样）而更加进一步，可以分析三阶段提交和 ACID 的关系，尤其是隔离性。

很多的人都会忽略可见性的问题，而实际上如果不能解决隔离性的问题，就不能称为解决了数据一致性的问题。

在三阶段提交协议里面，丝毫不涉及隔离性的问题。我这里直接举出一个例子来，就能看出来。

------------------------------

354. 你了解 TCC 吗？, page url: https://www.mianshi.icu/question/detail?id=856
354-856-答案：
简单题，在各个层级的面试中容易遇到，尤其是校招。

那在这个问题之下，你可以从两个角度刷亮点：一个角度是错误处理，也就是任何一个步骤、任何一个参与者失败了怎么办；另外一个角度是深入讨论 TCC 和 ACID，结论自然是 TCC 一点都没实现 ACID。
你要先看两阶段提交协议：你了解两阶段提交协议(2PC)吗？

TCC 非常类似 2PC 的流程步骤。

TCC（Try-Confirm-Cancel）是一种补偿性事务管理机制，特别适用于分布式系统中，尤其是微服务架构中保证事务的一致性。以下是对TCC模式的详细解释：
TCC，即Try-Confirm-Cancel，是一种适用于分布式系统的补偿性事务管理机制。

TCC模式涉及协调者和参与者两种角色，协调者负责控制事务流程，参与者执行具体操作并反馈结果。

其核心思想是将事务拆分为三个阶段：首先是Try阶段，各参与者执行事务的预处理操作；接着是Confirm阶段，如果所有参与者的Try阶段都成功了，事务则被提交；最后是Cancel阶段，若任一参与者在Try或Confirm阶段失败，协调者通知所有参与者回滚事务，释放资源，恢复到事务开始前的状态。
然而， TCC 对于如何处理各种异常情况并没有深入讨论。

比如说 Try 阶段，部分参与者超时了，怎么办？

Confirm 阶段，部分参与者超时了，又怎么办？协调者崩溃了又该怎么办？
而更加进一步，可以分析 TCC 和 ACID 的关系，尤其是隔离性。

很多的人都会忽略可见性的问题，而实际上如果不能解决隔离性的问题，就不能称为解决了数据一致性的问题。

在 两阶段提交协议里面，丝毫不涉及隔离性的问题。我这里直接举出一个例子来，就能看出来。

------------------------------

355. 你了解分布式事务解决方案 SAGA 吗？, page url: https://www.mianshi.icu/question/detail?id=857
355-857-答案：
略难的题。一般校招比较少问，或者说面试官水平一般的时候不太会问这个内容，面试公司如果有大规模分布式系统，有非常复杂的分布式事务，那么面这个问题的几率就会大增。

相比两阶段提交、三阶段提交和 TCC，SAGA 在实践中用得比较少。

类似于别的分布式事务方案，在SAGA面试中，同样是通过讨论 SAGA 事务的错误处理，以及 SAGA 事务和 ACID 的关系来赢得竞争优势。当然后续我要是提供了基于事件驱动的 SAGA 框架的案例，你就可以用我的案例来刷超级亮点。
这个方案很多地方和可靠消息的最终一致性方案很类似，所以你可以了解一下你了解可靠消息最终一致性方案吗？


SAGA模式将一个长事务拆分为多个本地事务，每个本地事务对应一个服务操作，并通过事件或消息驱动的方式来协调这些本地事务。SAGA模式通常分为两个阶段：事务分割阶段和补偿阶段。


SAGA模式涉及以下角色：
SAGA模式是一种有效处理长事务的方法，它通过将一个复杂的长事务拆分为多个独立的本地事务，每个本地事务对应一个服务操作，并通过事件或消息驱动的方式来协调这些本地事务的执行。具体来说，SAGA模式分为两个关键阶段：事务分割阶段和补偿阶段。

在事务分割阶段，长事务被细分为多个小的本地事务，每个本地事务独立执行并提交。每当一个本地事务成功执行，它会生成一个事件或消息，从而触发下一个本地事务的执行。

而在补偿阶段，如果某个本地事务执行失败，SAGA模式会启动补偿操作，补偿操作通常通过反向操作来实现，比如撤销之前的更新或恢复数据。
在当前大规模分布式系统中，消息队列通常被用作事件或消息的载体。在这种背景下，SAGA模式可能遇到的失败场景主要分为两类：

第一类是发消息失败。这种情况发生在当前步骤执行完毕后（无论成功还是失败），尝试将消息发送到消息队列时失败。例如，在之前的退款例子中，如果步骤3执行失败，需要步骤2进行补偿操作时，消息发送失败就会导致问题。

第二类是消费消息失败。这指的是消费者在接收到消息后处理失败。处理失败的原因可能多样，既可能是业务逻辑本身未成功执行，也可能是业务逻辑虽然成功，但在提交偏移量时出现了问题。
而很显然，这个方案和 ACID 也是不沾边的，尤其是隔离性的问题。

比如说在执行事务失败，开启了反向补偿的时候，其实并不能阻止别的服务读取到还没有补偿的数据。这就是类似于数据库中的脏读概念。

------------------------------

356. 什么是熔断？, page url: https://www.mianshi.icu/question/detail?id=858
356-858-答案：
简单题，在微服务话题下很常见。在校招和初级工程中有可能遇到，一般高端的面试不太会讨论这种简单的问题。

在这个问题之下，刷亮点有两个策略。第一个策略是谈及自己使用过的熔断案例，最好是项目经历中的案例，而后将话题引导过去你准备面试项目上；第二个策略是讨论熔断、降级、限流三者的本质，也就是它们其实是一回事。

熔断是一种重要的分布式系统容错机制，用于处理不可靠的远程服务或资源调用，防止单个服务故障导致整个系统级联故障（也称为雪崩效应）。它借鉴了电力系统中熔断器的原理，当系统检测到某个服务不可用或性能严重下降时，会自动“熔断”对该服务的调用，防止故障进一步扩散到其他服务，保证整个系统的稳定性和可用性。

可以将熔断机制想象成一个开关。当某个服务出现故障或不可用时，它会“断开”与该服务的连接，避免其他服务因持续调用该故障服务而受到影响，从而保障整个系统的稳定运行。一旦故障服务恢复，它会自动“闭合”，重新建立与该服务的连接。


熔断机制通常包含以下几个关键要素：
熔断是一种重要的分布式系统容错机制，一般通过有限状态机实现。常用于处理不可靠的远程服务或资源调用，防止单个服务故障导致整个系统的级联故障（也称为雪崩效应）。它借鉴了电力系统中熔断器的原理，当系统检测到某个服务不可用或性能严重下降时，会自动“熔断”对该服务的调用，防止故障进一步扩散到其他服务，从而保证整个系统的稳定性和可用性。

可以将熔断机制想象成一个开关。当某个服务出现故障或不可用时，它就“断开”与该服务的连接，避免其他服务因持续调用该故障服务而受到影响，从而保障整个系统的稳定运行。一旦故障服务恢复，熔断机制会自动“闭合”，重新建立与该服务的连接。

熔断机制通常包含几个关键要素：
我在实践中也用过很多次熔断来保护我的系统。
熔断经常和限流、降级联系在一起讨论，而且在实践中三者的界限也不是那么分明。

举个例子来说，当我们说触发熔断之后可以返回一个默认返回值，这本身也可以看做是一种降级措施。又或者在实践中可以将三者混合使用，例如说在限流策略中，被限流的请求可以直接熔断。

------------------------------

357. 为什么需要熔断？, page url: https://www.mianshi.icu/question/detail?id=859
357-859-答案：
简单题，在微服务话题下很常见。在校招和初级工程中有可能遇到，一般高端的面试不太会讨论这种简单的问题。

在这个问题之下，最佳刷亮点策略是谈及自己使用过的熔断案例，最好是项目经历中的案例，而后将话题引导过去你准备面试项目上。

前置知识：
熔断的主要作用是提升系统的稳定性和可用性，并且能够解决了以下几个关键问题：

首先，熔断有效防止了级联故障与雪崩效应。在分布式系统中，一个服务的故障可能导致依赖它的其他服务相继出现故障，从而使整个系统瘫痪。熔断能够快速识别并阻止对故障服务的持续请求，防止故障蔓延，避免级联故障和雪崩效应，提升整个系统的稳定性。


其次，熔断有助于防止系统资源的耗尽。如果故障服务的请求未得到及时阻止，这些请求将占用大量系统资源，如CPU、内存、网络带宽等，最终可能导致系统崩溃或性能严重下降。熔断通过停止对故障服务的请求并立即返回响应，迅速释放被熔断请求占用的资源，从而提升整个系统的可用性。
我在实践中也用过很多次熔断来保护我的系统。

------------------------------

358. 熔断器模式如何防止雪崩效应和级联故障？, page url: https://www.mianshi.icu/question/detail?id=860
358-860-答案：
简单题，在校招和社招都有可能遇到。

其实一句话就能说清楚：熔断中断了调用链路，链路都没了还有个屁的雪崩效应和级联故障了。如果要刷亮点，你可以对比降级和限流，强调熔断解决这两个问题的效果最好。
前置知识：
熔断器之所以对这两个效应有特别好的效果，归根到底就是一句话：熔断器是快速失败的，直接中断了调用链路，使得故障无法扩散出去。也就是彻底避免了雪崩效应和级联故障。
熔断经常和降级、限流并列，但是在防止雪崩效应和级联故障上，后两者是明显不如熔断的。

------------------------------

359. 什么是本地消息表？, page url: https://www.mianshi.icu/question/detail?id=861
359-861-答案：
略难一点，而且这个问题不仅仅是出现在分布式事务场景下，但凡你的业务有一定要发送消息成功的场景，面试官就会问。

在面试中，你可以提及自己研发一个 SDK，提供了本地消息表的通用实现，统一解决补偿任务、监控等问题。
本地消息表是实践中一种非常常见的保证业务成功之后，一定要发消息成功的解决方案。而且它还是可靠消息最终一致性和 SAGA 这两种分布事务解决方案中经常使用到的一种技术。


它的基本步骤和细节都非常多，你最后有就机会自己手写一下。顾名思义，本地消息表就是你需要在数据库中建一张表，这张表包含了以下几个信息：

当然你可以根据自己的实际需要，添加一些额外的字段。
本地消息表的核心思想是在数据库中创建一张专门的表来记录需要发送的消息。这张表通常包含消息内容、Topic、重试次数以及状态，如未发送、已发送、已失败。状态字段尤为重要，只有在超过重试次数后，消息状态才会标记为“已失败”。

假设我们需要在订单创建成功后发送一条消息到Kafka。

那么首先，在进行订单创建等业务数据变更的同时，将需要发送的消息记录写入本地消息表。这两步操作在同一个数据库事务中完成，确保业务数据和消息记录要么同时成功，要么同时失败，保持数据的一致性。此时，消息的状态为未发送。
我在公司的时候，就发现这个本地消息表是一个很通用的解决方案，但是每个业务方要用的时候都得自己实现一遍，因此我就搞了一个通用的库，用户引入这个库就可以了。

具体来说，用户要初始化好本地消息表，也可以交给我的库来初始化。而后每当用户要插入本地消息的时候，就传入一个事务的实例，消息的内容，我的库会自动插入。

当用户提交事务之后，我的库会直接发送消息，并且尝试更新数据库中消息状态为已发送。

------------------------------

360. 你了解可靠消息最终一致性方案吗？, page url: https://www.mianshi.icu/question/detail?id=862
360-862-答案：
简单题。事实上在面试中比较少问这个问题，更加多的是问本地消息表。

在这个问题之下，你要刷亮点，还是要落在讨论本地消息表的错误处理，以及该方案是否支持 ACID。
可靠消息最终一致性方案是分布式事务的一种常见解决方案，旨在在异步通信的场景下保障消息传递的可靠性和系统的最终一致性。

可靠消息最终一致性方案有三个参与者：

它的步骤稍微有点复杂：
（定义）分布式事务可靠消息最终一致性方案是一种常见的解决方案，它通过结合消息队列和事务处理机制来保证数据最终一致性。

也就是说，系统不保证在任意时刻所有节点数据都是一致的，但经过一定时间后，数据将达到一致状态。而可靠消息则确保消息在传递过程中不会丢失，并且能够被正确处理。

（参与方）参与方主要包括事务发起方、事务参与方和消息队列。事务发起方负责执行本地事务并将相关操作转化为消息，发送到消息队列。消息队列则负责存储和传递消息给各个事务参与方。事务参与方接收到消息后，执行本地事务，并根据执行结果向消息队列发送确认或取消消息。
这个方案大量使用了消息，所以大部分的超时、部分失败的场景都是出现在发送消息，消费消息两方面。

举个例子来说，在发起者执行本地事务之后，要发送一个消息到消息队列上，但是这个步骤有可能失败。这其实就是一个如何保证消息一定发出去的问题。很显然这个可以借助我们经常使用的本地消息表解决方案。
而很显然，这个方案和 ACID 也是不沾边的，尤其是隔离性的问题。

比如说当出现部分参与者执行成功，部分参与者执行失败的情况，并不能阻止别的服务读取到这些部分成功的数据。这类似于数据库中的脏读概念。

------------------------------

361. 你了解 XA 事务吗？, page url: https://www.mianshi.icu/question/detail?id=864
361-864-答案：
简单题，在各个层级的面试中容易遇到，尤其是校招。

那在这个问题之下，你可以从两个角度刷亮点：一个角度是错误处理，也就是任何一个步骤、任何一个参与者失败了怎么办；另外一个角度是深入讨论 2PC 和 ACID，结论自然是 2PC 一点都没实现 ACID。

你需要先看懂 2PC 的内容：你了解两阶段提交协议吗？，整个问题的内容和这个问题高度相似。

XA事务是一种基于两阶段提交（2PC）协议的分布式事务管理机制，旨在确保跨多个数据库或服务的事务能够保持一致性。它由X/Open组织定义，广泛应用于需要强一致性的金融、银行等关键业务场景。


它有两个关键角色：
XA事务是一种基于两阶段提交协议的分布式事务管理机制，旨在确保跨多个数据库或服务的事务能够保持一致性。它由X/Open组织定义，广泛应用于金融、银行等对数据一致性要求极高的关键业务场景。

在XA事务中，事务管理器负责协调整个事务的提交或回滚，而资源管理器则负责具体资源的操作。具体来说，XA事务的工作原理分为两个阶段。

首先是准备阶段，事务管理器（TM）向所有参与事务的资源管理器（RM，如数据库）发送准备提交的请求。各资源管理器将事务状态设置为‘预提交’，锁定相关资源，并返回是否准备好的状态给事务管理器。
然而，XA 只是提了有两个阶段，但是对于如何处理各种异常情况并没有深入讨论。

比如说准备阶段，部分资源管理器超时了，怎么办？

提交阶段，部分资源管理器超时了，又怎么办？事务管理器崩溃了又该怎么办？
而更加进一步，可以分析 XA 和 ACID 的关系，尤其是隔离性和一致性。

很多的人都会忽略可见性的问题，而实际上如果不能解决隔离性的问题，就不能称为解决了数据一致性的问题。

在 XA 里面，丝毫不涉及隔离性的问题。我这里直接举出一个例子来，就能看出来。

------------------------------

362. 怎么保证接口幂等？, page url: https://www.mianshi.icu/question/detail?id=866
362-866-答案：
略难的题，基本上任何面试中都有可能问到。

很多人都能回答出来借助唯一索引来保证幂等，但是这个回答有两个问题：部分失败问题，不高端没有竞争力。所以这里会教你使用布隆过滤器和Redis来做幂等。
幂等的意思是，同一个请求执行多次的结果是一样。大多数时候，幂等是通过拒绝多次执行来达成的。

1. 利用唯一索引来保证幂等
基本原理：
有多种方案可以保证接口是幂等的。

第一种也是最简单的方案就是利用唯一索引。通过在数据库表中设置唯一索引，如订单号或用户ID，可以确保每次插入或更新操作都不会产生重复数据。当尝试插入重复数据时，数据库会抛出唯一性约束异常，系统可以捕获该异常并进行相应处理。

这种方法实现简单，直接利用数据库的特性，保证了数据的一致性和唯一性，但在高并发情况下可能会成为瓶颈，且异常处理逻辑需要谨慎设计。
在实践中，所有方案都要注意一个关键问题：部分失败问题。

比如说单纯使用 Redis 来保证幂等的解决方案，当判定 Redis 上没有这个键的时候，有两种选择。

第一种选择是先插入这个键，如果要是处理失败，就删除这个键。但是这里有一个问题，就是当我处理失败之后，删除这个键失败了，又或者我直接崩溃了，那么后续本来可以借助重复请求重试的，结果现在都重试不了。

------------------------------

363. 什么是幂等？为什么幂等那么重要？, page url: https://www.mianshi.icu/question/detail?id=867
363-867-答案：
简单的概念题，基本上也就是校招之类的会问一下了。

在这个问题之下，你可以罗列一下自己使用过幂等的场景。注意的是，这里最好列举你项目里面最牛逼的，能够让你刷出亮点的场景。
幂等性（Idempotence）是一个重要的概念，它指的是一个操作或函数多次执行的结果与执行一次的结果相同。换句话说，无论这个操作执行多少次，系统的状态都保持不变，或者最终结果都是一样的。
幂等性是一个重要的概念，它指的是一个操作或函数多次执行的结果与执行一次的结果相同。换句话说，无论这个操作执行多少次，系统的状态都保持不变，或者最终结果都是一样的。

幂等性的重要性体现在几个方面：首先，它提高了系统的可靠性，特别是在分布式系统中，网络延迟或故障可能导致请求重复发送，幂等操作可以确保系统状态的一致性；

其次，它简化了错误处理和重试逻辑，因为不需要担心重复执行带来的副作用；
我在实践中设计过非常多的幂等接口。

第一个场景就是表单提交，通过要求表单带上一个唯一标识符，可以解决表单重复提交的问题，例如说在电商里面非常重要的重复创建订单问题。

第二个例子则是消费者幂等。实践中我会要求我的同事，要尽可能将消费者设计为幂等，而后叠加重试，可以确保消息一定会被消费，并且至多被消费一次。

------------------------------

364. 怎么利用唯一索引来保证接口幂等？, page url: https://www.mianshi.icu/question/detail?id=868
364-868-答案：
简单题，但是如果唯一索引不能做成业务本地事务的一部分，那么这个题就有点难了。基本上在校招或者初级工程师岗位的面试中会见到。

回答这个问题，你刷亮点就是要揭露出来，这个方案之所以靠谱，没有部分失败的问题，都是借助本地事务达成的。而如果操作唯一索引并不能作为本地事务的一部分，那么这个方案就同样有部分失败的问题。
在使用唯一索引来保证接口幂等的时候，有两种情况：

先来看第一种情况，这里将所有的细节抽象之后，就可以认为业务操作就是一大堆的数据库操作，其中一个操作会插入一行数据，这行数据所在的表就有一个唯一索引。过程类似于下图：


使用唯一索引来保证接口幂等的方法很简单，就是要在业务关键表上利用已有的唯一索引，或者创建一个新的唯一索引。

而后在处理请求的时候，先开启事务，而后插入唯一索引，如果插入成功，那么就说明没有冲突，可以继续往后处理业务。

如果要是冲突了，则回滚事务，直接结束。根据业务场景，可以返回类似 409 Conflict 之类的错误，告诉调用者业务冲突了。
当唯一索引操作不能作为业务的本地事务的一部分的时候，事情就变得比较复杂了。例如说在微服务架构下，处理一次请求不仅仅要操作数据库中的数据，还要调用下游很多个服务。

这种情况下就只能考虑采用状态控制和补偿任务的机制了。具体来说，有四个步骤。

首先，请求到来的时候，立刻插入一条唯一索引的数据，此时状态是初始化状态。

------------------------------

365. 只利用 Redis 怎么保证接口幂等？, page url: https://www.mianshi.icu/question/detail?id=869
365-869-答案：
略难的题，难点就在于要考虑解决部分失败的问题。这个问题很少问，因为一般来说考察幂等都是考察借助唯一索引来实现幂等。

具体步骤

整个过程如下图：

在只使用 Redis 来去重的情况，分成两个步骤：

首先，客户端会生成一个唯一的请求标识，比如使用UUID，并将其作为请求参数发送到服务器。如果服务器已经知道业务数据中包含可用的唯一标识，那么客户端就可以直接传递业务数据，无需额外生成标识。

接着，服务器在接收到请求后，会立即使用SETNX命令将这个唯一标识存储到Redis中。SETNX命令会检查该标识是否已存在，如果不存在则设置成功并返回true，表示这是一个新的请求；如果已存在则返回false，表示这是一个重复的请求，服务器会直接拒绝处理。只有当SETNX设置成功后，服务器才会继续执行相应的业务逻辑。
这里有一些小技巧可以缓解 Redis 的内存消耗，也就是给 Redis 的唯一标识符设置一个过期时间。

这种清理是基于这样一个假设：重复请求只会在近期出现。也就说，如果一个业务刚刚执行完毕，那么这个业务大概率还有可能遇到重复请求。但是如果一个业务已经执行了好久了，比如说已经过去了一周，那么这种情况下，这个唯一标识符的过期时间就可以设置为七天。

------------------------------

366. 服务触发熔断之后，是如何恢复的？, page url: https://www.mianshi.icu/question/detail?id=873
366-873-答案：
简单题，校招和社招中都很常见。

这个题目本质上就是考察你是否知道半开启这个状态，知不知道要避免抖动的问题。你可以讨论你在别的地方也用过类似的思想，避免了系统震荡或者系统过载的问题。
这个问题的答案就是讨论半开启状态，以及为什么要引入半开启状态，参考这个问题：什么是半开状态（Half-Open），它在熔断器模式中的作用是什么？
服务的恢复过程分成两个阶段。

首先，当服务触发熔断之后，一般来说会有一个定时器。当定时器到时间之后，熔断器就会退出完全开启的状态，进入到半开启的状态。
半开启的这种状态，可以看做是一个从关闭到开启状态的灰度。我也利用过很多这种思想。

比如说在我设计的 Redis 集群互为备份的容错方案中，也就是当 Redis 集群崩溃的时候，会切换到使用备份 Redis 集群。而当 Redis 集群再次恢复的时候，会把流量从备份 Redis 集群切回来正常的 Redis 集群，但是此刻并不是立刻切换 100% 流量，而是引入了一个灰度过程。

------------------------------

367. 你在实际项目中使用过熔断吗，遇到了哪些挑战，如何克服的？, page url: https://www.mianshi.icu/question/detail?id=874
367-874-答案：
简单题，在校招中不太会遇到，社招中可能遇到。

在这个问题之下，其实有一个非常好回答的点，就是讨论熔断中难以处理的漏判和误判问题。当然，你也可以根据自己的体会来刷亮点。
前置知识：
在电商订单系统项目中，我使用了熔断器，主要目的是保护核心的订单处理服务，防止下游依赖服务（如支付服务）出现故障影响整体流程。我们选择了Resilience4j作为熔断器的实现。在使用过程中，我们面临了以下几个挑战。

第一个挑战是阈值设置的难度。熔断器的有效性很大程度上依赖于阈值的合理设置。最开始，我们根据经验设定了请求成功率和平均延迟的阈值，但在实际运行中发现这些阈值并不总是适用。具体来说，高波动性的问题是支付服务有时会出现短暂的抖动，导致熔断器频繁地打开和关闭，从而使订单处理服务变得不稳定，进而影响用户体验。再比如在促销活动高峰期，即便支付服务没有出现故障，负载的增加可能导致指标超过阈值，触发不必要的熔断，从而影响订单处理能力。
其实实践中跟熔断有关的问题，都可以归结为一个问题：怎么知道服务是否还处于一种健康状态。

例如我刚说的阈值的问题，其实就是阈值这个东西理论上应该就是服务处于健康到不健康状态的临界点。但是很可惜的，我们也没办法确定这个临界点究竟在哪里。这个问题其实跟负载均衡面临的两个难点是一回事：

第一个是无法准确地实时地计算出服务器的负载。

------------------------------

368. 在微服务架构中，如何应用熔断器模式，能解决哪些问题？, page url: https://www.mianshi.icu/question/detail?id=875
368-875-答案：
简单题，在校招和社招里面都有可能遇到。

在这个问题之下，最好的还是列举自己在微服务架构下用的熔断的案例，从而将话题引导到自己的项目经历上。
你可以先参考：什么是熔断器？请简要解释其设计初衷和目的。

在微服务架构中，熔断器模式是一种重要的容错机制，能够有效地提高系统的稳定性和可靠性。
熔断器可以通过 SDK、服务网格或者 API 网关接入。

SDK 接入比较灵活，容易定制，但对应用的侵入性较强。而服务网格的实现则可以提供全局视角，管理跨服务的熔断策略，尽管在精细的策略定制方面可能受限。API 网关类似于网格服务，但是额外有网关瓶颈问题。
在实践中，我都会在系统核心业务的调用链路上接入熔断之类的措施。

------------------------------

369. 怎么利用 Redis 和唯一索引来保证幂等？, page url: https://www.mianshi.icu/question/detail?id=876
369-876-答案：
略难的题，在社招里面可能会遇到，校招不太常见。

实际上，如果你的项目经历中涉及到了幂等的问题，那么你可以考虑将这个解决思路作为你的面试案例。

你在面试的时候，要注意深入讨论部分失败的，并且要强调 Redis 的效果是能快速拒绝重复请求，而非重复请求最终都还是要落到数据库上的。
首先看 Redis 和唯一索引在这个案例中扮演的角色：

具体的执行步骤

这里有一个细节，因为 Redis 承担着拒绝重复请求的职责，所以最好不要在业务执行完毕之后就立刻删掉。相反，应该给 Redis 上的唯一标识符一个过期时间。举例来说，如果你预测一个业务在半小时内都有可能遇到重复请求，那么就可以将过期时间设置为半小时。
这个方案里面有两个关键角色。

第一个是Redis，作为一个高性能的键值存储，能够快速地存储和检索请求的唯一标识，是防止重复执行的关键。

第二个是数据库中的唯一索引防止了重复数据的插入或更新，这是作为整个幂等方案的兜底。
这里第一个值得讨论的问题就是 Redis 中存储的唯一标识符最好有一个过期时间。基本思路将 Redis 看做是一个快速去重的缓存，而过期时间的长度就是可能会出现重复请求的时间长度。

举个例子来说，如果预测重试请求只会出现在半小时内，那么这个过期时间就可以设置为半小时。这能有效减少 Redis 内存的消耗。

进一步来说，就算业务还没正常结束，但是唯一标识符过期了，那么也没关系，因为这个时候还有唯一索引来兜底。
此外，在使用这个方案的时候，还要小心一个问题，即唯一索引的操作能不能作为本地事务的一部分。

------------------------------

370. 怎么利用布隆过滤器和唯一索引来保证幂等？, page url: https://www.mianshi.icu/question/detail?id=877
370-877-答案：
略难的题，因为布隆过滤器本身也是一个热点，所以在面试中遇到的概率要大一些。校招和社招都有可能遇到。

在这个问题之下，要深入讨论布隆过滤器的假阳性问题，以及部分失败的问题。

------------------------------

371. 怎么利用布隆过滤器和唯一索引来保证幂等？, page url: https://www.mianshi.icu/question/detail?id=878
371-878-答案：
略难的题，因为布隆过滤器本身也是一个热点，所以在面试中遇到的概率要大一些。校招和社招都有可能遇到。

在这个问题之下，要深入讨论布隆过滤器的假阳性问题，以及部分失败的问题。
可以参考：怎么利用 Redis 和唯一索引来保证幂等？，并且要先阅读：怎么利用唯一索引来保证接口幂等？

从效果上来说，布隆过滤器主要用于快速判断一个元素是否可能存在于一个集合中。而唯一索引用于确保数据库表中某一列或某几列的组合值是唯一的，从而防止重复数据的插入。

因此总的来说，布隆过滤器更加像是一个缓存，只用于快速去重，最终都还是要靠唯一索引来兜底。整个步骤包括三步：
布隆过滤器主要用于快速判断一个元素是否可能存在于一个集合中，类似于一个高效的缓存，用于初步去重。而唯一索引用于确保数据库表中某一列或某几列的组合值是唯一的，从而从根本上防止重复数据的插入。

具体实现步骤可以概括为三个阶段：首先，通过布隆过滤器设置标记位，例如使用 Redis 的 BF.ADD 命令。如果元素不存在，返回1；如果存在，返回0。

其次，如果布隆过滤器认为元素未处理过，则执行业务逻辑，并在此过程中确保数据库中的唯一索引已设置好。
使用布隆过滤器的最理想的场景，就是判定是否为重复请求的时候消耗很大，那么通过布隆过滤器就可以极大的减少这种消耗。

举个例子来说，如果业务可以无脑插入唯一索引来去重，那么其实不太适合用布隆过滤器。因为如果要是布隆过滤器判定不存在，可以无脑插入；而如果布隆过滤器判定存在，还是可以无脑插入。
此外，在使用这个方案的时候，还要小心一个问题，即唯一索引的操作能不能作为本地事务的一部分。

------------------------------

372. 怎么利用布隆过滤器、Redis 和唯一索引来保证幂等？, page url: https://www.mianshi.icu/question/detail?id=879
372-879-答案：
略难的题，你如果不主动提，几乎不会被问到。

实际中这个方案几乎不会用，只有在超大数据量，超高并发的场景下才会考虑。而且它会引入更多的复杂度，但是收益又很小。

在面试中，这个方案看起来就非常牛逼了，很适合装逼刷亮点。大部分面试官没这个能耐看出这是一个华而不实的方案。
你要先看这几个问题：

从角色上来说：

整个执行流程如下图：
这是一个比较复杂的方案，要先明确布隆过滤器、Redis 和唯一索引三者的作用。

布隆过滤器负责快速判定请求是否为重复请求，承载最高的并发和最大的数据量；Redis则用于解决布隆过滤器的假阳性问题，存储近期业务的唯一标识符，既能撑住高并发，又节省内存；唯一索引则是最后的兜底机制，确保在任何步骤失败或遇到部分失败问题时，保障数据的一致性。

整个执行流程可以概括为：首先，通过布隆过滤器检测是否为重复请求并且设置好标记位，使用Redis的BF.ADD命令，返回1表示非重复请求，直接执行业务；返回0则可能是重复请求，需进一步借助Redis判定。
这个方案因为是三方参与，所以部分失败的问题更加严重。它主要有两种部分失败的场景。

第一种是布隆过滤器判定非重复后程序崩溃，这种场景下不需要做额外的事情。后续即便有重复请求到来，它也会时视为假阳性，从而被正常处理。
此外，在使用这个方案的时候，还要小心一个问题，即唯一索引的操作能不能作为本地事务的一部分。

------------------------------

373. 熔断器可能带来哪些负面影响？如何缓解？, page url: https://www.mianshi.icu/question/detail?id=883
373-883-答案：
略难的题，一般你得有使用经验才能意识到熔断的负面影响，而后去琢磨怎么改进。主要出现在社招中。
很显然，在计算机里面，没有什么东西是没有代价的。熔断也会带来一些问题：
首先，熔断器可能因阈值设置不合理或短暂网络波动而误触发，导致正常服务受影响。其次，熔断触发后的服务降级会提供有限功能或错误提示，影响用户体验。此外，引入熔断器会增加系统复杂性，需要额外配置和维护，且熔断器本身也会消耗资源，可能延长响应时间。

------------------------------

374. 在高并发环境下，熔断器可能会遇到哪些问题，如何优化？, page url: https://www.mianshi.icu/question/detail?id=884
374-884-答案：
略难的题，你得在高并发环境下用过才有可能遇到这些问题，一般出现在社招中，校招不常见。

其实很多解决方案都是在低并发场景下没有任何问题，但是在高并发环境下就会有问题。尤其是各种不起眼的资源消耗，在高并发环境下就蜕变成为一个不能容忍的资源开销。
在高并发环境下，独有的问题是：
熔断器在高并发环境下有三个独有的缺点：

首先，性能瓶颈是一个很大的挑战。因为在高并发情况下，熔断器需要处理大量的请求和状态切换，这可能导致CPU和内存资源消耗过高，响应延迟增加，甚至成为系统的瓶颈。为了解决这个问题，我会采用异步非阻塞的方式来实现熔断器，避免阻塞主线程。此外，选择高性能的熔断器库，比如Resilience4j或Hystrix，也是有效的策略。我还会优化算法，使用高效的数据结构，缓存常用的数据以减少重复计算，并合理配置线程池的大小和资源限制。选择轻量级的实现，可以进一步减少资源消耗。


其次，误判率增加。在高并发情况下，瞬时流量波动较大，容易导致熔断器误判，把健康的服务错误地断开，影响用户体验。为了解决这个问题，我会使用滑动窗口技术来平滑短时间内的抖动，使用百分位数而不是平均值作为判断依据。基于历史数据和实时数据动态调整阈值，使用多维度的指标综合判断，避免依赖单一指标。同时，采用自适应算法动态调整阈值，可以更精准地适应实际情况。

------------------------------

375. 熔断和限流有什么区别和联系？, page url: https://www.mianshi.icu/question/detail?id=885
375-885-答案：
简单题，校招和社招都有可能遇到。

你要刷亮点，就还是要站在一个抽象的故障处理三步曲的角度去，统一描述熔断、限流和降级。
前置知识：

熔断和限流是保证系统稳定性和可用性的常用策略，它们在应对系统过载和故障方面有着不同的侧重点。

两者的关键区别有三个：触发条件不同、侧重点不同、恢复机制也不同。
熔断和限流是确保系统稳定性和可用性的常用策略，它们在处理系统过载和故障时有着不同的侧重点。

首先，触发条件不同。限流的触发条件是系统资源接近饱和，或者请求数量超过系统每秒的处理能力。而熔断的触发条件则是在下游服务出现故障的时候，比如说高错误率、高延迟或超时，表明该服务的健康状态不佳。

其次侧重点不同。限流主要是为了防止过载，控制请求流量，确保系统能够持续提供服务，避免因为请求过多而被压垮。相对而言，熔断的目标则是快速失败并隔离故障，防止服务故障蔓延，保护系统的整体稳定性。
如果我们站在故障检测、故障处理和故障恢复的角度来看这个三者，那么可以发现熔断、限流和降级它们之间没有本质区别，都可以看做是在服务出现问题或者快要出问题的时候，采取的不同的治理措施。

------------------------------

376. 熔断和降级有什么区别和联系？, page url: https://www.mianshi.icu/question/detail?id=886
376-886-答案：
简单题，在校招和社招中都有可能遇到。

你可以站在故障三步曲的角度深入讨论这两者的区别和联系。从本质上来说，熔断、限流和降级并没有本质的去呗。
前置知识：

熔断和降级是保证系统稳定性和可用性的常用策略，它们在应对系统过载和故障方面有着不同的侧重点。

两者的关键区别有三个：触发条件不同、侧重点不同、恢复机制也不同。
熔断和降级是确保系统稳定性和可用性的常用策略，它们在处理系统过载和故障时有着不同的侧重点。

首先，触发条件不同。熔断的触发条件主要是在下游服务出现故障，比如高错误率、高延迟或是超时，这些都说明了那个服务的健康状态不佳。而降级的触发条件就更广泛了，包括系统资源不足、服务故障、请求量过大，甚至可能是基于一些预设的策略判断。

在侧重点方面，熔断的目标是快速失败并隔离故障，防止服务故障蔓延，保护整体系统的稳定性。当检测到下游服务的错误率高或者响应时间异常时，熔断机制会自动“熔断”对该服务的调用。相比之下，降级的侧重点则在于确保核心服务的可用性。当系统的一部分功能不可用时，通过牺牲一些非核心功能或降低服务质量，来保护核心功能的正常运行。
如果我们站在故障检测、故障处理和故障恢复的角度来看这个三者，那么可以发现熔断、限流和降级它们之间没有本质区别，都可以看做是在服务出现问题或者快要出问题的时候，采取的不同的治理措施。

------------------------------

377. 什么是降级?, page url: https://www.mianshi.icu/question/detail?id=887
377-887-答案：
简单题。在微服务相关的面试下非常高频，校招和初中级工程师面试中很常见。

在这个问题之下，你可以引出自己使用过的降级案例，引导话题并且刷亮点。而后进一步揭示出熔断、限流、降级三者的联系，指出它们本质上是一样的，从而赢得竞争优势。
降级是一种重要的分布式系统容错机制，通常是指在微服务架构或分布式系统中，当系统面临异常、压力过大、部分服务不可用或性能下降等情况时，为了保证系统的整体稳定性和核心功能的可用性，主动降低或关闭部分非核心功能或服务的策略。

通过降级，系统能够在资源受限或出现故障的情况下，依然为用户提供基本的服务，避免全局性的系统崩溃或严重影响用户体验。

其核心思想就是四个字：“弃车保帅”。当然，在计算机领域还有一个说法，叫做提供有损服务。而在降级的过程中，还要进一步考虑优雅降级和优雅恢复的问题，这个跟限流、熔断是类似的。
降级是分布式系统中一种重要的容错机制，尤其在微服务架构中尤为关键。其核心思想是“弃车保帅”。

也就是在系统出现问题的时候，主动降低或关闭非核心功能，以确保核心功能的可用性和系统整体稳定性。通过这种方式，系统能在资源受限或出现故障时，依然为用户提供基本服务，避免全局崩溃或严重影响用户体验。因此也被称为提供有损服务。

降级主要解决过载和故障两大问题，采用的手段有三种。
在实践中我用过很多降级策略来保护我的系统，保证用户体验。

比如说在允许异步处理请求的场景下，我就大量使用同步转异步的手段。典型的例子就是在短信服务中，当短信服务本身已经到性能瓶颈的时候，新来的发短信的请求就会转发到消息队列上，等后续再慢慢处理。
熔断经常和限流、降级联系在一起讨论，而且在实践中三者的界限也不是那么分明。

举个例子来说，当我们说触发熔断之后可以返回一个默认返回值，这本身也可以看做是一种降级措施。又或者在实践中可以将三者混合使用，例如说在限流策略中，被限流的请求可以转为异步，这同样可以看做是一种降级措施。

这三者都可以看做是故障检测、故障处理和故障恢复这个框架之下的不同回答。比如说在降级这里，什么时候触发降级对应的就是故障检测步骤。

------------------------------

378. 为什么需要降级？, page url: https://www.mianshi.icu/question/detail?id=888
378-888-答案：
简单题，在微服务的面试中比较有可能遇到，不管是校招还是社招都有可能遇到。

在这个问题之下，你可以提及你使用降级的例子来证明使用降级的必要性，从而将话题引导过去你使用的降级措施上，并且进一步刷出亮点，赢得竞争优势。
前置知识：

在分布式系统和微服务架构中，服务降级是一项关键的容错机制，它通过主动关闭非核心功能或降低服务质量，来保障核心功能的可用性，维护系统整体稳定性，并尽可能提升用户体验。

从本质上来说，降级就是为了应付两个问题：过载和故障。
在分布式系统和微服务架构中，服务降级是一项关键的容错机制。其核心目的是通过主动关闭非核心功能或降低服务质量，来保障核心功能的可用性，维护系统整体稳定性，并尽可能提升用户体验。

降级主要应对两大问题：过载和故障。过载发生在系统请求超过处理能力时，类似高速公路拥堵，突发流量（如非法攻击、大促、秒杀等）会导致响应延迟甚至系统崩溃。

故障则指系统组件或服务异常，如依赖服务不可用、系统Bug等，这时降级确保在部分功能失效时仍能提供服务，例如短信发送失败时改用邮件。
在实践中我用过很多降级策略来保护我的系统，保证用户体验。

比如说在允许异步处理请求的场景下，我就大量使用同步转异步的手段。典型的例子就是在短信服务中，当短信服务本身已经到性能瓶颈的时候，新来的发短信的请求就会转发到消息队列上，等后续再慢慢处理。

------------------------------

379. 常见的降级策略有哪些?, page url: https://www.mianshi.icu/question/detail?id=891
379-891-答案：
简单题。

在实践中，有非常多的降级策略，所以你在回答的时候根据自己的记忆，随便回答几种就可以，反正面试官也不可能知道所有的降级策略。

而后，你还是可以通过提及自己使用过的降级策略来赢得竞争优势。
你需要先看什么是降级？。在这里提及到主要有三个手段：

但是说到非常具体的策略，就非常多了，希望下面这些花里胡哨的技巧没有吓到你。你不需要掌握全部，你在面试前根据自己的理解，将这些策略应用在自己的业务中，设计几个自己的策略，就能在面试中交差了。

常见的降级策略有：
从大方向上来说，降级有三种手段：减少资源损耗、同步转异步、采用备用方案。

而具体的策略则有非常多。

最常见的是采用预设数据。也就是在实时计算数据有问题的情况下，使用提前准备好的数据。举个例子，在首页个性化推荐功能有问题的时候，使用运营团队预先配置好的推荐内容。
在实践中我用过很多降级策略来保护我的系统，保证用户体验。

比如说在允许异步处理请求的场景下，我就大量使用同步转异步的手段。典型的例子就是在短信服务中，当短信服务本身已经到性能瓶颈的时候，新来的发短信的请求就会转发到消息队列上，等后续再慢慢处理。

------------------------------

380. 服务降级的目的和作用是什么？, page url: https://www.mianshi.icu/question/detail?id=893
380-893-答案：
简单题，在校招和社招中都有可能遇到。

说白了就是任何一个服务治理措施，它的目的基本上都可以归纳为保障系统高可用，提升用户体验。这基本上是通用话术了，你要是想不起来类似的问题怎么回答，就用这个万能回答。

你在回答的时候要引入自己的案例，并且最好是能够和自己的项目亮点关联在一起的案例。
前置知识：

服务降级的目的是在系统过载或故障的情况下，通过有计划地降低或关闭非核心功能，保障核心功能的稳定性和可用性，保障用户体验。

具体来说，服务降级的核心作用有两个：
简单来说，就是应对过载和系统故障，保障系统可用性和用户体验。

一方面来说，当系统面临高并发、资源不足或依赖服务故障等压力时，我们可以通过降级非核心功能或降低服务质量，来确保核心功能的可用性，维持系统的基本运行。比如，电商网站在大促期间可能会关闭商品评论功能，确保订单流程的顺畅。

另一方面，服务降级还能提升用户体验。虽然可能会导致部分服务质量下降，但合理的降级策略可以避免系统长时间无响应或崩溃，提升用户的整体体验。比如，视频网站在网络状况不佳时会自动降低视频清晰度，确保播放流畅。
在实践中，我就用降级策略来保障系统的可用性，提升用户体验。

比如说我曾经维护过一个系统，原本在用户执行敏感操作的时候，需要通过短信来发送验证码，但是当时短信服务已经崩溃了。

在这种情况下，就转为通过邮件系统来发送验证码。

------------------------------

381. 降级有哪些实施方式?, page url: https://www.mianshi.icu/question/detail?id=894
381-894-答案：
简单题，校招和社招都会遇到。

同样的，在这种可以结合实践的题目下，一定要结合实践来谈自己使用过的案例，从而引导话题，并且刷出亮点。
服务降级的实施方式主要有以下几种（后面有简化版，所以你可以考虑跳过这部分）：



在实践中，这些方式有些情况下会组合在一起使用，构建一些非常复杂的降级方案。

如果上面的内容还是过于复杂，你可以使用这个简化版：
服务降级的实施方式主要有以下几种：

首先是通过配置中心动态调整。我们可以把降级策略配置在像 Nacos、Apollo、Etcd 这样的配置中心中，应用程序在运行时动态地读取这些配置。当需要进行服务降级时，可以通过修改配置中心的参数，动态地开启或关闭降级功能，或者调整降级参数，而不需要重启应用。

其次，是使用专门的降级框架来实现服务降级。这些框架通常提供了熔断、限流、隔离等功能。比如鼎鼎有名的 Hystrix 和 Sentinel。
在实践中，一些通用的降级方案我都是通过 Hystrix 等框架来接入的。

不过有一些业务强相关的降级策略，就是只能自己手写代码了。比如说我就使用过一个短信服务降级为邮件服务的降级策略。

原本在用户执行敏感操作的时候，需要通过短信来发送验证码，但是当时短信服务已经崩溃了。

------------------------------

382. 什么情况下会触发服务降级?, page url: https://www.mianshi.icu/question/detail?id=895
382-895-答案：
简单题，在校招和社招中都有可能遇到。

你在这个题目下，可以深入讨论触发服务降级和触发熔断、限流、降级都是一回事：也就是我怎么知道服务是否还健康。
触发服务降级可以分为被动触发和主动触发两种方式：



本质上来说，这个问题和：“什么情况下会触发限流”，“什么情况下会触发熔断” 是同一个问题。也就是如果按照故障检测、故障处理和故障恢复三个步骤来说，它们都问的是同一个问题：故障检测。
服务降级的触发方式主要分为被动触发和主动触发两大类。

被动触发，亦称自动化降级或自适应降级，是指系统依据预设规则或实时监控数据自动启动降级机制。通常在两种情境下触发：

一是系统过载，如流量激增，如秒杀活动导致的突增。或资源耗尽，如CPU、内存、磁盘空间、数据库连接池、线程池、网络带宽等资源不足或达瓶。
不管是手动降级或者是自动降级，我都有丰富的经验。相比之下，我用自动化的降级策略更多。

比如说我使用过一个短信服务降级为邮件服务的降级策略。

原本在用户执行敏感操作的时候，需要通过短信来发送验证码，但是当时短信服务已经崩溃了。
本质上，这个问题触及了服务治理和负载均衡的核心，它与我们探讨的“什么情况下会触发限流”、“什么情况下会触发熔断”等问题有着紧密的联系。这三者共同构成了分布式系统中故障处理的重要环节。

如果我们从故障检测、故障处理和故障恢复这三个步骤来分析，会发现它们实际上都在问一个根本性的问题：如何有效地进行故障检测。进一步结合负载均衡，就会发现它们都是试图回答一个问题：如何知道某个服务现在是否健康，是否能够承担更多的请求负载。

------------------------------

383. 如何实现服务降级？可以使用哪些工具或框架？, page url: https://www.mianshi.icu/question/detail?id=897
383-897-答案：
简单题，校招和社招都会遇到。

同样的，在这种可以结合实践的题目下，一定要结合实践来谈自己使用过的案例，从而引导话题，并且刷出亮点。因为问题特别强调了工具，所以在案例选择上最好是结合了某个工具或者开源框架的案例。
降级有哪些实施方式?的另一种问法，而这个问题则是进一步问了有什么工具和框架可以使用。
服务降级的实施方式主要有以下几种：

首先是通过配置中心动态调整。我们可以把降级策略配置在像 Nacos、Apollo、Etcd 这样的配置中心中，应用程序在运行时动态地读取这些配置。当需要进行服务降级时，可以通过修改配置中心的参数，动态地开启或关闭降级功能，或者调整降级参数，而不需要重启应用。

其次，是使用专门的降级框架来实现服务降级。这些框架通常提供了熔断、限流、隔离等功能。比如鼎鼎有名的 Hystrix 和 Sentinel。
在我的工作经历中，使用过 Hystrix，也使用过 Sentinel。

------------------------------

384. 降级实现过程中可能遇到的问题有哪些?, page url: https://www.mianshi.icu/question/detail?id=898
384-898-答案：
略难的题，因为这个东西你要是没有真的设计过降级策略，你是没有感觉的。也因此，在校招中很少遇到，大部分都是面微服务架构经验丰富的人才会有这种题目。

在这个问题之下，你可以举一个具体的例子来说明自己遇到的问题，以及对应的解决方案，从而进一步证明自己确实非常有落地降级方案的经验。
在服务降级实现过程中，可能会遇到以下问题：
在服务降级的实现过程中，可能会遇到以下问题：

首先，难以确定降级策略。比如，如何识别核心服务和非核心服务、如何选择合适的降级指标和阈值、如何设计合理的降级预案等。

其次，难以实现自动化降级。比如：如何实时监控系统状态、如何自动触发降级、如何减少降级策略的误判、如何快速恢复被降级的服务、人工干预的难度也是需要考虑的。
我自己就设计过非常多的降级策略，对我来说最大的问题还是业务特征导致降级策略要复杂多变。

举个例子来说，设计过一个短信降级到邮件的降级策略。也就是说，如果要是短信发布出去，可以考虑转为邮件发送。

这个看起来很简单，但是在实践中有很多要考虑的点。比如说不是所有的短信都能降级的。比如说类似于验证码你可以说降级到邮件，然后前端告诉用户去邮件找验证码，这个是可以的。但是营销类的短信，就不能转邮件，因为我们国家的用户是没有看邮件的习惯的。
设计降级方案中的这些难点，也反应到了降级有关的中间件和开源软件上。

最为典型的例子是，已有的微服务有关的开源软件，包括微服务网关，几乎没有设计通用的跨服务降级方案。也就是说，它们并不会提供当 A 服务触发降级之后，将 B 服务降级了的治理方案。

因为在设计这一类的解决方案的时候，有一个很大的问题：需要用户配合，才能知道某个服务比另外一个服务更加重要，或者说比另外一个服务更加核心。更进一步，还要考虑服务触发降级的条件，两者一结合，就非常复杂。

------------------------------

385. 服务降级会对系统造成什么影响?, page url: https://www.mianshi.icu/question/detail?id=899
385-899-答案：
略难的题，它也有一点反直觉。也就是说大部分时候我们会认为说降级似乎是一个很好的东西，因为大部分的高可用微服务，都有类似的策略

这一类的有什么影响的问题，一般都是偏重于负面影响，所以对于正面影响你可以一带而过，但是要深入讨论负面影响以及对应的规避方案，从而赢得竞争优势。
服务降级虽然可以保障系统的稳定性和核心服务的可用性，但不可避免地会对系统造成一定的影响，这些影响需要在设计和实施降级策略时仔细考虑和权衡：
首先，服务降级有正面的影响。它可以保障核心业务的可用性，避免系统在高负载或部分功能故障时崩溃，防止出现雪崩效应。同时，通过优化资源的使用，确保关键功能的正常运行，还能提升用户对核心功能的体验。

然而，服务降级也会带来一些负面影响。用户体验可能会下降，因为降级可能导致部分功能无法使用，影响用户的信任度，甚至损害品牌形象。比如，社交平台在高峰期降级了图片上传功能，用户就无法分享图片，这会让用户感到不便。

另外，降级可能引发数据一致性的问题，需要后续处理。举个例子，订单系统降级了部分校验功能，可能导致部分订单状态异常，我们需要在事后进行人工审核，这增加了工作量。
因此，为了最大限度地发挥服务降级的正面作用，同时降低其负面影响，我们需要采取一些措施。

首先，精心设计降级策略。根据具体的业务场景和系统架构，选择合适的降级方案，明确哪些功能可以降级，降级的方式和条件，以及降级的优先级。

其次，完善监控和告警机制。我们需要实时监控系统的性能和降级状态，及时发现和处理问题，确保系统稳定运行。

------------------------------

386. 服务降级适用于哪些场景？, page url: https://www.mianshi.icu/question/detail?id=900
386-900-答案：
简单题，一般在社招中能遇到，校招比较少遇到，因为校招没实践经验。

这种和实践有关的题目，刷亮点必然是结合自己的实践经验，讲述一两个有亮点的案例，从而赢得竞争优势。到这里你其实差不多能够发现，所有服务治理类的八股文，刷亮点很大程度上，都是靠自己的实践案例达成的。
前置知识：

其实一句话就能说清楚，需要保障服务高可用，并且能接受有损服务的使用场景。具体来说，可以是这些场景：
从原则上来说，降级适用于任何需要保障高可用，同时又能接受有损服务的场景。具体来说有三个比较典型的场景。

首先，高并发业务，服务降级非常关键。比如在“双十一”“618”等大型促销活动期间，用户访问量会骤增，可能导致系统过载。为了保障核心业务的正常运行，我们可以对非核心功能进行降级处理，例如暂时关闭商品评论、个性化推荐、积分兑换等，减少系统负载，提高核心功能的响应速度。

其次，在依赖服务故障的情况下，服务降级也非常重要。例如，支付服务不可用时，引导用户选择货到付款，或暂时保存订单，待支付服务恢复后再完成交易。在微服务架构中，如果某个非核心微服务出现异常，为防止故障蔓延，我们可以对该服务进行降级处理。例如，订单统计服务故障时，暂时停止实时统计，改为离线批处理，确保下单和支付等核心流程不受影响。
在实践中，基本上核心业务和核心接口，都会考虑接入服务降级措施。

比如说我就在维护短信服务的时候，就大量应用了降级措施，因为短信服务是整个系统中的一个关键服务。

举个例子来说，一般敏感操作都需要使用验证码进行二次校验，所以如果验证码不可用，那么很多操作是无法进行的。

------------------------------

387. 在什么情况下需要进行服务降级？, page url: https://www.mianshi.icu/question/detail?id=901
387-901-答案：
简单题，一般在社招中能遇到，校招比较少遇到，因为校招没实践经验。

这种和实践有关的题目，刷亮点必然是结合自己的实践经验，讲述一两个有亮点的案例，从而赢得竞争优势。到这里你其实差不多能够发现，所有服务治理类的八股文，刷亮点很大程度上，都是靠自己的实践案例达成的。

相当于服务降级适用于哪些场景？的另一种问法，答案大致相同但是需要注意衔接语言。
从原则上来说，降级适用于任何需要保障高可用，同时又能接受有损服务的场景。具体来说有三个比较典型的场景。

首先，高并发业务，服务降级非常关键。比如在“双十一”“618”等大型促销活动期间，用户访问量会骤增，可能导致系统过载。为了保障核心业务的正常运行，我们可以对非核心功能进行降级处理，例如暂时关闭商品评论、个性化推荐、积分兑换等，减少系统负载，提高核心功能的响应速度。

其次，在依赖服务故障的情况下，服务降级也非常重要。例如，支付服务不可用时，引导用户选择货到付款，或暂时保存订单，待支付服务恢复后再完成交易。在微服务架构中，如果某个非核心微服务出现异常，为防止故障蔓延，我们可以对该服务进行降级处理。例如，订单统计服务故障时，暂时停止实时统计，改为离线批处理，确保下单和支付等核心流程不受影响。
在实践中，基本上核心业务和核心接口，都会考虑接入服务降级措施。

比如说我就在维护短信服务的时候，就大量应用了降级措施，因为短信服务是整个系统中的一个关键服务。

举个例子来说，一般敏感操作都需要使用验证码进行二次校验，所以如果验证码不可用，那么很多操作是无法进行的。

------------------------------

388. 你能举例说明一个具体的服务降级场景吗？, page url: https://www.mianshi.icu/question/detail?id=903
388-903-答案：
简单题，但是要装逼还是比较难的。

在这个问题之下，你能不能赢得竞争优势，纯粹就是看你的案例好不好。而后你要小心面试官追问你案例中的细节，所以你一定要准备非常充分才可以。

而且你可以预期的是，在一场面试中，面试官肯定会问你服务治理有关的内容，所以不仅仅是降级的案例，熔断、限流的案例你也要准备一些。
你可以结合自身经历回答，或者在常见的降级策略有哪些?中选一个然后结合项目经历回答，最后实在不行就从服务降级适用于哪些场景？挑选一个。

一般来说，挑选的降级案例，要有竞争力。那么如何评判竞争力呢？

如果你实在没办法，那么可以考虑使用我这个案例，我在很多地方都提到过。这个方案就是短信服务的降级方案。
我之前负责一个短信通知平台，该平台为公司内部多个业务系统提供短信发送服务。我们集成了多个第三方短信服务提供商，以保证短信发送的可靠性。在这个过程中，我们也遇到过需要服务降级的情况，尤其是在高峰期流量压力过大，或者某个第三方短信服务提供商（假设为A）的接口出现故障，导致部分短信发送延迟或失败，影响用户体验和业务流程，例如用户注册验证码、交易提醒等。

为了应对这种情况，我们制定了一系列降级策略。首先，我们实现了短信服务提供商的自动切换。当检测到某个提供商接口不可用或响应大量超时的时候，系统会自动切换到其他可用的提供商。

其次，我们根据业务重要性进行降级。 我们将不同的业务系统进行了优先级划分。对于核心交易系统，我们优先保证其短信发送的成功率，即使成本较高。而对于一些非核心业务系统，例如营销推广短信，我们会在服务提供商故障时选择降级为异步发送，等系统恢复的时候再发送。

------------------------------

389. 你在实际项目中使用过降级吗，遇到了哪些挑战，如何克服的？, page url: https://www.mianshi.icu/question/detail?id=904
389-904-答案：
略难的题，关键在于你得提供有分量的、有特色的降级案例，并且讨论在设计这个降级策略的时候，你遇到过什么样的问题，以及如何解决的。

所以，你在回答这个问题的时候，能不能赢得竞争优势，就看你的案例了。
可以结合降级实现过程中可能遇到的问题有哪些?、服务降级会对系统造成什么影响?中的内容以及你在工作中的经历或者简历中的项目来回答。

简单来说，就是从可能遇到的问题里面挑一两个就可以了，而后要进一步讲自己是如何克服的。这里我依旧沿用短信平台的回答。

在降级实现过程中可能遇到的问题有哪些?中我使用的案例是同一个服务甚至于同一个方法，因为业务方的使用场景不同，导致设计的降级策略要非常复杂和非常灵活，这体现的是难以设计一个通用的、合适的降级策略。
我在设计降级策略的时候，遇到比较多的问题是业务多样性导致降级策略也要跟着多样。

举个例子来说，我设计过一个短信降级到邮件的降级策略。也就是说，如果要是短信发布出去，可以考虑转为邮件发送。

这个看起来很简单，但是在实践中有很多要考虑的点，比如说不是所有的短信都能降级的。举例来说，类似于验证码可以说降级到邮件，然后前端告诉用户去邮件找验证码，这个是可以的。但是营销类的短信，就不能转邮件，因为我们国家的用户是没有看邮件的习惯的。

------------------------------

390. 服务降级如何影响用户体验？, page url: https://www.mianshi.icu/question/detail?id=905
390-905-答案：
简单题。校招和社招都有可能出现，但是社招更多一些，因为社招在设计降级策略的时候，就是要考虑用户体验的。

对用户体验的影响是既有积极的一面，也有消极的一面。和服务完全正常相比，有损服务会让用户体验下降；但是和服务彻底崩溃不可用相比，降级能保证用户功能基本可用。
服务降级对用户体验的影响是双面的，它既可以保障核心功能的可用性，提升整体用户体验，又可能在某些情况下导致部分用户体验下降。

简单来说，就是和服务正常的时候比起来，用户体验是下降的；但是和服务彻底崩溃完全不可用比起来，用户体验是有一个基本保障的

这取决于降级策略的设计和执行。
简单来说，降级就是比上不足，比下有余。也就是说，和服务正常的时候比起来，用户体验是下降的；但是和服务彻底崩溃完全不可用比起来，用户体验是有一个基本保障的。

从积极的角度来看，服务降级能够有效避免系统完全崩溃，保证核心功能的可用性，尽可能减少对用户的影响。比如：商网站举例，在高峰期降低商品图片的质量或关闭个性化推荐功能，虽然会影响浏览体验，但这总比网站完全崩溃要好得多。

从消极的角度来看，降级不可避免地会导致某些功能不可用或者性能下降，这肯定会影响用户的操作体验。
这些负面影响是可以通过一些手段来减弱或者避免的。

首先，要有合理的降级策略。这需要根据业务的重要性和资源状况来制定，优先保证核心功能的可用性，尽量减少对用户的影响。

降级的实现应该尽量避免给用户带来突兀的感觉，使用友好的提示、替代页面等方式，在降级功能的同时尽量提升用户体验。例如，使用简单的静态页面替换复杂的动态页面，或者使用预先准备好的默认图片。一个好的降级方案，应该是用户没有多少感知的方案。

------------------------------

391. 如何评估降级的有效性？, page url: https://www.mianshi.icu/question/detail?id=906
391-906-答案：
略难的题，校招几乎不会遇到，社招深入讨论服务治理了才会遇到。

在实践中，很多人搞服务治理有一个毛病，就是只接入而不评估效果的。在这个问题之下，你最好的策略就是举例子说明自己是如何评估自己的降级策略的效果的。
评估服务降级的潜在影响需要一个多维度的评估体系，不能仅仅关注技术层面，还需要考虑业务和用户体验方面。 评估方法包含以下几个方面：


怎么理解上面的四个点呢？首先评估影响肯定是业务和技术两个角度评估，注意这个时候的业务影响强调的是公司的影响，那自然用户体验也是不能少的，所以总共就是三个角度来评估：业务、技术和用户体验。

而后为了全方面评估，一般都是三个方面都要评估一遍。
评估服务降级的潜在影响实际上需要一个多维度的评估体系。一般来说要从业务、技术和用户体验三个角度来评估。

首先，评估业务影响。比如说评估降级对关键指标的影响，对于电商平台，我们可能需要关注订单转化率、客单价、用户留存率等；而对于社交平台，可能更关注用户活跃度、信息发布量和用户互动率。也可以分析降级对业务流程的影响。也就是说，降级了哪些功能后，用户操作流程会发生怎样变化，这些变化是否会影响用户完成核心任务。

然后，评估技术影响。比如，降级后系统的资源利用率、响应时间，吞吐量等指标会发生怎样的变化，是否仍能满足核心业务的需求；降级后，系统的可靠性和可用性是否会下降，是否可能出现新的故障点或者风险，这些都需要稳定性测试来验证；还有可恢复性方面，我们得评估降级策略的恢复机制是否有效可靠。
举个例子来说，我当时设计过一个短信发送验证码失败，就转为用邮箱发送验证码的降级策略。

但是为了评估这个策略对用户体验的影响，我们搞过一次用户调研。当然结果只能说差强人意。用户普遍表示更加喜欢使用短信来接收验证码，但是也能接收使用邮件来接收，只不过认为这种方式比较麻烦，毕竟大部分的用户没有使用邮箱的习惯。

------------------------------

392. 降级方案的设计原则有哪些？, page url: https://www.mianshi.icu/question/detail?id=907
392-907-答案：
简单题，这个问题其实没有标准答案。一般在社招会遇到，毕竟校招没什么实践经验，总结不出来什么原则。

可能出乎你的预料，很多有关原则类的，其实都没有什么标准答案。所以实际上你根据自己的理解言之有理，那么面试官就不会和你计较。
设计有效的降级方案需要遵循一系列原则，才能在保障核心业务稳定性的同时，最大限度地减少对用户体验的影响。主要有以下原则：
在设计服务降级方案时，我们首先要保护核心业务，即使需要牺牲一些非核心功能。在系统设计初期，我们就明确核心功能的定义，确保任何降级策略都不会影响核心业务的正常运转。

同时，降级行为应具备可控性和灵活性，比如可以根据需要手动开启或关闭降级开关，或根据系统负载动态调整降级力度，以应对不同情况。

降级方案必须快速生效，平滑过渡。一方面要避免故障长时间蔓延，一方面也要避免用户体验突兀。

------------------------------

393. 如何设计合理的降级策略?, page url: https://www.mianshi.icu/question/detail?id=908
393-908-答案：
略难的题，校招不太会遇到，社招有可能遇到，但是只有在很深入讨论微服务的时候才会遇到。

如何设计 XXX 策略这种题目，你都可以结合自己的实际案例来作为证据，引导话题并且赢得竞争优势。
设计合理的降级策略可以遵循一下步骤：
设计合理的降级策略需要遵循一些重要的步骤，以确保在保护核心业务的同时，尽量减少对用户体验的影响。

首先得明确区分核心功能和非核心功能。核心功能是那些对业务至关重要的，即使系统出现问题时也必须保证它们的可用性。非核心功能就可以根据具体情况进行降级或者关闭。

接着，我们需要根据核心功能和非核心功能的划分来制定相应的降级策略。降级策略要根据业务特征，因地制宜制定。这个步骤就是最为复杂，降级的效果最终好不好，差不多就是取决于这个步骤。
在实践中，我可以说是设计过非常多的降级策略。

比如说我在设计短信平台的降级策略的时候，就经历过好几次的迭代。最开始的时候，我的设计仅仅是短信服务如果负载很高，就将请求转异步，后续慢慢处理。

而后在接入了新业务之后，意识到有些业务是可以接受降级的时候，使用邮件来替代短信。在这个阶段，我就设计了统一的根据业务来执行不同降级策略的机制。

------------------------------

394. 选择降级方案时需要考虑哪些因素？, page url: https://www.mianshi.icu/question/detail?id=909
394-909-答案：
略难的题，因为这个题目要求有设计降级方案的经验。

当然要说非常简单也可以，因为几乎稍微动脑一想，就可以说出来两三个点，只要言之有理，面试官就不会把你怎么样，毕竟这也是一个没有标准答案的问题。
选择降级方案时需要综合考虑以下因素：
选择降级方案时，需要综合考虑以下几个因素：

首先是业务的重要性。这是最核心的因素，需要结合业务目标和用户需求进行分析。在这个的基础上就可以对降级方案进行分析，总结优劣。

其次要考虑技术因素，有些方案可能需要较大的改动，而有些则比较简单易行，我们应尽量选择那些技术上可行且实施难度较低的方案。

------------------------------

395. 你刚刚提到服务降级，那么对应的服务恢复策略是什么？, page url: https://www.mianshi.icu/question/detail?id=911
395-911-答案：
简单题。一般来说这个问题出现在你提到了自己使用的降级策略，而后面试官问你怎么从降级中恢复过来。

你基本上可以认为，不管是降级，还是熔断、限流，恢复策略都是监控系统健康状态，立刻（或者一段时间之后）发送试探性的流量。如果成功了就加大流量，如果不成功就减小流量。

总结就是自动化灰度恢复过程。
服务降级的恢复策略，需要根据具体的降级方案和系统状态来决定，并没有一个通用的“一键恢复”方法。

但是最核心的目标一定是安全、平稳地将服务恢复到正常状态，避免因为恢复过程不当而引发新的问题。
最佳的恢复策略可以用一句话来形容：自动化灰度恢复。

自动化恢复是指减少人手工干预，依赖降级过程自己恢复。例如说可以结合监控指标的阈值设置自动化恢复脚本，当指标达到预设标准时，自动触发恢复流程。
我惯常用的就是监控服务的性能，而后在发现服务恢复正常之后，试探性正常处理部分流量。如果成功了，就加大流量；而如果失败了，就减少流量。

举个例子来说，我在短信里面设计过一个降级为邮件发送的策略。在触发降级之后，绝大部分短信是直接转为邮件发送。

------------------------------

396. 如果降级恢复失败，应该怎么办？, page url: https://www.mianshi.icu/question/detail?id=912
396-912-答案：
略难的题，因为很少有面试官会这么问，一般出现在社招中。

正常来说，大部分的服务治理措施，都会有一个类似开关的东西，允许你关掉整个机制。所以在恢复失败的情况下，最佳策略就是直接禁用掉整个降级机制。
如果降级恢复失败，最直接的做法就是直接关闭整个降级机制。正常来说可以给降级一个开关，只要关上这个开关，降级机制就不会再生效。
如果降级恢复失败，最直接的做法就是直接关闭整个降级机制。正常来说可以给降级一个开关，只要关上这个开关，降级机制就不会再生效。

------------------------------

397. 限流和降级有什么区别和联系？, page url: https://www.mianshi.icu/question/detail?id=913
397-913-答案：
简单题，在校招和社招中都有可能遇到。

在这个问题之下，你可以站在故障三步曲的角度，深入讨论熔断、限流和降级的关系。从本质上上来说，它们其实没多少区别。
前置知识：

限流和降级是保证系统稳定性和可用性的常用策略，它们在应对系统过载和故障方面有着不同的侧重点。

两者的关键区别有三个：触发条件不同、侧重点不同、恢复机制也不同。
限流和降级是确保系统稳定性和可用性的常用策略，它们在处理系统过载和故障时有着不同的侧重点。

首先，触发条件不同。在限流的情况下，触发条件通常是当系统资源接近饱和或已经达到预设的阈值，比如请求数量超出了系统每秒的处理能力。而降级的触发条件则更为广泛，它不仅包括系统资源不足、服务故障和请求量过大，还可能是基于一些预设策略的判断。

在侧重点方面，限流更多的是集中在控制请求流量，目的是避免系统因请求过多而被压垮。通过限制流量，我们能确保系统的稳定性，使其能够持续提供服务。而降级则更关注于保证核心服务的可用性。当系统部分功能不可用时，我们会通过牺牲一些非核心功能或降低服务质量，从而保护核心功能的正常运行。
如果我们站在故障检测、故障处理和故障恢复的角度来看这个三者，那么可以发现熔断、限流和降级它们之间没有本质区别，都可以看做是在服务出现问题或者快要出问题的时候，采取的不同的治理措施。

------------------------------

398. 限流、熔断、降级三者之间的区别和联系？, page url: https://www.mianshi.icu/question/detail?id=914
398-914-答案：
简单题，在校招和社招中都有可能遇到。

你可以站在故障三步曲的角度，深入讨论三者的区别的联系。从本质上来说，三者并没多少区别。
前置知识：

限流、熔断和降级是保证系统稳定性和高可用性的常用策略，它们在应对系统过载和故障方面各有侧重。

三者的关键区别有三个：触发条件不同、侧重点不同、恢复机制也不同。
限流、熔断和降级是确保系统稳定性和可用性的常用策略，它们在处理系统过载和故障时有着不同的侧重点。

首先，触发条件不同。限流的触发主要是在系统资源接近饱和，或者已经达到了预设的阈值，比如请求数量超出了系统每秒的处理能力。熔断则是在下游服务出现故障时触发，比如高错误率、高延迟或者超时，这都能反映那个服务的健康状况不佳。而降级的触发条件则更为广泛，可能包括系统资源不足、服务故障、请求量过大，甚至是基于一些预设策略的判断。

在侧重点上，限流主要是为了控制请求流量，避免系统因请求过多而被压垮。它的目的是通过限制流量来保证系统的稳定性，确保系统能够持续提供服务。熔断注重的是快速失败并隔离故障，阻止服务的故障蔓延，以保护系统整体的稳定性。熔断机制能够在检测到下游服务的错误率高或者响应时间异常时，自动“熔断”对该服务的调用。而降级则集中在确保核心服务的可用性，当系统的一部分功能不可用时，我们会通过牺牲非核心功能或降低服务质量来保护核心功能的正常运作。
如果我们站在故障检测、故障处理和故障恢复的角度来看这个三者，那么可以发现熔断、限流和降级它们之间没有本质区别，都可以看做是在服务出现问题或者快要出问题的时候，采取的不同的治理措施。

------------------------------

399. 为什么要限流，加机器不行吗？, page url: https://www.mianshi.icu/question/detail?id=915
399-915-答案：
简单题。在讨论限流的时候，很可能遇到。

其实有一个原因你肯定能想到，那就是突发流量你怎么加机器都没用，毕竟一些攻击者能搞出一些天量的流量。
前置知识：
虽然增加机器确实可以提高系统的处理能力，但这并不是解决系统过载问题的银弹，甚至在某些情况下，单纯增加机器可能会适得其反。因此，限流仍然是一个非常必要的策略。

第一，突发流量往往难以预测，我们可能无法准确计算需要增加多少机器来应对。增加机器的过程需要时间，而在流量迅速上升时，往往来不及应对，此时限流机制能够迅速生效，立即对流量进行控制，避免系统被突发流量压垮。

第二，增加机器会带来直接的成本问题。无论是硬件还是运维的开支，这些都是不可忽视的，特别是如果只是为了应对短暂的流量高峰，增加机器可能并不划算。限流作为一种更经济高效的方式，能够快速灵活地应对这些突发流量，而无需承担额外的硬件费用。

------------------------------

400. 微服务架构下的服务降级策略有哪些？, page url: https://www.mianshi.icu/question/detail?id=916
400-916-答案：
简单题，你可以罗列你用过的、了解过的所有降级策略。

在这个问题之下，你可以结合实践，阐述自己用的降级策略，引导话题并且赢得竞争优势。
前置知识：
从大方向上来说，降级有三种手段：减少资源损耗、同步转异步、采用备用方案。

而具体的策略则有非常多。

最常见的是采用预设数据。也就是在实时计算数据有问题的情况下，使用提前准备好的数据。举个例子，在首页个性化推荐功能有问题的时候，使用运营团队预先配置好的推荐内容。
在实践中我用过很多降级策略来保护我的系统，保证用户体验。

比如说在允许异步处理请求的场景下，我就大量使用同步转异步的手段。典型的例子就是在短信服务中，当短信服务本身已经到性能瓶颈的时候，新来的发短信的请求就会转发到消息队列上，等后续再慢慢处理。

------------------------------

401. 什么是微服务路由？, page url: https://www.mianshi.icu/question/detail?id=917
401-917-答案：
简单题，在校招中有可能遇到，但是在社招中更加有可能遇到。

在面试中，你可以结合自己的实践来讨论自己使用过的路由策略，引导话题，刷出亮点。
微服务路由是微服务架构中至关重要的一个组件，它负责将客户端的请求定向到正确的微服务实例。可以将其视为一个智能的交通调度系统，依据不同的目的地（服务）和路况（服务实例的健康状况、负载情况等）将请求引导到最合适的路径（服务实例）。

微服务路由的核心功能包括：

整个核心机制你可以看这个图：
微服务路由在微服务架构中是一个至关重要的组件，主要负责把客户端的请求定向到合适的微服务实例。可以把它想象成一个智能的交通调度系统，依据不同的目的地和路况，比如服务的健康状况和负载情况，将请求引导到最合适的服务实例。

微服务路由的核心功能有几个方面。首先是服务发现，路由组件需要实时了解所有可用的微服务实例及其地址，这个过程就是普通的服务发现过程。

接下来是请求路由。当路由组件收到请求后，需要根据请求的信息，比如URL、请求头、请求参数等，加上预定义的路由规则，来筛选出可用的微服务实例来处理这些请求。这些路由规则其实是非常灵活的，可以基于域名、路径、请求内容，甚至自定义的header来进行路由。
我自己使用过很多路由策略。

最经常使用的就是标签路由。简单来说，就是服务端节点上会有一些标签，而后客户端在发送请求的时候根据这些标签来筛选。比如说在 VIP 系统中，部分高性能节点被标注了 VIP 标签，那么 VIP 请求就会优先落到这些节点上。

------------------------------

402. 为什么需要微服务路由？, page url: https://www.mianshi.icu/question/detail?id=918
402-918-答案：
简单题。

类似于为什么需要 XXX 之类的问题，有一个万能的回答：业务需要。当然，你在回答的时候要举出具体的业务场景，证明你们需要这个东西。所以在这个问题之下，你可以使用同样的思路，使用一些比较高端的案例，从而引导话题，并且赢得竞争优势。
前置知识：

使用微服务路由的原因很简单：业务需要复杂的路由规则。

比如说业务需要搞 AB 测试，那么引入微服务路由是最快的落地方式；又比如说业务上要根据业务特征来筛选目标节点，最直观的就是 VIP 用户的请求要发送到特定的节点上，这种也是要依赖微服务路由来完成的。
选择微服务路由的核心原因是业务对复杂路由规则的依赖。例如，进行AB测试时，微服务路由能迅速实现；针对特定业务特征，如将VIP用户请求定向到特定节点，也需依赖微服务路由。
我在实践中就用过微服务路由来解决一些具体的问题。

比如说我的业务参与过 AB 测试，那么在流量分发上，是借助微服务路由来完成的。类似的，也参与过全链路压测的落地，压测流量分发同样是借助微服务路由来完成的。

------------------------------

403. 微服务路由的目的和作用是什么？, page url: https://www.mianshi.icu/question/detail?id=919
403-919-答案：
简单题，在校招和社招中都有可能遇到。
前置知识：
微服务路由的核心目的是高效、可靠地将客户端请求定向到正确的微服务实例，确保微服务架构的整体稳定性和高可用性。

具体来说，路由组件在多个关键方面发挥着重要作用。

首先，它实现了负载均衡，将请求均匀分发到多个服务实例，避免单点过载，显著提升系统吞吐量和性能；

------------------------------

404. 常见的微服务路由策略有哪些？请举例说明并比较它们的优缺点。, page url: https://www.mianshi.icu/question/detail?id=920
404-920-答案：
简单题，在社招中比较有可能遇到。

同样地，这种题目你就可以大谈特谈你的实践案例，从而引导话题，刷出亮点。
前置知识：

常见的微服务路由策略有很多，选择哪种策略取决于具体的应用场景和需求。以下列举几种常见的策略，并比较它们的优缺点及适用场景：
在实践中，有很多的路由策略，归根节点就是看自己的需要来。

首先是基于标签的路由。这是根据服务实例的标签进行路由，比如服务实例被打上 VIP 标签，就代表着节点可以服务于 VIP 用户的请求。这个策略灵活性不错，方便进行环境隔离和功能特性隔离，但需要服务注册中心支持标签的功能。

第二是基于内容的路由，它是通过请求的正文内容来判断路由，比如根据请求里面的用户信息判定是不是 VIP，如果是 VIP 就需要将请求发送到特定节点上 。这种方法非常灵活，可以实现复杂的路由逻辑。但实现起来比较复杂，而且性能开销也较大，因为需要解析请求正文。通常这种策略会和其他路由策略结合使用，适合需要复杂路由逻辑的场景。
我自己使用过很多种路由策略，最经常用的就是标签路由。

------------------------------

405. 微服务中，什么是客户端路由？, page url: https://www.mianshi.icu/question/detail?id=921
405-921-答案：
简单题，一般出现在社招里面，因为校招没有实践经验，所以不太会问。

在这个问题之下，一方面你可以谈及你使用过的客户端路由策略，一方面你也可以深入讨论环节客户端路由的弊端，从而赢得竞争优势。
前置知识：

在这里提到了客户端路由的缺陷：

其中可维护性差和难以扩展是可以通过监听机制来解决的。也就是说，每一个客户端都监听自己要使用的服务的节点变化情况，而后实时更新自己缓存的状态。例如说动态监听节点的上线、下线信息，又或者是监听分组、权重变化信息等。
在微服务架构中，客户端路由是一种由客户端直接与各个微服务实例进行通信的机制。简单来说，客户端会通过服务注册中心，比如Eureka、Consul或ZooKeeper，获取所有可用服务的实例列表以及相关的信息，例如地址、端口和健康状态等。然后，客户端会根据自己实现的负载均衡算法，比如轮询、随机或者权重，选择一个实例来发出请求。

这种路由方式的一个显著特点是，客户端必须维护所有微服务实例的地址信息，并在实例发生变化时及时更新这些信息。在这种情况下，客户端承担了服务发现、负载均衡和故障转移等功能。这意味着，它需要能够检测服务实例的故障，并自动切换到其他可用的实例。

客户端路由的优点在于实现比较简单直接，所有逻辑集中在客户端。客户端可以根据自身的业务需求选择最合适的服务实例，比如根据用户的地理位置选择离用户最近的实例。这能够潜在提高性能，因为这避免了额外的API网关请求，从而减少了网络跳数。
从实践上来说，我用得比较多的是客户端路由，尤其是客户端路由中的标签路由策略。

标签路由真的是非常灵活、非常强大。我用标签路由解决过 VIP 用户请求发送到专属节点、读写请求路由到读写节点、AB 测试等问题。

------------------------------

406. 微服务中，什么是服务端路由？, page url: https://www.mianshi.icu/question/detail?id=922
406-922-答案：
简单题，在校招中不太常见，在社招中有可能遇到。

一般来说，服务端路由就是指网关路由，所以如果你是用过微服务网关或者 API 网关之类的，就可以结合自己的实践经验来讨论路由。
前置知识：
在微服务架构中，服务端路由是一种将客户端请求路由到适当微服务实例的机制。这种路由方式由服务端组件负责，比如API网关或服务注册中心，客户端无需关心每个服务实例具体的位置，简单地将请求发送到服务端组件。

服务端路由的运作方式是这样的：所有客户端的请求会通过一个或多个中央的路由组件，这些组件维护所有微服务实例的注册信息，并根据这些信息来转发请求。路由组件通常会实现负载均衡算法，并具备健康检查功能，可以定期检查服务的状态，确保请求不会被发往故障的实例。此外，服务端路由还可能实施流量控制策略，比如限流和熔断，保障微服务在高并发情况下的稳定性。

使用服务端路由的优点之一是，它极大地简化了客户端的开发和维护。客户端不再需要知道各个服务实例的信息，只需发送请求即可。这种集中管理的方式也使得服务发现和负载均衡的逻辑都集中在服务端路由组件，便于统一管理。同时，服务实例的添加、删除和更新都由服务端组件处理，客户端代码可以保持不变。这一模式还增强了系统的整体安全性，因为路由组件可以集中处理身份验证和授权。

------------------------------

407. 微服务路由与负载均衡的区别和联系？, page url: https://www.mianshi.icu/question/detail?id=924
407-924-答案：
简单题，当你提及了你用过路由的时候，面试官就可能会问你这个问题。

一句话就能说清楚这两者的区别：微服务路由是筛选候选节点，而负载均衡则是选出最终节点。
微服务路由和负载均衡是微服务架构中两个紧密相关的概念，但它们并非同一个东西。 一句话就能说清楚两者的区别：
微服务路由和负载均衡是微服务架构中紧密相关的概念，但它们并不是完全相同的东西。

------------------------------

408. 已经有了负载均衡，为什么还需要微服务路由？, page url: https://www.mianshi.icu/question/detail?id=925
408-925-答案：
简单题，如果你的项目中用了负载均衡，又用了路由策略，那么面试官可能会提出这个疑问。

而回答这个问题也很简单，那就是微服务路由关注的是哪些节点符合条件，也就是筛选候选节点；而负载均衡则是从符合条件的候选节点中挑选出最终的一个。
前置知识：
虽然负载均衡和微服务路由在某些方面有重叠，但它们在微服务架构中扮演的角色其实是不同的。

首先，负载均衡的主要职责是将请求均匀地分发到多个实例，以提升系统的可用性和性能。它确保没有单个实例被过载，从而提升整体吞吐量和响应速度。负载均衡是一个非常重要的环节，但它只关注“如何分发”请求。

相对而言，微服务路由的功能更为广泛和复杂。微服务路由不仅涉及到将请求导向正确的服务实例，还包括服务发现、请求转发、协议转换，甚至是处理一些复杂的路由规则。微服务路由必须要知道客户端请求的目标是什么，并且能够根据业务需求动态地调整路由。

------------------------------

409. 如何选择服务路由策略？, page url: https://www.mianshi.icu/question/detail?id=927
409-927-答案：
略难的题，如果要是你没有实践经验，没有真的决策过使用什么路由，怕是不知道怎么回答。

但是其实答案很简单，就是综合考虑业务需求和技术需求，而且大多数时候，业务需求就直接决定了你的路由策略是什么样。看似可选，实际上没有什么选择。
选择服务路由策略需要综合考虑多个因素，没有放之四海而皆准的最佳方案，最佳策略取决于具体的应用场景和需求。
选择服务路由要考虑的就是两个因素。

第一个因素是业务因素。很多时候，业务就直接决定了你的路由策略可以设计成什么样子，比如说在业务上根据用户是否是 VIP 用户来进行路由。
就我个人经验来说，我认为很多时候路由策略是没什么可选的，也就是业务需求或者技术需求就直接决定了你只能这么做。

------------------------------

410. 服务路由失败后如何处理？, page url: https://www.mianshi.icu/question/detail?id=928
410-928-答案：
略难的题，只会在深入讨论微服务路由或者微服务容错的时候才会问到。

类似这种如何处理的问题，最佳的装逼方式就是使用实践案例，证明自己有丰富的经验。
服务路由失败时的处理方式取决于多种因素，包括失败的原因、系统的容错能力还有业务需求。为了有效应对这些问题，一个完善的处理策略应该包含以下几个方面：

示例：
这实际上是一个发现问题、定位问题和解决问题的事情。

首先就是要做好监控和告警，这样能够快速发现问题、定位问题。

紧接着就是最为重要的解决问题了。当服务路由失败的时候，能够采取的措施有很多。
举个例子来说，我就设计过一个针对 VIP 用户的路由策略。

------------------------------

411. 如何在微服务中实现动态路由？, page url: https://www.mianshi.icu/question/detail?id=929
411-929-答案：
略难的题，这需要你有一定的实践经验才能知道。

在这个问题之下，你可以讨论一些借助注册中心实现的动态路由策略，很容易就给注册中心带来巨大的压力。当然，这会把话题引导过去服务注册与发现，以及注册中心选型上。
在微服务架构中实现动态路由，需要一个系统能够灵活地根据各种因素实时调整请求的路由路径。 这通常涉及到服务发现、配置中心和负载均衡等多个组件的协同工作。 以下是几种常见的实现方式：
在微服务架构中实现动态路由，确实需要一个系统能够灵活地根据各种因素实时调整请求的路由路径。这通常涉及到服务发现、配置中心和负载均衡等多个组件的协同工作。下面我可以介绍几个常见的实现方式。

首先是基于配置中心的动态路由。简单来说就是将路由规则写入配置中心，路由中心监听配置变更，执行路由。优点是简单好用，缺点则是需要人手工修改配置。如果对舒适性的要求不高，那么可以优先考虑使用这个路由策略。

接下来是基于服务发现的动态路由。这种方式利用服务注册中心，比如Eureka、Consul或者ZooKeeper，来实时维护服务实例的注册信息，负载均衡器或API网关会根据这些信息进行路由。这种策略自动适应实例变化，减少人手工干预，适合健康状态路由和动态实例场景。
但是如果借助服务注册中心来实现动态路由，那么会面临一个问题：注册中心可能成为性能瓶颈。

------------------------------

412. 路由期间，目标服务节点发生故障，该如何处理？, page url: https://www.mianshi.icu/question/detail?id=931
412-931-答案：
略难的题，它也是一个容错的问题。

本质上来说，它跟负载均衡如何处理故障节点是同一个问题，所以用的策略也是类似的。在这个问题之下，你可以结合自己实践中的例子，引导话题，并且刷出亮点。
目标服务节点发生故障，首要问题就是要检测到，其次就是处理：
在微服务架构中，面对目标服务节点故障，首要任务是保证能够及时发现。这可以通过通过心跳机制和监控系统来发现，如未收到心跳或监控指标异常时发出告警。

而在发现之后，则要快速启动故障处理，有很多种处理策略。

最直接的就是重试，如果节点是偶发性的故障，那么重试很有可能成功。更进一步地，重试可以和故障转移结合在一起，也就是在重试的时候结合负载均衡器将流量导向健康节点。
我在实践中比较多的是使用故障转移措施，也就是当节点在出现问题之后，选择另外一个健康节点来重试。而后这个失败节点会被标记为不可用。

------------------------------

413. 在你在实际项目中，遇到了哪些微服务路由相关的问题，如何解决的？, page url: https://www.mianshi.icu/question/detail?id=932
413-932-答案：
略难的题，你需要有实践经验才能回答好。

你可以在面试之前，准备一个路由有关的问题排查案例，在面试中使用可以起到引导话题，刷亮点的效果。
在微服务架构的实际项目实施中，路由问题是不可避免的挑战。以下是几个可能遇到的典型路由问题，你可以选取两三个回答。
之前我们就遇到过很多问题。

比如，我们曾遇到路由引入额外性能开销的问题，特别是复杂的路由策略会显著增加性能负担。为此，我们优化了路由逻辑的性能，并在必要时换用了简单的高性能路由策略，有效降低了性能开销。

另一个突出的问题是路由规则复杂化导致配置维护困难。随着微服务数量和业务逻辑的增加，路由规则变得日益复杂，维护成本急剧上升，每次修改都需要重新部署应用，导致配置管理混乱且易出错。这个问题其实很难解决，只能是通过引入配置中心，规范变更流程来在一定程度上缓解。

------------------------------

414. 微服务中，什么是客户端路由和服务端路由？它们之间有什么区别？, page url: https://www.mianshi.icu/question/detail?id=933
414-933-答案：
简单题，在校招和初级工程师面试中可能遇到。

在这个案例下，你可以结合自己的实践来输出案例，引导话题，刷出亮点。
客户端路由和服务端路由是微服务架构中两种不同路由机制，它们的主要区别在于路由决策发生的位置：
在微服务架构中，客户端路由和服务端路由是两种不同的路由机制，区别在于路由决策的地点。
从实践上来说，我用得比较多的是客户端路由，尤其是客户端路由中的标签路由策略。

------------------------------

415. 什么是隔离？, page url: https://www.mianshi.icu/question/detail?id=934
415-934-答案：
简单题，基本概念题，在校招和初级工程师面试中有可能遇到。

在谈及隔离的时候，最好的面试方案就是联系自己的实践经历，而后提及用过的隔离策略。一方面引导话题，一方面也是刷出亮点。
在软件架构设计中，特别是微服务架构下，“隔离”指的是一种设计策略，旨在将系统中的不同部分（例如，不同的服务、模块、组件或功能）相互隔离，防止一个部分的故障或异常影响到其他部分。 其核心目标是增强系统的健壮性、容错能力和稳定性。

想象一下一艘巨轮，为了安全，它被分隔成许多独立的舱室。如果一个舱室进水，其他舱室仍然可以正常工作，不会导致整艘船沉没。 软件系统中的隔离策略就类似于这些舱室的舱壁，它们限制了故障的蔓延。
在软件架构设计中，特别是微服务架构下，“隔离”实际上是一种设计策略，目的就是把系统中的不同部分，比如服务、模块、组件或功能相互隔离。这样，某一个部分的故障或异常就不会影响到其他部分。这个策略的核心目标是增强系统的稳定性和可用性。

隔离可以体现在多个层次上。首先是物理隔离，这种方式把不同的服务部署在不同的物理服务器上，这是最彻底的隔离方式，不过成本也比较高。接着是虚拟化隔离，我们可以利用虚拟机或容器技术，在同一台物理机上创建多个隔离的环境来运行不同的服务，这样成本就比物理隔离低，但隔离程度也相对较低。

还有进程隔离，将不同的服务部署在不同的进程中，这种方法成本较低，并且隔离效果相对良好，因为一个进程崩溃不会影响到其他进程。然后是线程池隔离，虽然在同一个进程里，但可以为不同的服务分配独立的线程池，这样就可以限制每个服务的并发请求数量，并且一个服务的线程池出现问题时，也不会影响到其他服务的线程池。
我在实践中使用过非常多种隔离策略。

最常用的就是资源隔离。举个例子来说，大部分第三方资源如数据库，Redis 等，只要条件允许，我都会尝试给核心服务申请独立的资源。防止因为别的业务出现故障影响了自己的业务。

比如说我之前就遇到过因为共享 Redis 而出现过问题。当时我的一个业务和另外一个组的业务共享一个 Redis 集群，但是因为那个组出现了一个大 key 的问题，导致 Redis 的查询时间非常长，导致我们这边的接口频繁出现超时。

------------------------------

416. 为什么需要隔离？, page url: https://www.mianshi.icu/question/detail?id=935
416-935-答案：
简单题，在校招和社招中都有可能出现。

在这个问题之下，你可以使用自己实践中的隔离的例子来证明隔离的必要性，引导话题，刷出亮点。
需要隔离主要基于以下几个原因：


这里面，你只需要记住最关键的一点：提升系统稳定性和可用性。
首先，隔离可以提升系统稳定性和可用性，这是隔离最根本的目的。如果没有隔离，一个组件的故障可能会像多米诺骨牌那样，引发一系列的连锁反应，最终导致整个系统瘫痪。隔离可以有效限制故障的影响范围，即使某个组件出现问题，其他组件也能继续正常运行，保证系统的整体稳定性和可靠性。例如，在微服务架构中，一个服务的崩溃不会影响其他服务。

其次，隔离还提高了系统的可维护性和可扩展性。独立的组件可以独立开发、测试和部署，而不必担心会影响到其他组件。这样可以加快开发速度，简化测试流程，并且方便进行功能的迭代和升级。例如，使用微服务架构时，我们可以独立地扩展某个高负载的服务，而不需要重新部署整个应用。
举个例子来说，早期我接手了一个核心服务，它是和别的业务组一起共用一个 Redis 集群的。于是后面他们上线了一个功能，引发了大 key 问题，导致我这边的接口经常超时。这就是一个因为没有隔离引发故障的典型例子。

------------------------------

417. 隔离的目的和作用是什么？, page url: https://www.mianshi.icu/question/detail?id=936
417-936-答案：
简单题，在校招和社招中都有可能遇到。

在这个问题之下，还是要结合自己的实践案例来刷亮点。
隔离的目的和作用可以概括为以下几点：

主要目的： 提升系统的稳定性和可用性，防止单个组件或服务的故障影响到整个系统。
隔离的主要目的是提升系统的稳定性和可用性，防止单个组件或服务的故障影响到整个系统。

具体来说，隔离能有效降低故障影响的范围提升系统可用性。当某个组件或服务出现问题时，隔离可以限制这个故障的影响，确保其他组件或服务仍然能够正常运行，这样系统的部分可用性就得以保持避免了整个系统瘫痪的情况，从而整体提升了系统的可用性。

同时，隔离还可以增强系统的稳定性，让系统更好地应对各种异常情况，比如网络故障、硬件故障或软件bug等。这能够减少宕机和服务中断的次数，使系统更加平稳运行。
举个例子来说，早期我接手了一个核心服务，它是和别的业务组一起共用一个 Redis 集群的。于是后面他们上线了一个功能，引发了大 key 问题，导致我这边的接口经常超时。这就是一个因为没有隔离引发故障的典型例子。

------------------------------

418. 常用的隔离策略有哪些？, page url: https://www.mianshi.icu/question/detail?id=937
418-937-答案：
简单题。

在这个问题之下，装逼还是要罗列自己用过的隔离策略，而后面试官就可以对其中的细节进行追问，你就可以装逼了。
常用的隔离策略可以从不同的维度进行分类，以下列举一些常见的策略：
常用的隔离策略可以从不同的维度进行分类，以下是一些常见的策略：

隔离策略可根据不同维度分为多种类型。首先是基于部署环境的隔离，包括物理隔离（如将不同服务部署在不同物理服务器上）和虚拟机隔离（如使用VMware等虚拟机技术）。物理隔离安全性高，但成本高、资源利用率低；虚拟机隔离成本相对较低，资源利用率高，但存在性能损耗。

其次是运行环境隔离，如线程池隔离和容器隔离。线程池隔离实现简单，但隔离性较弱；容器隔离（如Docker）成本低、资源利用率高、启动速度快，但隔离性相对较弱且可能存在安全风险。
我在实践中用过很多种隔离策略。

比如说为核心服务设置单独的连接池和线程池，以及为核心业务准备高质量、更加可控的 Redis 集群，防止别的业务干扰了自己的核心业务。

------------------------------

419. 在微服务中，常见的隔离策略有哪些？请举例说明并比较它们的优缺点。, page url: https://www.mianshi.icu/question/detail?id=938
419-938-答案：
简单题。

类似这种问题，你都可以根据自己的实践经验，谈一谈自己用过的隔离措施。
从常用的隔离策略有哪些？中选择几个与简历中项目相关的或者能记得住的回答。
常见的微服务隔离策略也是有很多的。

隔离策略可根据不同维度分为多种类型。首先是基于部署环境的隔离，包括物理隔离（如将不同服务部署在不同物理服务器上）和虚拟机隔离（如使用VMware等虚拟机技术）。物理隔离安全性高，但成本高、资源利用率低；虚拟机隔离成本相对较低，资源利用率高，但存在性能损耗。

其次是运行环境隔离，如线程池隔离和容器隔离。线程池隔离实现简单，但隔离性较弱；容器隔离（如Docker）成本低、资源利用率高、启动速度快，但隔离性相对较弱且可能存在安全风险。
就我的个人经验来说，最常见的策略应该是线程池隔离和连接池隔离。大部分情况下，我们都会考虑给核心服务配置独立的线程池和连接池。这种策略施行起来非常简单，因为大部分的微服务框架都会提供类似的功能。

------------------------------

420. 选取隔离策略需要考虑哪些因素？, page url: https://www.mianshi.icu/question/detail?id=939
420-939-答案：
简单题。

类似于这种选择类的题目，基本上可以认为要考虑的因素就是业务因素与技术因素。而且正常情况下，业务因素优先级高于技术因素。

同样地，在这个问题之下你可以结合自己的实践案例来刷亮点。
选择隔离策略是一个多方面权衡的过程，通常需要综合考虑以下几个关键因素：
选择隔离策略是一个多方面权衡的过程，大体上可以分为技术因素和业务因素，包括这些点。

首先，业务需求和风险评估是基础。我们必须对业务进行全面的风险评估，明确不同业务模块的数据敏感程度、业务的重要性，以及可能面临的威胁类型，比如数据泄露、拒绝服务攻击、内部威胁等等。对于高风险和高敏感度的业务，比如金融交易系统，确实需要更严格的隔离策略，而低风险的业务可能就可以采用相对轻松一些的方案。我会根据风险等级来制定不同的隔离策略，进行定量和定性的风险评估，确保这些策略能够有效降低风险。

接下来，系统架构和技术选型也是重要的考虑因素。不同的系统架构会直接影响到隔离策略的选择。例如，在微服务架构中，我们可以运用像Docker和Kubernetes这样的容器化技术，或者使用服务网格比如Istio，实现微服务间的隔离。而如果是单体应用，可能就得考虑进程隔离或者虚拟机隔离。从数据库层面来看，我们能通过用户权限控制、数据加密和审计等手段实现数据隔离。在消息队列方面，还要考虑消息的安全性以及不同队列之间的隔离。总之，我们需要根据具体的系统架构和技术栈，选择最适合的隔离技术，确保它能与现有系统兼容。
举一个例子来说，在设计隔离策略的时候，很重要的一个点就是受制于公司已有的资源数量。
正常来说，所有类似这种策略决策要考虑的就是业务因素和技术因素。其中业务因素是前置条件，也就是所有决策首先要满足业务约束。而后是考虑技术因素，很多情况下技术因素是一个取舍问题。

------------------------------

421. 进程隔离、线程隔离和容器隔离的区别和联系？, page url: https://www.mianshi.icu/question/detail?id=941
421-941-答案：
略难的题。这个题目的难点在于你可能只知道容器隔离这个名词，但是对容器技术的了解不深。

在这个问题之下，你可以结合实践来说在什么场景下应该使用什么隔离。

前置知识：

进程隔离、线程隔离和容器隔离都是为了实现资源隔离和故障隔离的技术，但在隔离粒度、实现机制和适用场景上存在差异。

区别：
进程隔离、线程隔离和容器隔离这三种技术其实都是为了实现资源和故障的隔离，但它们在隔离的粒度、实现机制和适用场景上是有区别的。

首先，进程隔离是最彻底的隔离方式。这种方式能够提供很高的安全性，但资源消耗相对较高，因为每个进程都需要独立的资源。此外，进程间的通信效率比较低，需要通过一些像管道、消息队列或共享内存这样的IPC机制来实现。一般来说，这种隔离方式适用于需要高安全性和隔离性的场景，比如运行不受信任的代码或者防范恶意攻击的情况。

再说线程隔离，它的隔离粒度是线程级别的，多个线程共享同一个进程的资源，所以资源消耗相对较低。因为线程间能够直接共享内存空间，所以它们的通信效率也很高。不过，线程的故障隔离就比较弱了，假如有一个线程崩溃，可能会影响整个进程。这种隔离方式很适合需要轻量级隔离并且想要提高并发性能的场景，比如在多线程服务器上。
从技术选择上来说，选择哪种隔离技术其实是要看具体的应用场景和需求。

------------------------------

422. 实施隔离策略的过程中遇到哪些问题?该如何解决？, page url: https://www.mianshi.icu/question/detail?id=943
422-943-答案：
简单题，一般在社招里面会出现。

回答这个问题的最好方式，就是使用自己在实践中使用隔离策略遇到的问题来回答，但是要注意必须是一个硬技术的问题。
实施隔离策略的过程中，通常会遇到以下问题：
在实施隔离策略时，我们常遇到性能瓶颈、资源浪费和复杂性增加的问题。

首先是性能瓶颈，隔离机制如容器化会增加性能开销，尤其在高并发下影响显著。解决办法则是通过测试、可观测性数据来找出性能瓶颈，而后针对性优化。

其次则是资源浪费，例如说在使用 Redis 集群隔离的时候，需要额外准备一些 Redis 集群。这个不好解决，缓解方案是采购廉价但是性能稍微差点资源，为非核心服务使用，核心服务用昂贵但是性能好的资源。
举个例子来说，我在使用线程池隔离的时候就遇到过线上故障。

当时我们部署的服务里面，有些是核心接口，有些是非核心接口。那么为了进一步保证核心接口的服务质量，就采用了线程池隔离的技术，给不同的核心服务都配置了线程池。

但是这就带来了一个问题，单个线程池的线程数量不多，但是多个线程池的线程数量加在一起，就超过了机器能够支撑的上限，导致程序崩溃了。

------------------------------

423. 隔离策略会对系统造成什么影响?, page url: https://www.mianshi.icu/question/detail?id=944
423-944-答案：
简单题，一般在社招中出现。

回答这个问题你就还是可以进一步结合自己的实践经验，阐述对系统造成的影响，最好是使用一些和性能、可用性有关的案例。
隔离策略虽然能显著提升系统的稳定性、安全性和可用性，但也可能会对系统造成一些负面影响，主要体现在以下几个方面：
隔离策略确实能显著提高系统的稳定性、安全性和可用性，但同时也可能带来一些负面影响，我想和您分享一下这些方面。

首先，性能损耗是一个需要关注的问题。任何隔离策略都会增加一定的性能开销。比如，虚拟机和容器需要额外的虚拟化层，进程间通信会涉及上下文切换，网络隔离还需要额外的网络配置和路由，再加上资源配额的监控和管理，这些都会消耗系统资源。

除了性能之外，复杂性增加也是一个值得注意的方面。实施隔离策略需要复杂的设计和实现，大家需要考虑不同策略的组合和协调，还得设计合适的监控和管理机制，这无疑增加了开发和维护的成本和难度。当系统出现问题时，由于隔离策略的存在，找到问题的根源和调试会变得更加复杂，这就需要团队具备更强的排查能力和精细的监控体系。
举个例子来说，我在使用线程池隔离的时候就遇到过线上故障。

当时我们部署的服务里面，有些是核心接口，有些是非核心接口。那么为了进一步保证核心接口的服务质量，就采用了线程池隔离的技术，给不同的核心服务都配置了线程池。

但是这就带来了一个问题，单个线程池的线程数量不多，但是多个线程池的线程数量加在一起，就超过了机器能够支撑的上限，导致程序崩溃了。

------------------------------

424. 你在实际项目中使用过隔离策略吗，遇到了哪些挑战，如何克服的？, page url: https://www.mianshi.icu/question/detail?id=945
424-945-答案：
简单题，一般在社招里面出现。

在这种强调你遇到过什么问题下，最忌讳的就是你回答我没有使用过隔离策略。如果你这么回答，那么很容易就寄掉。你要选择一个能够体现你技术能力或者对业务深入思考的案例，而后引导话题，刷出亮点。
结合简历中的项目经历从常用的隔离策略有哪些？中选择几个隔离策略，结合你的遇到的问题或者从实施隔离策略的过程中可能遇到哪些问题?该如何解决？中选几个问题及解决方案回答。
在实施隔离策略时，我主要遇到过两个问题，性能问题和资源浪费。

这里我各举一个例子。

首先是性能瓶颈，我在使用线程池隔离的时候就遇到过线上故障。

------------------------------

425. 如何在微服务架构中实现数据隔离？, page url: https://www.mianshi.icu/question/detail?id=946
425-946-答案：
略难的题，难在很可能你都没听过这个东西，一般只会出现在社招。

在微服务架构，不仅仅代码是隔离的，也就是代码可以独立部署；数据也应该是独立，也就是一个微服务对应一个数据库，而且只有这个微服务可以方位这个数据库。
在微服务架构中实现数据隔离，核心手段就是两个：
在微服务架构中实现高效且安全的数据隔离，是确保系统稳定性、可扩展性和安全性的关键所在。其核心手段主要围绕两个方面来展开。
更重要的是，就最佳实践来说，我们强调服务间应通过定义良好的API接口进行数据交互，而非直接访问对方的数据库。

例如，如果订单服务需要获取买家信息以完成订单处理，它不应该直接查询用户服务的数据库，而是应该调用用户服务提供的GetUserInfo接口。这种做法不仅符合微服务的自治原则，还能促进服务间的松耦合，使得各个服务可以独立演进，不会因数据依赖问题而相互牵制。

------------------------------

426. 在微服务架构中，怎样实现服务之间的依赖隔离？, page url: https://www.mianshi.icu/question/detail?id=947
426-947-答案：
略难的题，一般出现在社招，校招不太可能会问。

正常来说，一般的业务研发是比较少考虑这种问题的，只有说站在一个比较高的视野上才会进行这种思考。那么在这个问题之下，刷亮点的最好方式，就是举一个通过网关来跟第三方服务保持隔离，并且要在这个网关上叠加丰富完善的服务治理措施，这样可以同时展现你在微服务架构和微服务治理上的深刻理解。
在微服务架构中，服务间的依赖隔离至关重要。通常针对不同的依赖服务类型选不同的隔离策略：
在微服务架构中，服务间的依赖有很多种，具体情况需要具体分析。

对于外部依赖服务，比如支付网关、短信服务等，这些服务往往不在我们的直接掌控之中。为了降低它们对我们服务稳定性的影响，一般需要引入了专门的第三方调用网关。这个网关作为所有内部微服务与外部服务交互的桥梁，能够有效地进行管理和控制。而后借助网关统一解决鉴权、服务治理和第三方容错等问题。

对于内部依赖服务，即其他微服务，虽然它们可能是我们自己开发和维护的，但也需要做好隔离和容错措施，但是能做的事情不多，只能是加强内部服务治理，确保服务间调用的稳定可靠，并针对内部服务崩溃的情况，制定熔断、降级等容错策略。
举个例子爱说，在之前我就维护过一个针对短信服务的网关，或者说内部的一个对第三方接口进行二次封装的短信微服务。

------------------------------

427. 在微服务中，如何实现流量隔离？, page url: https://www.mianshi.icu/question/detail?id=949
427-949-答案：
略难的题，一般出现在社招。

所谓的流量隔离其实就是防止某个微服务故障之后，这个流量会把另外一些微服务也冲垮，这个东西有点像是熔断。

你在这个问题之下要刷亮点，就还是要使用自己实践中的案例，还能起到引导话题的效果。
在微服务架构中，实现流量隔离是为了确保单个服务的故障或性能问题不会影响到其他服务，并提高系统的整体稳定性和弹性。 这可以通过多种策略和技术在不同的层面实现：
在微服务架构中，可以通过多种策略和技术在不同层面来实现流量隔离。

从网络层来看，我们可以使用虚拟网络技术，比如VLAN或VXLAN，将不同的微服务放在不同的虚拟网络中，实现物理隔离。这种方式是比较彻底的，但是配置和管理上会复杂一些。另一个简单的方法是使用防火墙或者容器编排平台，比如Kubernetes，我们可以利用容器网络命名空间，这样每个容器都有自己的网络命名空间，彼此之间无法直接通信，只有通过特定的网络策略才能进行配置。

在应用层，我们的关键组件是API网关。它可以根据不同的请求路径、请求头或其他条件，将请求路由到相应的微服务。如果某个微服务出现故障，API网关就可以进行故障转移或者返回友好的错误信息，避免客户端受到影响。同时，API网关也可以实施流量控制，比如限流和熔断，以防止服务过载。此外，服务网格、负载均衡器等也都可以实现不同程度的流量隔离。
举个例子来说，在一些国际业务中，通常都是部署了多个集群。这些集群从代码到数据，从网络到硬件都是彻底隔离的。比如说亚太地区和欧洲地区就是一个彻底隔离的。

------------------------------

428. 请介绍你的这个本地消息表的项目, page url: https://www.mianshi.icu/question/detail?id=950
428-950-答案：
简单题。

参考项目内容中的项目介绍部分，根据自己的定位来选择一个介绍方式。

------------------------------

429. 为什么你要做这个本地消息表的项目？, page url: https://www.mianshi.icu/question/detail?id=951
429-951-答案：
简单题。

你只需要强调做这个项目是因为在实践中真的需要，而后再次强调一下在公司内很受欢迎，有效改造了历史老旧系统，提供了系统可用性和稳定性。
在项目介绍中我已经提到了，从理论上来说一家公司需要这种通用解决方案的理由很简单：
从公司整体业务需求来看，大量业务场景都明确要求在处理完业务逻辑后必须发送一条消息。这种广泛存在的需求，凸显了构建一个统一解决方案的紧迫性。

在实践中，其实有很多类似这样的东西，只不过是缺乏一个带头的人站出来说我们一起改进这个东西。

------------------------------

430. 在你方案里面，你是怎么支持分库分表的？, page url: https://www.mianshi.icu/question/detail?id=952
430-952-答案：
略难的题，当然如果你本身要是有接触过分库分表，那么就是一个很简单的题目。

记住，在支持分库分表的时候，核心是解决两个问题：知道插入哪个本地消息表，补偿任务要记得处理所有的目标表。
为了方便你进一步理解，记得先阅读分库分表面试题集，这里不会系统讨论分库分表怎么分的问题。

你记住，只要你不是设计分库分表中间件，那么对你来说你是不需要关心分库分表的细节的。

因此在这种通用解决方案里面，最重要的就是根据自己的需要抽象出来解决分库分表问题的接口。
要在本地消息表上支持分库分表，有两个关键点。

第一个关键点是在本地事务阶段，要知道本地消息应该插入到哪个表。也就是，本地消息表也需要做分库分表。

第二个关键点是考虑补偿任务扫描表的问题，也就是说我们需要知道分库分表之后究竟有多少库有多少表。
从设计上来说，我们其实并不关心用户究竟怎么分库分表，但是一些必要的原则还是可以遵循的。

第一条必须遵守的规则就是本地消息表要和业务保持同库，这是因为在本地消息表要求插入本地消息数据的时候，必须和业务操作在同一个数据库事务中。

从这一条里面也可以推论出来，只需要同库，但是分表可以用不同的方案。例如说，订单按照买家 ID 分库分表之后，对应的本地消息表必须同样使用买家 ID，应用同样的分库规则来分库。但是本地消息表可以不分表，也可以按照别的规则来分表，这些都不影响。

------------------------------

431. 在本地消息表的解决方案中，你为什么要支持分库分表？, page url: https://www.mianshi.icu/question/detail?id=953
431-953-答案：
简单题。

最佳原因就是你也不想搞那么麻烦的，但是业务上分库分表了，你只能跟上。
在面试中，有一个很好的理由或者说借口，就是业务需要，又或者一帮傻逼同事搞出来了傻逼问题逼得你不得不这么干。
其实最开始我也没想支持分库分表的，但是后面调研业务的时候发现有一些业务已经分库分表了，而且他们同样需要本地消息表的解决方案，因此我也就只能考虑支持分库分表了。

------------------------------

432. 为什么本地消息表分库一定要和业务的分库规则一样？, page url: https://www.mianshi.icu/question/detail?id=954
432-954-答案：
简单题。

你记住最为关键的：数据库事务可以操作多个不同的表，但是数据库不支持跨库事务。
你要记住本地消息表解决方案中的一个关键的图：
这是因为本地消息表的解决方案要求执行的业务操作和存储本地消息的时候，必须要保持 ACID 的特性。

------------------------------

433. 在你这个项目里面，有什么难点？, page url: https://www.mianshi.icu/question/detail?id=955
433-955-答案：
简单题，毕竟难点我已经列举出来了。
参考项目说明中的难点部分项目难点，你根据面试官的偏好，选择一个难点来回答。

但是在选择的时候要注意：

这里就不再重复了。

------------------------------

434. 你的补偿任务是如何实现的？, page url: https://www.mianshi.icu/question/detail?id=956
434-956-答案：
简单题。

在这个问题之下，一定要记得提及动态调度的问题，这是你刷亮点，赢得竞争优势的地方。当然，这实际上也是我推荐的可以作为项目难点的东西。
补偿任务的基本思路是：

一个顺利的执行补偿任务的序列图如下：


补偿任务的核心思路是通过分布式锁机制来确保任务的唯一性和高效执行。

具体来说，部署了相同补偿任务的节点会竞争分布式锁，只有抢到锁的节点才能开始执行补偿任务。每个任务都是一个无限循环，每次循环会从数据库中筛选出一批处于初始化状态且更新时间小于默认阈值的消息进行补偿发送。

如果某次循环未找到任何需要补偿的消息，系统会进入一秒的睡眠状态；若连续多次发送失败，则判定补偿任务执行失败，并退出循环。
而且，这种设计还有一种好处，就是实现了动态调度的效果，也就是借助抢占-让出机制，可以让补偿任务在不同的节点之间流转，防止某个节点占着补偿任务，结果一直执行失败。

举例来说，节点A一旦抢到分布式锁，便开始执行补偿任务，但如果连续多次发送补偿消息失败，节点A会释放锁并进入睡眠状态。

此时，其他节点如B和C会尝试抢占分布式锁，假设节点B成功抢占，则接替执行补偿任务，而节点C抢占失败则会进入睡眠。

------------------------------

435. 为什么你的补偿任务不是一个定时任务？而是一个无限循环？, page url: https://www.mianshi.icu/question/detail?id=957
435-957-答案：
简单题。

很多人总是以为如果是什么异步任务，那么就应该是定时执行的。比如说每分钟执行一次，或者每秒钟执行一次。但是实践中还有无限循环这种做法，但是如果无限循环期间发现没有数据需要处理，就会陷入睡眠。

这种机制的好处是实时性更高。
你需要先看你的补偿任务是如何实现的？ (meoying.com)

而后，你要注意在这个解决方案中，我们的补偿任务是一个无限循环。
理论上来说，使用定时任务也是可以的。

------------------------------

436. 我不同的业务可以共用一个本地消息表吗？, page url: https://www.mianshi.icu/question/detail?id=958
436-958-答案：
简单题。

其实你从系统设计里面就可以看到，我们基本上没有做任何的限制。也就是说，你既可以一个业务一个本地消息表，也可以多个业务共享一个本地消息表，这是无所谓的事情。当然原则上来说，我们是建议不同业务使用不同的本地消息表。
从设计上来说，我们并没有做这种限制。这个其实和设计理念有关系，比如说一些人可能会认为在方案设计上就强制要求不同的业务使用不同的本地消息表会比较好。
在系统设计方面，我们并没有强制要求不同业务使用不同的本地消息表，这其实体现了我们的设计理念。有些人可能会认为在方案设计上就应该强制区分，但我们认为灵活性和可扩展性更为重要。
然而，从原则上讲，我们并不鼓励业务共享一个本地消息表，主要基于以下两点考虑：

第一个是性能影响。多个业务共用同一张本地消息表，容易导致该表承受巨大的写压力，进而影响系统性能。

------------------------------

437. 如果补偿任务执行的时候消息发送出去了，但是更新数据库状态失败了，会怎么样？, page url: https://www.mianshi.icu/question/detail?id=959
437-959-答案：
简单题，典型的部分失败问题。

在我们的方案中，如果失败了，那么下一次就还会重试。当重试次数耗尽的时候，那么就会触发告警，这时候就需要人手工介入。
记住这个方案的一个基本设计原则：我们追求的是最终一致性。

所以我们应对各种失败情形的手段就是重试 + 幂等 + 告警。

简单来说就是不断重试，直到成功。如果要是一直重试都失败了，那么就告警，这时候就需要人手工介入。
并不会造成特别大的问题。

在这种情况下，因为更新失败，所以数据库中该本地消息的状态还是处于初始化状态，因此补偿任务在执行下一个循环的时候就会再次找到它，而后再次尝试重新发送。

------------------------------

438. 如果你的补偿任务发送消息失败/超时，会怎么样？, page url: https://www.mianshi.icu/question/detail?id=960
438-960-答案：
简单题，还是部分失败的问题。

补偿任务发送失败了，依旧会更新数据库，只不过是更新重试次数和更新时间。这样达到了间隔时间之后，会被补偿任务再次找出来，继续重试。

这算是一种统一的借助数据库来控制定时任务、补偿任务等执行重试的机制。
需要先看你的补偿任务是如何实现的？ (meoying.com)
看这张图：


在发送失败的时候，有两种情况。

第一种情况是发送次数还没达到上限，那么就会更新数据库中的发送次数和时间戳。

第二种情况是发送次数已经发到了上限，那么就会直接将数据库中的状态更新为发送失败。

------------------------------

439. 什么是超时控制？, page url: https://www.mianshi.icu/question/detail?id=961
439-961-答案：
简单题，在校招和社招中都有可能遇到，一般作为深入讨论超时机制的起始问题。

在这个问题之下，你可以引用一个缺乏超时控制引发的线上故障问题。一方面是引导问题，另外一方面也是刷出亮点。
超时控制是一种防护机制，用于为操作或任务设定最大执行时间。如果操作在规定时间内未完成，系统会主动中断该操作并采取相应的处理措施，例如报错、重试、降级、熔断等。超时控制在分布式系统中尤为重要，例如网络通信、数据库操作、线程池管理等场景，可以防止请求长时间阻塞或资源被占用，从而提高系统的响应速度、稳定性和可用性，同时提升用户体验。

超时控制通常应用在以下场景：


超时控制策略的类型可以从不同的角度进行分类，并没有一个绝对的标准分类方法。以下是一些常见的分类方式：
超时控制本质上是一个防护机制，它为操作或任务设定了最大执行时间。如果操作在规定时间内未完成，系统会主动中断并采取相应的处理措施，例如报错、重试、降级或熔断等。超时控制在分布式系统中尤其重要，像网络通信、数据库操作、线程池管理这些场景，它可以防止请求长时间阻塞或资源被占用，从而提高系统的响应速度、稳定性和可用性，同时提升用户体验。

超时控制通常应用在很多场景中，例如：在网络通信中，比如HTTP请求或RPC调用，超时控制可以防止因网络问题导致请求挂起；在数据库操作中，它可以避免长时间阻塞数据库连接；在线程池管理中，它可以避免长时间执行的任务阻塞线程池；在分布式锁、消息队列、缓存操作、文件操作以及调用任何外部系统或服务时，设置超时时间都非常重要，可以有效防止外部系统故障影响自身系统的稳定性。
超时控制在分布式系统中是很重要的。比如说我就解决过因为超时控制设置不当引发的连接不足的问题。

问题其实也很简单，就是在系统中有一些数据库查询没有设置超时时间，使用了一个默认的极长的超时时间。而后在一次数据库性能抖动的情况下，高并发请求直接把连接池里面所有连接都拿走。但是因为超时时间太长了，以至于这些请求都持有连接不放，在等待响应。

------------------------------

440. 为什么需要超时控制？, page url: https://www.mianshi.icu/question/detail?id=962
440-962-答案：
简单题，在校招和社招中都有可能遇到。

类似地，在这个问题之下，你需要准备一些超时控制的案例，最好是负面案例，从而强调超时控制的作用以及在缺乏超时控制的时候可能遇到的问题。
在分布式系统中，超时控制是至关重要的，因为它直接关系到系统的稳定性、性能和可用性。 如果没有有效的超时控制，系统将面临一系列严重的问题：
在分布式系统中，超时控制是非常重要的，因为它直接关系到系统的稳定性、性能和可用性。如果没有有效的超时控制，系统可能会面临一系列严重的问题。

首先，资源耗尽是一个大问题。长时间未响应的请求会持续占用系统资源，比如线程、网络连接和内存等，最终可能导致资源耗尽，影响其他请求的处理，甚至可能导致整个系统崩溃。超时控制可以及时终止这些阻塞请求，释放资源，避免资源饥饿。

其次，性能下降也是一个关键因素。长时间等待无响应的请求会降低系统的整体吞吐量和响应速度，从而影响用户体验。超时控制能够快速识别并放弃那些无望的请求，把资源分配给其他请求，这样就能提高系统的性能。
举个例子来说，我就遇到过一个因为缺乏超时控制导致线程池线程耗尽的问题。

问题的根源很简单，在调用下游服务的时候，没有设置超时时间，而默认的超时时间是 10s。在这种情况下，在下游服务性能突然出现问题的情况下，迟迟没有办法返回响应的情况下，我这边的线程都在发起调用之后等响应，导致新来的请求完全拿不到线程。

------------------------------

441. 超时控制的目的和作用是什么？, page url: https://www.mianshi.icu/question/detail?id=963
441-963-答案：
简单题，在校招和社招中都有可能遇到。

回答这个问题，还是要考虑使用一个案例来证明超时控制的作用。最好是选择一些比较有特色的案例，比如说跟性能优化、问题排查有关的案例，这样在面试中凸显自己性能优化和问题排查的能力。
超时控制的主要目的是提升系统的稳定性和可用性及用户体验。它通过设置一个时间限制，来避免因为网络延迟、服务器故障或其他原因导致的长时间等待，它主要有以下几个作用：


总而言之，超时控制是构建高性能、高可用性系统的重要机制。它通过设置时间限制，来防止资源浪费、提高响应速度、增强系统健壮性，最终提升用户体验、系统稳定性和可用性。合理的超时控制需要根据具体的应用场景和系统架构进行配置，避免设置过短或过长，影响系统效率和用户体验。
超时控制的主要目的是提升系统的稳定性、可用性和用户体验。它通过设置一个时间限制，来避免因为网络延迟、服务器故障或其他原因导致的长时间等待。主要有以下几个作用：

首先也是最为关键的作用，就是提升用户体验。一个明确的超时响应比起无限期的等待要好得多。用户可以根据超时信息采取相应的行动，比如重试操作或联系客服，而无限期的等待只会让用户感到困惑和沮丧。

其次，超时控制能提升系统的可用性。即使在出现网络延迟、服务器故障等异常情况下，超时机制也能保证系统不会因为单个请求的长时间阻塞而导致整体瘫痪，这样就能提升系统的可用性。
举个例子来说，我就遇到过一个因为缺乏超时控制导致线程池线程耗尽的问题。

问题的根源很简单，在调用下游服务的时候，没有设置超时时间，而默认的超时时间是 10s。在这种情况下，在下游服务性能突然出现问题的情况下，迟迟没有办法返回响应的情况下，我这边的线程都在发起调用之后等响应，导致新来的请求完全拿不到线程。

------------------------------

442. 超时控制有哪些类型？, page url: https://www.mianshi.icu/question/detail?id=964
442-964-答案：
简单题，在校招和社招中都有可能遇到。

在这个问题之下，你需要考虑的就是使用一些案例，证明自己有丰富的实践经验。
超时控制的类型可以从不同的角度进行分类，并没有一个绝对的标准分类方法。以下是一些常见的分类方式：
超时控制的类型可以从不同的角度进行分类，其实并没有一个绝对的标准分类方法。常见的分类方式有几种。

首先，基于时间的分类是最常见的，可以分为固定超时时间、指数退避超时时间以及自适应超时时间。固定超时时间简单易实现，但缺乏灵活性，无法适应网络状况的变化。比如，HTTP请求的超时时间通常设置为几秒钟。而指数退避超时，每次重试时超时时间会呈指数级增长，这种方式适用于网络抖动的情况，可以避免因为短暂的网络延迟而导致的误判。再有就是自适应超时时间，它会根据历史请求的响应时间动态调整超时时间。这种实现比较复杂，但更灵活，能够更好地适应网络状况的变化，通常需要监控和统计请求的响应时间。

也可以是基于作用范围的分类。客户端超时控制是指客户端在发起请求后设置超时时间，如果在规定时间内没有收到服务器的响应就算超时。服务端超时控制则是服务器在处理请求时设置超时时间，如果在规定时间内没有完成请求处理就算超时。还有链路超时控制，这种方式针对分布式系统，为整个请求链路设置一个全局的超时时间，关注整个链路的完整性和响应时间。
在微服务架构中，最为关键的是链路超时控制，也是我使用得最多的超时控制。一般来说它要求在请求入口的时候从用户体验的角度上设置一个超时时间，例如说整个请求的超时时间不超过 1s。

------------------------------

443. 什么是客户端超时控制？, page url: https://www.mianshi.icu/question/detail?id=965
443-965-答案：
简单题。

一般来说，在实践至少需要设置客户端超时，这也是大多数微服务框架、各种客户端支持的做法。
客户端超时控制是指客户端在发起请求后，设置一个时间限制，如果在规定的时间内没有收到服务器的响应，则客户端会认为请求超时，并采取相应的处理措施。 这是一种防止客户端长时间等待无响应请求的机制，可以提高客户端的响应速度和稳定性。

客户端超时控制通常由客户端应用程序或网络库实现，例如，在发起HTTP请求时，可以设置connect timeout（连接超时）和read timeout（读取超时）。
客户端超时控制是指在客户端发起请求后，设置一个时间限制。如果在这个时间内没有收到服务器的响应，客户端就会认为请求超时，并采取相应的处理措施。这种机制可以防止客户端长时间等待无响应的请求，从而提高响应速度和稳定性。

通常，客户端超时控制是由客户端应用程序或网络库来实现的。比如，在发起HTTP请求时，我们可以设置连接超时和读取超时。

连接超时是指客户端尝试与服务器建立连接的时间限制。如果在规定的时间内无法建立连接，就会认为连接超时。读取超时则是指客户端等待服务器返回数据的时间限制。即使连接成功，如果服务器在规定时间内没有返回数据，也会认为读取超时。
从设计超时机制的角度来说，客户端超时也是必不可少的。

------------------------------

444. 什么是服务端超时控制？, page url: https://www.mianshi.icu/question/detail?id=966
444-966-答案：
简单题。

服务端超时控制在实践中也算常用，你要刷亮点就可以讨论为什么大多数的框架都不仅仅是支持服务端超时控制，因为服务端超时控制并不怎么靠谱，客户端有可能收不到服务端返回的超时响应。

因此，服务端超时控制一般也要结合客户端超时来使用。
服务端超时控制是指服务器在处理客户端请求时，设置一个时间限制，如果在规定时间内未能完成请求处理，则服务器会认为请求超时，并采取相应的措施。 这是一种防止服务器长时间处理单个请求，占用过多资源，影响其他请求处理的机制，可以提高服务器的稳定性和吞吐量。

服务端超时控制的实现方式多种多样，取决于具体的服务器软件和编程语言：


当发生服务端超时时，服务器可以采取多种措施：
服务端超时控制是指服务器在处理客户端请求时，设定一个时间限制。如果在这个规定的时间内没有完成请求的处理，服务器就会认为请求超时，并采取相应的措施。这种机制可以防止服务器长时间处理单个请求，从而占用过多的资源，影响其他请求的处理，最终提高服务器的稳定性和吞吐量。

实现服务端超时控制的方式有很多，具体取决于服务器的软件和编程语言。比如，可以使用线程池来限制并发请求的数量，并为每个请求分配一个最大处理时间。如果请求处理时间超过这个限制，线程池就会中断请求处理，释放资源。还有一种方式是使用定时器来监控请求的处理时间，如果超过预设的时间，就会触发超时处理逻辑。此外，异步编程模型也可以避免单个请求阻塞服务器的处理线程，从而提高并发处理能力。通常，异步编程会结合定时器或其他机制来实现超时控制。很多服务器框架，比如Spring和Node.js，内置了超时控制机制，这样配置和使用起来就非常方便。
服务端超时控制最大的弊端就是它虽然可以尝试返回超时响应，但是客户端有可能完全拿不到这个超时响应。

------------------------------

445. 客户端超时控制与服务端超时控制的区别和联系？, page url: https://www.mianshi.icu/question/detail?id=967
445-967-答案：
简单题，一般校招不太会问，在社招会遇到。

回答这个问题，还是要深入讨论服务端超时控制的一个极大的弊端：客户端可能收不到服务端的超时响应。这也就导致对于一个框架来说，客户端超时控制是必不可少的。
前置知识：

客户端超时控制和服务端超时控制都是为了提高系统稳定性和可用性及用户体验而设置的机制，但它们的作用对象、实现方式和目标略等方面略有不同：

区别：
客户端超时控制和服务端超时控制都是为了提高系统的稳定性、可用性和用户体验而设置的机制，但它们在作用对象、实现方式和目标上有一些不同之处。

首先，作用对象不同。客户端超时控制主要作用于客户端，而服务端超时控制则是针对服务器。其次，作用时机也不同。客户端超时控制在客户端发起请求后生效，主要是在等待服务器响应的期间；而服务端超时控制是在服务器接收到请求后，处理请求的过程中生效。

在超时类型上，客户端超时控制主要关注网络连接和数据读取的超时，比如连接超时和读取超时。而服务端超时控制的范围更广，可能包括请求处理超时、数据库操作超时等。
在实践中，一般来说客户端超时控制和服务端超时控制都是一起使用的，而且对大部分框架来说，至少会有一个客户端超时控制。

------------------------------

446. 什么是链路超时控制？, page url: https://www.mianshi.icu/question/detail?id=968
446-968-答案：
略难的题，难在一般人学习微服务的时候可能只会接触到超时控制，而没有深入研究一般的超时控制和链路超时控制的区别。

在这个问题之下，你可以讲清楚超时控制的基本原理，而后在这个过程中引出有关于链路超时控制的超时时间设置、超时时间传递等问题，引导话题并且刷出亮点。
链路超时控制是分布式系统中重要的性能优化和容错机制，用于在服务调用链路中对请求的最大响应时间进行限制，保证请求可以在规定时间内处理完毕，保障用户体验。

注意：链路超时控制的出发点是用户体验，其余的提高可用性之类的都是

和单个服务节点的超时控制不同，链路超时控制需要针对整个请求链路设置超时机制，而不仅仅是针对单个服务节点。如果在规定的时间内，请求没有完成整个链路上的所有处理步骤，就会被认为是链路超时。链路超时控制需要考虑整个请求链路上所有节点的处理时间以及网络延迟等因素。这就意味着它通常需要更复杂的机制来跟踪请求在链路上的状态，并根据链路上各个节点的响应情况来判断是否超时。
在分布式系统中，链路超时控制是一个非常重要的性能优化和容错机制。它用于在服务调用链路中对请求的最大响应时间进行限制，保障用户体验，也可以起到提高系统可用性和稳定性的效果。
一般来说，在微服务架构中都要考虑接入链路超时控制。

但是在接入过程中有很多复杂的问题需要考虑。第一个问题是链路超时时间不好设置，从理论上来说，这个数字应该是根据用户体验来决定的，也就是产品经理应该提供这个值。但是很多产品经理不够专业，无法提供这种值。

------------------------------

447. 链路超时控制与单机超时控制的区别和联系？, page url: https://www.mianshi.icu/question/detail?id=969
447-969-答案：
略难的题，当然如果你要是知道什么是链路超时控制，这个题目就是一个简单题。

在这个问题之下你可以根据单机超时控制的弊端来引申出为什么我们会需要链路超时控制，以及链路超时控制在提升用户体验方面的巨大作用。
链路超时控制和单节点超时控制都是分布式系统中重要的超时机制，但它们关注的层面和实现方式有所不同：

区别：

联系：
链路超时控制和单节点超时控制在分布式系统中都是非常重要的超时控制机制，但它们关注的层面和实现方式有所不同。

首先，作用范围上，单节点超时控制主要针对单个服务节点的请求处理时间，而链路超时控制则是针对整个请求链路，涵盖多个服务节点。单节点超时关注的是单个服务的响应速度，而链路超时则关注整个请求处理流程的完成时间。

在超时时间的设置上，单节点超时的设置相对简单，主要考虑单个服务的处理能力和网络延迟。而链路超时的设置就复杂得多，从理论上来说就是根据用户体验来设置，但是在实践中一般是需要在用户体验和系统设计之间取得一个平衡。
从微服务架构的角度来说，单机超时控制显然是不够的。

------------------------------

448. 在分布式系统中，链路超时控制是如何实现的？, page url: https://www.mianshi.icu/question/detail?id=970
448-970-答案：
略难的题。

其实链路可以看做是一个个客户端-服务端调用组成的。所以链路超时控制就是要协调这些客户端和服务端，一方面控制住自己代码执行的时间，一方面是控制住发起调用的时间。
在分布式系统中，实现链路超时控制的核心是链路超时时间的管理和传播，确保整个请求链路中的每个服务都能在规定的时间内完成处理，并在超时发生时快速中断链路，避免资源浪费和级联故障。

你可以将整个链路看做是一个个客户端-服务端调用组成的。举例来说，A -> B -> C 这样的链路，也就是由 A 调用 B 和 B 调用 C 两个环节组成的。因此整个链路的超时控制核心要解决的就是 A 调用 B 这样的客户端-服务端调用里面，客户端怎么控制这个调用的时间，怎么传递超时时间，以及 B 怎么控制自己处理请求的时间。

所以总的来说就是三个步骤：客户端超时控制、超时时间传递、服务端超时控制：
正常来说，链路超时控制看做是由多个客户端-服务端这样的环节构成的。对于每一个客户端-服务端环节来说，客户端在发起调用的时候要启动一个计时器，如果超时了就直接中断，不会再继续等待服务端的响应。

然后，这个超时时间（或者截止时间）需要沿着调用链向下游服务传递。在进程内，可以使用上下文对象或线程局部存储来传递；跨进程的话，一般来说是客户端要把超时时间写入到请求的某些部分，比如说使用 HTTP Header、gRPC Metadata 或者消息体来传递。

在服务端，收到请求之后要从请求中取出来超时时间，而后计算剩余的超时时间。接下来，服务端需要校验剩余时间是否足够，如果不足，就直接拒绝请求。同时，设置自身处理请求的超时时间，确保不超过剩余时间。

------------------------------

449. 在分布式系统中，链路超时时间是如何在整个链路中传递的？, page url: https://www.mianshi.icu/question/detail?id=971
449-971-答案：
略难的题，这个题其实考察的是框架的设计原理，包括 RPC 协议之类的设计原理。

在实践中，单一框架内传递链路超时时间还是很容易的，比如说都是 gRPC 调用，或者都是 Dubbo 调用，这种就很简单。但是如果涉及了跨框架链路超时时间传递，一般都要自己手搓。因此你可以考虑自己实践一下怎么在不同的框架中传递超时时间，而后假装自己在公司里面提供了通用的链路超时控制解决方案，从而刷亮点并且赢得竞争优势。

在分布式系统中，链路超时时间的传递需要考虑进程内和跨进程两种场景，以及传递的超时时间是时间戳还是剩余超时时间。


注意这两者之间有一个极大的区别就是时间戳不需要考虑网络传输时间，但是剩余超时时间需要考虑网络传输时间。如图：


在分布式系统中，链路超时时间的传递需要考虑进程内和跨进程两种场景，还要考虑我们传递的超时时间是时间戳还是剩余超时时间。

先说进程内传递吧，就是在单个服务的进程内部，链路超时时间通常是通过上下文（Context）或者线程本地变量（Thread Local）来传递的，这种方式依赖于编程语言的特性。以 Java 为例，可以使用 ThreadLocal 或者类似的 Context 对象（比如 Spring 的 RequestContext 或 gRPC 的 Context），把链路超时时间存储在上下文中，供整个调用链上的方法共享和读取。举个例子，在 gRPC 中，Context 是线程安全的，可以用来存储超时时间，并在方法调用过程中自动传递。而在 Go 语言中，标准库提供了 context.Context，用于传递超时时间等请求范围内的元数据。通过使用 context.WithTimeout，可以创建一个带有超时时间的上下文，然后在调用链中传递。

再说跨进程传递，在服务之间的跨进程调用中，链路超时时间需要通过通信协议来传递，具体实现依赖于协议的特性和设计。
我在公司就解决过跨框架的超时传递问题。例如说，如果是全部使用 Dubbo 协议，那么 Dubbo 框架自己就会管链路超时。但是如果 Dubbo 调用里面还混合了 HTTP 调用或者 gRPC 调用，那么就要自己手动管了。

在我们公司的业务里面，有一些业务场景是 RPC 接口里面调用 HTTP 接口，也有一些场景里面是 HTTP 接口调用 RPC 接口。在这种情况下，我在 HTTP 框架和 RPC 包里面借助拦截器里面统一处理了链路超时的问题。

------------------------------

450. 在分布式系统中，链路节点如何检测整个链路是否已经超时？, page url: https://www.mianshi.icu/question/detail?id=972
450-972-答案：
简单题，一般只会出现在社招，只有非常深入讨论超时机制，以及超时机制怎么实现的情况下，才会问到这种问题。

其实这个问题主打措手不及，因为一旦你搞清楚了链路超时控制是如何传递超时时间的，就知道如何检测了。而后你可以深入讨论超时控制说到底是重度依赖时钟中断的。
在链路超时时间传递里面，例如说 A 调用 B，要把链路超时时间从 A 传递到 B，提到有两种方法：

那么对于客户端，也就是 A 来说：它需要在本地启动一个计时器，计时器就是剩余超时时间。到点了 A 就知道链路已经超时了；

而对于服务端 B 来说，它在收到请求之后，要立刻检测一下有没有超时。如果已经超时了，那么 B 就会直接拒绝这个请求，比如说返回一个超时响应。而后 B 也会开启一个计时器，到点了 B 就知道已经超时了，执行超时处理逻辑。
在链路超时判定机制中，当A调用B时，我们有两种主要方法将链路超时时间从A传递到B。第一种方法是传递时间戳，这是一个具体的、绝对的时间点，比如传递一个表示“2024年12月1日11时11分11秒123毫秒”的毫秒数值。第二种方法是传递剩余超时时间，这是一个相对值，它表示从当前时刻起，整条链路还允许的最大响应时间，例如传递“300毫秒”。
而不管是 A 启动了计时器，还是 B 启动了计时器，都是一种抽象的说法，实践中并不是意味着它们真的创建了类似 Timer 或者 Ticker 的对象。

例如说 A 启动的计时器，在代码上可能就是发起系统调用，传入了一个阻塞时间。而后操作系统内核在阻塞了 A 的线程之后，将 A 加入阻塞队列，并且加上了一个超时时间。

------------------------------

451. 在分布式系统中，链路节点在检测到链路超时后，有哪些常见的处理策略？, page url: https://www.mianshi.icu/question/detail?id=973
451-973-答案：
简单题，一般在社招里面会问到。

这个问题其实很好装逼，因为你可以进一步结合熔断、限流、降级、同步转异步这一类的容错措施来回答，体现你对服务治理的深刻理解。而后，如果你能举出实践中的例子，就可以进一步装逼了。
参考如何处理超时请求？适当修改一些衔接语即可，两者基本一样。
在分布式系统中，当链路节点检测到链路超时时，有以下几种常见的处理策略可以选择。

首先是快速失败的策略。当检测到链路超时后，系统会立即返回错误信息，而不进行任何重试或降级操作。这种方式简单直接，响应速度快，可以避免不必要的资源浪费。不过，它的容错性较差，可能会导致用户体验下降。这个策略适用于那些对实时性要求非常高，但容错性要求不高的场景。

接下来是重试机制。当链路超时后，链路节点会尝试重新发送请求。我们可以设置重试次数和重试间隔，以避免频繁重试导致系统负载过高。通常会使用指数退避算法来设置重试间隔，这样可以避免所有节点同时重试造成雪崩效应。需要注意设置重试次数和间隔，避免无限重试。这个策略适用于对实时性要求不高，且超时原因可能是短暂的网络抖动或服务波动的情况。
我在实践中使用过很多策略。
此外我还设计过一个比较高级的重试策略。

它的基本思路是这样的。如果是偶发性的超时，那么重试是能够解决问题的。但是如果并不是偶发性的超时，而是系统负载比较高引发的超时，那么重试就会进一步加重服务的负载，引起更多的超时。

举个例子来说，A 调用 B，如果 B 负载很高从而返回了超时响应。如果此时 A 重试的话，就会产生更多的流量，进一步加剧 B 的负载。

------------------------------

452. 如何确定一个合理的超时时间？需要考虑哪些因素？, page url: https://www.mianshi.icu/question/detail?id=975
452-975-答案：
简单题，不过这个题目一般人都很难回答好。道理很简单，因为很多人在设置超时时间的时候，并不会仔细思考，而是随便设置一个。

在这个问题之下，你可以讨论一个很有意思的点：就是以用户体验为中心设置的超时时间，可能在技术上难以达到，该怎么办。在实践中，一般都是要先考虑优化性能，只有在优化都搞完了也没办法的情况下，才会进一步考虑提高超时时间。
我教你两个最简单的技巧，在面试中肯定非常好用的东西。
从最佳实践上来说，超时时间只有一个标准，那就是用户体验，尤其是链路超时时间。

举例来说，当产品说用户最多只能等 1s，如果时间再长就会有用户开始流失。那么这个时候整条链路的超时时间就只能是 1s。

而具体到单机超时时间，那么就是考虑下游服务的响应时间，一般是 999 线。
这种方法可以说是非常政治正确，但是在实践中会遇到很多问题，最核心的问题就是用户体验要求的时间可能从技术层面上很难达到。

例如说从用户体验角度出发，链路超时时间不能超过 1s。但是可能这个业务就是非常复杂，以至于整条链路上的各个环节执行时间加一起，不太可能做到 1s 之内。

------------------------------

453. 如果超时时间设置过短，会产生什么问题？, page url: https://www.mianshi.icu/question/detail?id=976
453-976-答案：
简单题，在校招或者社招都有可能遇到。

在这个问题之下，你可以讨论因为超时-重试-加重超时这个现象来刷亮点，把它做成自己的一个问题排查案例。
如果超时时间设置过短，主要会产生以下问题：
如果超时时间设置得过短，会引发以下问题。

首先，请求失败率飙升是最直接的后果。很多正常的请求在完成处理之前就被判定为超时而失败，这会导致客户端需要重试，从而增加系统负载和网络开销。如果重试机制设计不当，甚至可能引发级联故障，导致系统崩溃。

其次，用户体验会严重下降。频繁的请求失败会直接导致应用功能不可用或间歇性不可用，用户可能会遇到错误提示、页面加载失败或者操作无响应等情况。这会极大影响用户体验，导致用户流失和负面评价。
其中我就曾经遇到过一个超时时间引发的系统故障。

当时我接入了一个下游服务，超时时间设置在 300ms，并且设置了超时之后重试的机制。原本一直运行得很好，但是有一天下游服务更新了版本，它的响应时间 999 线飙升到了 800 ms。

在这种情况下，我调用下游服务的时候出现大量超时，而重试又严重加剧了下游服务的负担，导致响应时间进一步延长，导致我这边超时更加严重。在这种超时-重试-更严重的超时-更严重的重试循环下，很快我的服务和下游服务都崩溃了。

------------------------------

454. 如果超时时间设置过长，会产生什么问题？, page url: https://www.mianshi.icu/question/detail?id=977
454-977-答案：
简单题。

在这个问题之下你也同样可以使用因为超时时间过长引发的线上故障案例，凸显你丰富的实战经验和问题排查能力。
如果超时时间设置过长，会产生以下问题：
如果超时时间设置得过长，会引发一些严重的问题。

首先，资源耗尽是最主要的问题。长时间的等待会占用大量的系统资源，比如线程池中的线程、数据库连接和网络连接。如果有很多请求都处于等待状态，最终会导致系统资源耗尽，这会影响其他服务的正常运行，甚至可能导致系统崩溃。这在高并发场景下尤其严重。

其次，响应延迟也是一个问题。如果在处理请求时发生了死锁或阻塞，过长的超时时间会导致响应延迟增加，影响用户体验。用户需要等待更长的时间才能得到结果，这无疑会降低他们的满意度。
举个例子来说，我就遇到过一个超时时间过长引发的线上故障。

问题的根源很简单，在调用下游服务的时候，没有设置超时时间，而默认的超时时间是 10s。在这种情况下，在下游服务性能突然出现问题的情况下，迟迟没有办法返回响应的情况下，我这边的线程都在发起调用之后等响应，导致新来的请求完全拿不到线程。

------------------------------

455. 常见的超时控制策略有哪些？, page url: https://www.mianshi.icu/question/detail?id=978
455-978-答案：
简单题。

这个问题简单回答就是链路超时控制和单机超时控制，而后进一步结合自己的实践案例来刷亮点。
前置知识：
最常见的超时控制策略就是两种：链路超时控制和单机超时控制。前者强调的是通过超时控制控制住整条链路，每一个环节的超时时间都会被这个时间控制。后者就是普通的单一环节或者某一次调用的超时控制。

一般来说在分布式系统中，应该要优先使用链路超时控制。


除了这两个，也可以从其它角度来区分不同的超时策略。

我在 X 公司中，曾经在公司里面推行过链路超时时间。当时的背景是这样的，我们当时的服务都有超时控制，但是都是单机超时控制。这就出现了一个问题，虽然每一个环节的响应时间都没有问题，但是加在一起，这个响应时间就很长了，以至于用户会没有耐心继续等下去，造成用户流失和业务损失。

因此我在搞清楚这个问题之后，就开始推进这件事，要求各个业务方都需要接入链路超时控制。这个东西从技术上来说并不是一个很难的问题，只是说需要有很强的协调沟通能力，以及规划能力。因为整个链路是由很多个环节构成的，不同环节是不同的组在维护，所以需要协调这些组一起接入。

------------------------------

456. 如何处理超时异常？, page url: https://www.mianshi.icu/question/detail?id=983
456-983-答案：
简单题。这个问题等同于如何处理超时请求，一般出现在社招中。

这个问题其实很好装逼，因为你可以进一步结合熔断、限流、降级、同步转异步这一类的容错措施来回答，体现你对服务治理的深刻理解。而后，如果你能举出实践中的例子，就可以进一步装逼了。
如何处理超时请求？的另一种问法，参考其答案并适当修改一些衔接语即可，只能算是另外一种问法。
在处理超时异常时，我们通常可以采取几种策略来应对这个问题。

首先是快速失败的策略。当检测到超时异常时，系统会立即返回错误信息，而不进行任何重试或降级操作。这种方式简单直接，响应速度快，可以避免不必要的资源浪费。不过，它的容错性较差，可能会导致用户体验下降。这个策略适用于对实时性要求极高，但容错性要求不高的场景。

接下来是重试机制。当发生超时异常时，系统会尝试重新发送请求。我们可以设置重试次数和重试间隔，比如使用指数退避算法来避免频繁重试导致系统负载过高。这种机制简单易实现，可以提高系统的容错性，处理一些短暂的网络抖动或服务波动。但如果超时是由于服务端故障或网络拥塞等长期问题导致的，重试可能就无效了，反而会加剧系统负载。因此，我们需要谨慎设置重试次数和间隔，避免无限重试。这个策略适用于对实时性要求不高，且超时原因可能是短暂的网络抖动或服务波动的情况。
我在实践中使用过很多策略。
此外我还设计过一个比较高级的重试策略。

它的基本思路是这样的。如果是偶发性的超时，那么重试是能够解决问题的。但是如果并不是偶发性的超时，而是系统负载比较高引发的超时，那么重试就会进一步加重服务的负载，引起更多的超时。

举个例子来说，A 调用 B，如果 B 负载很高从而返回了超时响应。如果此时 A 重试的话，就会产生更多的流量，进一步加剧 B 的负载。

------------------------------

457. 如何处理超时请求？, page url: https://www.mianshi.icu/question/detail?id=984
457-984-答案：
简单题，一般在社招里面会问到。

这个问题其实很好装逼，因为你可以进一步结合熔断、限流、降级、同步转异步这一类的容错措施来回答，体现你对服务治理的深刻理解。而后，如果你能举出实践中的例子，就可以进一步装逼了。
前置知识：

处理超时请求时，通常可以采取以下几种策略：
在处理超时请求时，我们通常可以采取几种策略来应对这个问题。

首先是快速失败的策略。当检测到请求超时时，系统会立即返回错误信息，而不进行任何重试或降级操作。这种方式简单直接，响应速度快，可以避免不必要的资源浪费。不过，它的容错性较差，可能会导致用户体验下降。这个策略适用于对实时性要求极高，但容错性要求不高的场景。

接下来是重试机制。当请求超时时，系统会尝试重新发送请求。我们可以设置重试次数和重试间隔，比如使用指数退避算法来避免频繁重试导致系统负载过高。这种机制简单易实现，可以提高系统的容错性，处理一些短暂的网络抖动或服务波动。但如果超时是由于服务端故障或网络拥塞等长期问题导致的，重试可能就无效了，反而会加剧系统负载。因此，我们需要谨慎设置重试次数和间隔，避免无限重试。这个策略适用于对实时性要求不高，且超时原因可能是短暂的网络抖动或服务波动的情况。
我在实践中使用过很多策略。
此外我还设计过一个比较高级的重试策略。

它的基本思路是这样的。如果是偶发性的超时，那么重试是能够解决问题的。但是如果并不是偶发性的超时，而是系统负载比较高引发的超时，那么重试就会进一步加重服务的负载，引起更多的超时。

举个例子来说，A 调用 B，如果 B 负载很高从而返回了超时响应。如果此时 A 重试的话，就会产生更多的流量，进一步加剧 B 的负载。

------------------------------

458. 超时控制策略会对系统造成什么影响?, page url: https://www.mianshi.icu/question/detail?id=985
458-985-答案：
简单题，一般出现在社招。

这种有什么影响的题目，虽然明面上可以是正面影响，但是实践中一般强调的是负面影响，所以你回答的重点应该是负面影响和对应的解决方案上。

并且在这个问题之下，你同样可以使用因为超时机制使用不当引发线上故障的案例，来佐证对系统的影响。
超时控制策略对系统既有正面影响又有负面影响，需要仔细权衡。
超时控制策略对系统既有正面影响又有负面影响，所以我们需要仔细权衡。

从正面影响来看，合适的超时控制策略可以显著提高系统的稳定性、提升用户体验并优化资源利用率。

不过，负面影响也不容忽视。实现完善的超时控制策略需要额外的代码和配置，这会增加系统的复杂性，从而提高开发和维护的成本。还有，如果超时时间设置不合理，可能会导致频繁的重试或熔断，反而降低系统性能。此外，超时控制策略本身也可能存在缺陷，可能引入新的错误或问题。例如，熔断器配置不当可能导致服务长时间不可用。
举个例子来说，我就遇到过一个超时时间过长引发的线上故障。

问题的根源很简单，在调用下游服务的时候，没有设置超时时间，而默认的超时时间是 10s。在这种情况下，在下游服务性能突然出现问题的情况下，迟迟没有办法返回响应的情况下，我这边的线程都在发起调用之后等响应，导致新来的请求完全拿不到线程。

------------------------------

459. 接入超时控制的过程中遇到了哪些问题?如何解决的？, page url: https://www.mianshi.icu/question/detail?id=986
459-986-答案：
简单题。

这种遇到过什么问题，如何解决的题目，基本上都是送分题，前提是你准备了相应的案例。在这个问题之下，最好的装逼方式就是通过用户体验强制超时时间应该很短，而后你被逼着优化性能。

很显然，如果你用这种话术，面试官肯定会追着问你怎么优化性能的。
实施超时控制策略过程中通常会遇到以下问题：


你重点要记住前面三个：超时时长、超时时间传递、超时处理策略，剩下的三个并不是那么关键。
在实施超时控制策略的过程中，通常会遇到以下问题。

首先是超时参数的设定。一个常见的问题是如何确定合适的超时时间。如果设置得过短，可能会导致误判和频繁超时；而设置得过长，则会增加响应时间和资源消耗。不同服务和操作的最佳超时时间差异很大。一般来说，可以根据用户体验来设置一个超时时间，而后如果系统不能满足，则要考虑性能优化，或者牺牲用户体验来设置超时时间了；

另一个问题是超时上下文的传播。在分布式系统中，如何有效地在各个服务之间传递超时信息是个挑战。不同的技术栈和通信协议可能需要不同的实现方式。我们可以选择合适的上下文传播机制，比如使用自定义的HTTP Headers来传递超时信息；使用gRPC Metadata来传递超时信息等。
举个例子来说，我之前接入链路超时控制的时候，就遇到过一个超时时长的问题。

当时产品经理的要求是，要想保证用户体验良好，作为一个核心接口，整个链路的超时时间都不能超过 300ms。也就是说，从服务端收到请求到返回响应，不能超过 300ms。

------------------------------

460. 如何评估超时控制策略的有效性？, page url: https://www.mianshi.icu/question/detail?id=990
460-990-答案：
略难的题，主要还是在于很多时候你接入了超时控制之后，你会很少关注超时控制的效果好不好。

在这个问题之下，你可以考虑使用自己的实践案例来说明如何评估超时控制策略的有效性。有一个比较好的引导点就是讨论自己执行过性能测试或者压力测试，并且进一步总结自己观察到的性能测试下出现的情况，如果你平时没这部分实践，那么你可以直接使用我的答案。
评估超时控制策略的有效性可以从以下几个方面进行：


而在实践中，其实用户体验反馈这种主要依赖于产品经理和运营团队去收集和分析，而后面的这些技术指标才是我们要关注的事情。
评估超时策略的效果，最好的手段就是让产品团队或者运营团队去收集用户体验的反馈。因为之前超时时间长短是直接根据用户体验来设置的，所以它对用户体验的影响非常直接。
我平时就执行过很多性能测试，在执行性能测试的时候，有一个重点就是观测在什么情况下会开始出现大量的超时请求，这可以作为我们评估系统是否能够满足超时时间要求的一个重要标准。

正常来说，在压测的时候会明显观测到两个和超时时间有关的点。第一个是超时开始连续出现的点，在这个点之前，超时虽然有但是都是偶发现象。在这个点之后，超时开始连续出现，但是此时超时请求还是处于极少数，比率并不高。

------------------------------

461. 如何针对不同类型的服务(读操作、写操作等)设置差异化的超时控制策略?, page url: https://www.mianshi.icu/question/detail?id=991
461-991-答案：
略难的题，一般只会出现在社招，而且是深入讨论到了灵活设置不同超时策略的时候才会问到。

大部分面试官没那个水平问出这个问题，毕竟他们也就是随便设置一下。只有在一些对性能、可用性要求很高的场景下才会要求灵活、动态地设置超时时间，或者说有针对性地设计超时控制策略。
前置知识：

在微服务架构中，针对不同类型的服务（如读操作、写操作等）设置差异化的超时控制策略关键在于分析清楚不同服务的性能特征和需求。根据这些特性制定相应的超时策略，可以有效管理请求的响应时间和资源使用，从而确保系统的高可用性和优化用户体验。

一般不会搞得那么麻烦，正常就是区分读服务和写服务，然后各自设置不同的超时时间。一般来说读服务可以将超时时间设置得短一些，而写服务可以将超时时间设置得长一些。
在微服务架构中，为不同类型的服务，如读操作、写操作和计算密集型服务，设置差异化的超时控制策略的关键在于分析各服务的性能特征和需求，并以此作为制定相应超时策略的依据。

对于读操作服务，它通常是I/O密集型的，因为主要任务是从数据库或外部存储中获取数据。在某些情况下，可能还涉及复杂的处理，比如过滤和排序，这时可能会变成CPU密集型。针对读操作，我们一般可以设置较短的超时时间，通常在300毫秒到1秒之间，这样可以快速反馈用户请求，提升用户体验。如果超时发生，我们可以结合缓存机制，比如使用Redis，减少对数据库的访问，降低延迟。

------------------------------

462. 如果服务链路中的某个服务超时，那么如何确保整个服务链路能够被正确地中断？, page url: https://www.mianshi.icu/question/detail?id=995
462-995-答案：
略难的题，一般出现在社招中。

在这个问题之下，可以稍微讨论一下在链路超时之后要不要回滚业务的问题，并且进一步提及，如果要回滚业务，那么就要小心分布式事务内部回滚的问题，尤其是回滚失败的问题。
假设服务链路为 A -> B -> C -> D。并且我们进一步假设，在发起调用的时候，客户端和服务端都有计时器。例如说在 C 调用 D 的时候，C 自己会记录 D 有没有超时，而 D 也会用计时器控制自己能不能及时返回响应。因此当 D 超时之后：


另外，可以根据具体情况引入熔断机制、监控告警机制来辅助整个超时处理流程。熔断机制可以在检测到服务 D 超时的情况下，迅速切断该服务与上游服务之间的调用，避免后续请求进一步影响其他服务或客户端。

同时，监控告警机制可以帮助我们实时监控服务链路中的各个服务的健康状态。一旦服务 D 频繁超时，系统可以发送告警，及时通知运维团队进行处理。这种预警机制能够帮助团队快速定位问题，从而减少对用户的影响。
假设我们的服务链路是 A -> B -> C -> D，如果 D 这个服务超时了，我们可以采取一些步骤来确保整个服务链路能够正确地中断。

首先，D 发现自己超时了会立刻返回一个超时响应。

而后，C 收到超时响应了，或者自己也启动了计时器发现超时了，则可以考虑采取以下几种措施。第一种是重试，正常来说都是调用方决定是否要重试的。其次是考虑中断业务，并且回滚。还可以考虑执行降级策略，又或者直接返回一个超时响应。
在整个过程中，比较难处理的就是中断业务并且回滚了。

------------------------------

463. 在微服务中，如何处理依赖服务超时？, page url: https://www.mianshi.icu/question/detail?id=996
463-996-答案：
简单题
参考如何处理超时请求？适当修改一些衔接语即可。
在处理依赖服务超时时，我们通常可以采取几种策略来应对这个问题。

首先是快速失败的策略。当检测到依赖服务超时时，系统会立即返回错误信息，而不进行任何重试或降级操作。这种方式简单直接，响应速度快，可以避免不必要的资源浪费。不过，它的容错性较差，可能会导致用户体验下降。这个策略适用于对实时性要求极高，但容错性要求不高的场景。

接下来是重试机制。当依赖服务超时时，系统会尝试重新发送请求。我们可以设置重试次数和重试间隔，比如使用指数退避算法来避免频繁重试导致系统负载过高。这种机制简单易实现，可以提高系统的容错性，处理一些短暂的网络抖动或服务波动。但如果超时是由于服务端故障或网络拥塞等长期问题导致的，重试可能就无效了，反而会加剧系统负载。因此，我们需要谨慎设置重试次数和间隔，避免无限重试。这个策略适用于对实时性要求不高，且超时原因可能是短暂的网络抖动或服务波动的情况。
我在实践中使用过很多策略。
此外我还设计过一个比较高级的重试策略。

它的基本思路是这样的。如果是偶发性的超时，那么重试是能够解决问题的。但是如果并不是偶发性的超时，而是系统负载比较高引发的超时，那么重试就会进一步加重服务的负载，引起更多的超时。

举个例子来说，A 调用 B，如果 B 负载很高从而返回了超时响应。如果此时 A 重试的话，就会产生更多的流量，进一步加剧 B 的负载。

------------------------------

464. 什么是重试机制？, page url: https://www.mianshi.icu/question/detail?id=997
464-997-答案：
简单题，但是因为过于简单你反而会忘记装逼。

在这个问题之下，还是可以结合实践来刷亮点，例如说在回答的时候提及自己设计过的一些特殊的、有亮点的重试方案。在这里我直接给出一个一个在超时机制里面也说过的实践案例，你可以作为参考。
重试机制是一种常用的容错策略，通过在操作失败时自动重新执行操作（即“重试”），旨在克服由于网络抖动、服务短暂不可用等临时性故障导致的失败，从而提高操作的成功率和系统的可用性。它广泛应用于分布式系统、网络请求、数据库操作等场景，用以提升系统的可用性、稳定性以及用户体验。在遇到特定类型的错误时，系统会按照预设的规则进行重试，直到操作成功或达到预设的限制（例如最大重试次数或最长重试时间）。

以下是重试机制常见的应用场景：

重试机制的设计需要结合业务需求和系统特性，以下是常见的重试策略：
重试机制是一种常用的容错策略，就是在操作失败时自动重新执行操作，也就是“重试”。这样可以克服网络抖动、服务短暂不可用等临时故障导致的失败，提高操作成功率和系统可用性。它广泛应用于分布式系统、网络请求、数据库操作等场景，提升系统的可用性、稳定性和用户体验。当遇到特定类型的错误时，系统会按照预设的规则进行重试，直到操作成功或者达到预设的限制，比如最大重试次数或最长重试时间。

重试机制适用于以下场景：暂时性网络问题，比如网络抖动、网络超时；还有远程服务调用失败，例如服务不可用等；以及幂等性操作的业务场景，例如订单状态更新、消息确认、缓存更新等。

常见的重试策略有，固定间隔重试、随机间隔重试、指数退避重试、指数退避加随机抖动等。选择合适的重试策略需要根据场景和需求进行权衡。固定间隔适合简单场景，但可能引发重试风暴；随机间隔适合分布式系统中的资源竞争问题；指数退避是最常用的策略，适合大多数网络问题和服务过载场景；指数退避加随机抖动在高并发场景中表现更优，能有效避免重试风暴；
我在实践中就设计过一个很有意思的重试机制。这个重试机制是为了解决超时问题的，也就是在超时的时候尝试重试。

但是在实践中，如果无脑重试就容易出现一个问题：如果超时本身是因为系统负载很高而引发的，那么重试就会进一步加重系统的负担。因此在这种情况下，反而不应该重试。

------------------------------

465. 为什么需要重试机制？, page url: https://www.mianshi.icu/question/detail?id=998
465-998-答案：
简单题。

其实这个问题大部分人都能想到基本的答案，你要刷亮点可以从两个角度出发：一个是使用自己实践中的案例来证明使用超时机制的必要性；另外一个则是从重试最终都要失败的角度，阐述监控和告警以及人手工介入作为兜底。
重试机制的核心目的是应对不可避免的瞬时错误和不确定性。简单说就是处理偶发性失败。偶发性失败有很多种原因，包括：
重试机制的核心目的，是为了应对那些不可避免的瞬时错误和不确定性。简单来说，就是为了处理偶发性失败。

偶发性失败主要源自两个方面。

第一个是网络抖动。网络请求可能会因为连接中断，延迟等瞬时问题而失败，但这些问题通常是临时的。重试机制可以通过重新发送请求，在网络恢复后提高操作的成功率。比如，微服务调用或数据库写入请求因为短时的网络波动失败了，重试之后就成功完成了，这样就提高了系统的稳定性。
我在实践中就设计过一个很有意思的重试机制。这个重试机制是为了解决超时问题的，也就是在超时的时候尝试重试。

但是在实践中，如果无脑重试就容易出现一个问题：如果超时本身是因为系统负载很高而引发的，那么重试就会进一步加重系统的负担。因此在这种情况下，反而不应该重试。
虽然重试很好用，但是实际上重试也并不是银弹。重试的效果是降低业务失败出错的概率，但是不代表引入重试之后业务就一定会成功。

举个例子来说，分布式环境下基本上追求的都是最终一致性，而最终一致性很多情况下就是通过重试来保障的，但是重试也不能保障一定会满足最终一致性。例如说在本地消息表的解决方案中，异步补偿任务找出没有发送成功的消息，而后补发。

------------------------------

466. 重试机制的目的和作用是什么？, page url: https://www.mianshi.icu/question/detail?id=999
466-999-答案：
简单题。

在这个问题之下你同样可以结合实践，引述自己使用过的重试策略，一方面是证明重试机制的效果，一方面则是表达自己有丰富的重试设计经验。
这个问题非常类似于为什么需要重试机制？ (meoying.com)。为什么侧重于解决了什么问题，而目的则是强调要达成什么效果。不过你也可以在面试中将这两个问题看做是一样的，随便回答。
重试机制主要用来应对偶发性错误，从而提高系统的稳定性和可用性。具体来说，它有以下作用：

首先，重试机制可以提升操作的成功率。它通过多次尝试增加失败操作完成的概率，减少因为一些短暂问题，例如说超时等。

其次，重试机制可以提高用户体验。在用户操作失败时，通过后台重试提高请求完成率，避免直接向用户暴露错误，从而提供流畅的服务体验。比如，用户在线下单时，服务调用失败，重试机制在后台重新发起请求，用户完全没有感知，订单最终也成功了。
举个例子来说，我在实践中最经常用的就是借助重试来提高微服务调用的可靠性，它能有效解决超时带来的问题。

但是在实践中，如果无脑重试就容易出现一个问题：如果超时本身是因为系统负载很高而引发的，那么重试就会进一步加重系统的负担。因此在这种情况下，反而不应该重试。

------------------------------

467. 重试机制适用于哪些场景？, page url: https://www.mianshi.icu/question/detail?id=1000
467-1000-答案：
简单题。

在这个问题之下，你同样可以阐述自己在实践中使用的重试案例来刷亮点。当然，最好的案例还是有些特色的，或者说有一些特殊之处的。
前置知识：
重试机制的核心在于处理瞬时错误，也就是那些可能由于临时性故障、网络波动或资源竞争等原因导致的错误，并且我们预期它在短时间内能够自动恢复。使用重试机制的前提是操作本身是可恢复的，并且副作用是可控的，也就是要保证操作的幂等性。

重试机制有很多适用场景。首先是网络问题，比如网络连接中断或波动导致的请求失败，请求超时但服务端可能仍在处理，稍后重试可能成功，或者 DNS 解析失败等等。

其次，在微服务架构下，服务间调用也可能失败，这也是重试机制的适用场景。例如，服务提供方暂时不可用或负载过高，或者跨服务通信超时。调用第三方 API 失败时也可以使用重试机制，例如调用支付网关、邮件服务等。
我在实践中就设计过一个很有意思的重试机制。这个重试机制是为了解决超时问题的，也就是在超时的时候尝试重试。

但是在实践中，如果无脑重试就容易出现一个问题：如果超时本身是因为系统负载很高而引发的，那么重试就会进一步加重系统的负担。因此在这种情况下，反而不应该重试。

------------------------------

468. 重试机制不适用于哪些场景？, page url: https://www.mianshi.icu/question/detail?id=1002
468-1002-答案：
简单题，一般出现在社招。

在这个问题之下，你可以深入讨论非幂等接口的重试问题，从而引出表单重复提交这个典型场景，并进一步总结出来重试 + 幂等这一个分布式系统中的经典容错机制。
参考重试机制适用于哪些场景？ (meoying.com)

重试机制虽然很有用，但并非适用于所有场景，以下是一些不适用的情况：
重试机制不适用于以下场景。

首先，非幂等操作不适合重试。如果一个操作不是幂等的，也就是说，多次执行会产生不同的结果，就可能导致数据不一致。这时候就需要结合幂等性设计，确保操作的安全性。例如，资金转账、下单、发送短信这些操作，重试都可能导致重复操作，造成数据错误。

其次，永久性错误也不适合重试。如果错误是永久性的，重试是无效的，只会浪费资源。比如参数校验失败，用户提交的参数不符合要求；权限不足，用户没有权限执行操作；硬件故障或服务配置错误，这些都需要人工修复问题，而不是依赖重试。
这里值得探讨的是非幂等接口的重试问题。

理论上来说，要想重试非幂等接口，就要先把接口变成幂等的。比如说表单重复提交问题，解决方案就是先预先生成一个唯一凭证，而后在提交表单的时候就带上这个唯一凭证。
这其实也是分布式系统里面的很重要也很常用的一种保证接口调用成功的机制，也就是重试加幂等。

------------------------------

469. 常见的重试策略有哪些？, page url: https://www.mianshi.icu/question/detail?id=1003
469-1003-答案：
简单题，在校招和社招里面都能遇到。

在这个问题之下，可以结合实践用自己设计的有特色的重试策略来刷亮点。
前置知识：
重试机制的核心在于通过合理设计重试间隔和次数，平衡操作成功率和系统资源消耗。几种常见的重试策略如下：

首先是固定间隔重试，也就是每次重试间隔相同。优点是实现简单，但缺乏灵活性，容易引发“重试风暴”，比较适合错误发生概率稳定且持续时间相对固定的场景，例如定时任务失败后的重新执行。

其次是随机间隔重试，每次重试间隔在一个范围内随机选择。它可以避免多个客户端同时重试，降低“重试风暴”的风险，但重试间隔的随机性可能导致重试不够及时。它比较适合多个客户端同时访问同一个服务，且存在资源竞争的场景，例如分布式锁争抢。
我在实践中用过很多的重试策略，除了这些常见的重试策略之后，我还设计过一个比较独特的重试策略。这个重试机制是为了解决超时问题的，也就是在超时的时候尝试重试。

但是在实践中，如果无脑重试就容易出现一个问题：如果超时本身是因为系统负载很高而引发的，那么重试就会进一步加重系统的负担。因此在这种情况下，反而不应该重试。

------------------------------

470. 什么是进程内重试？, page url: https://www.mianshi.icu/question/detail?id=1004
470-1004-答案：
简单题，在校招和初级工程师面试中可能遇到。

一般来说，在重试的时候，最简单的做法就是进程内重试。所以你可以结合实践来回答自己使用过的进程内重试，这里依旧会使用在重试中一直使用的案例：区分偶发性超时和频繁超时的重试机制。
进程内重试是指在失败之后在当前进程进行重试的机制，很显然，它的优点是简单高效。而缺点则是在系统崩溃的时候，会丢失重试信息，即便系统恢复过来，想要重试也无法继续重试。
进程内重试就是在同一个进程里，如果某个操作失败了，就多试几次，主要用来处理那些可能很快就会自动恢复的错误，比如资源暂时用不了，并发冲突，或者数据还没准备好等等，这样能让程序更稳定，更不容易出错。它主要的目标是快速恢复失败的操作，尽可能提高系统的可用性，同时还不用增加跨进程通信的成本。
正常在实践中，只有在能够容忍重试信息丢失的情况下才会考虑使用进程内重试。举个例子来说，当发起微服务调用超时的时候就可以使用进程内重试。这个重试信息丢失也不要紧，因为大部分业务都是可以失败的。

我就设计过一个很有特色的进程内重试机制，也就是针对微服务调用超时的重试策略。

但是在实践中，如果无脑重试就容易出现一个问题：如果超时本身是因为系统负载很高而引发的，那么重试就会进一步加重系统的负担。因此在这种情况下，反而不应该重试。

------------------------------

471. 什么是跨进程重试？, page url: https://www.mianshi.icu/question/detail?id=1005
471-1005-答案：
简单题。

你要刷亮点，可以从两个角度刷亮点。一方面是你在实践中使用的跨进程重试案例，另外一方面就是讨论跨进程重试类似于分布式任务调度，都要考虑负载均衡以及抢占（分布式锁）之类的问题。
跨进程重试是是指失败发生在一个进程上，但是允许在另外一个进程上进行重试。从这个模式上来说，你很容易就能想到，要想做到这一点，要么是借助进程间通信，比如说 A 进程通知 B 进程重试；要么是将重新信息存储起来，而后别的进程查询到这个重试的信息之后，自己进行重试。

所以在跨进程重试中，最重要的一点就是要选择合适的进程来重试。比如说当你需要重试的时候，有 A B C 三个进程——一般来说三个进程就是三个实例，那么应该让谁来重试？
跨进程重试是为了解决分布式系统中进程间通信的瞬时错误，像网络故障、服务暂时不可用等，而采取的一种容错机制。它通过在客户端重复调用服务端的操作，直到成功或达到预设的限制，来提高系统的可用性和用户体验。
在实践中，跨进程重试也用得很多。比如说我就设计过一个本地消息表的通用解决方案，里面就使用了跨进程重试机制。
跨进程重试里面，有一个很高级的话题，就是跨进程重试本质上就是一个分布式任务调度。

举个例子来说，当 A  发现自己业务执行失败之后，可以将重试信息存储在数据库中。而后另外两个节点 B 或者 C 都可以从这里将重试信息取出来，尝试重试。这其实就是一个分布式任务调度的问题。

------------------------------

472. 进程内重试与跨进程重试的区别和联系？, page url: https://www.mianshi.icu/question/detail?id=1006
472-1006-答案：
简单题。

刷亮点的要点在于指出这两者经常结合在一起使用，并且使用一个实践中的具体案例。
前置知识：

区别：
进程内重试与跨进程重试在实现复杂度、策略选择等方面存在显著差异，但它们同样服务于系统的稳定性和可用性这一核心目标。

进程内重试通常实现起来较为简单。通过循环结构、异常处理机制等编程手段，就能轻松实现。其重试策略也相对直接，例如，固定间隔重试、限制最大重试次数等。进程内重试的优势在于它能迅速响应并处理偶发性失败，避免操作因短暂的异常而直接失败。
我在实践中就设计过两者结合的重试策略。

它的基本原理是，如果在处理业务的时候失败了，那么会立刻尝试重试，此时也就是进程内重试。但是这个重试一般就是重试一次，有些特殊的业务可能是重试两三次。这个过程一般很简单，只需要搞一个循环，检测一下业务执行情况下就可以。

------------------------------

473. 如何评估重试策略的有效性？, page url: https://www.mianshi.icu/question/detail?id=1010
473-1010-答案：
略难的题，难点在于很多人没有评估过，或者不知道怎么评估。

重试策略的有效性评估是很简单的事情，至少比评估熔断、限流、降级等的有效性简单多了。只需要评估在没有重试策略的情况下和有重试策略的情况下，业务执行的成功率就可以了。而后我用了当年我统计不同重试次数的成功率的案例来刷亮点，你照着背就可以了。
评估重试策略很简单，你只需要统计两个东西：
评估这个效果很简单，一般就是统计两个东西。

第一个在没有重试或者没有触发重试的时候，业务成功率。
而且，我以前为了确定重试次数的上限，还统计过不同重试次数的业务成功率。具体来说，当时我统一设计了重试十次，而后分别统计在一次重试就成功的业务占比，两次重试就成功的业务占比等等。

------------------------------

474. 重试策略会对系统造成什么影响?, page url: https://www.mianshi.icu/question/detail?id=1011
474-1011-答案：
简单题，一般出现在社招中。

你在这个问题之下，要刷亮点就可以阐述雪崩问题和重试风暴问题，以及你对应的解决方案。而且，有些时候我会认为面试官问这个问题，可能就是醉翁之意在重试风暴。
重试策略对系统的影响，既有积极的方面，也有消极的方面，关键在于策略是否合理。设计得当的重试策略可以提高系统的可用性及用户体验；但设计不当的重试策略则可能加剧系统负载，甚至引发雪崩效应。

正面影响：
重试策略对系统的影响有好有坏，关键看策略是否合理。

好的影响是重试策略可以提升系统稳定性、可用性及用户体验，还可以简化错误处理等。当然，重试策略也有一些负面影响，我们也需要考虑相应的缓解措施。

首先，重试操作会增加目标服务的负载，尤其是在高并发场景下，不合理的重试策略可能导致目标服务过载甚至崩溃。为了缓解这个问题，我们可以限制重试次数和频率，例如使用指数退避算法，逐渐增加重试间隔；或者使用熔断机制，当失败率达到一定阈值时停止重试，保护下游服务；还可以合理设置超时时间，避免长时间占用资源。
我之前就解决过重试风暴的问题，它是由解决超时问题而引入的重试机制引起的。

在实践中，如果无脑重试就容易出现一个问题：如果超时本身是因为系统负载很高而引发的，那么重试就会进一步加重系统的负担。因此在这种情况下，反而不应该重试。

------------------------------

475. 在微服务架构中进行跨服务调用时，如何高效处理重试？, page url: https://www.mianshi.icu/question/detail?id=1012
475-1012-答案：
简单题。

在面试中其实你可以主动提起这个问题的，比如说在提到自己提高系统可用性的时候，可以综合阐述自己为了提高系统调用的成功率，使用了一些什么样的重试策略，如何设计出来的。

当然，在刷亮点的时候，还是可以使用一直提到的区分可重试不可重试的高端重试策略。
在微服务架构中，进行跨服务调用时处理重试需要综合考虑业务需求、错误类型、系统架构和用户体验，以下是关键点：
在微服务架构中，跨服务调用处理重试需要注意几点：

首先，要识别可重试的错误。瞬态错误（如网络抖动）适合重试，永久性错误（如参数错误）不应重试，服务不可用可以重试但要结合熔断和限流。未知错误要谨慎，最好记录日志和告警。可以通过错误码或异常类型来判断是否需要重试。

其次，幂等性设计很重要，要确保多次调用不会产生副作用。常见方法包括使用唯一请求 ID、状态校验和幂等接口设计。例如，支付服务调用库存服务时，可以用唯一订单 ID 避免重复扣减库存。
我在实践中就设计过一个很有意思的重试机制。这个重试机制是为了解决超时问题的，也就是在超时的时候尝试重试。

但是在实践中，如果无脑重试就容易出现一个问题：如果超时本身是因为系统负载很高而引发的，那么重试就会进一步加重系统的负担。因此在这种情况下，反而不应该重试。

------------------------------

476. 实施重试策略的过程中遇到了哪些问题?如何解决的？, page url: https://www.mianshi.icu/question/detail?id=1014
476-1014-答案：
略难的题，这个一般只会出现在社招了，毕竟校招是没有啥经验的。

在这个问题之下，你要装逼，最佳的方案就是解决重试引发的雪崩和重试风暴问题，其它的都不重要。
在实施重试策略的过程中，通常会遇到以下几个问题：
在实际实施重试策略的时候，有可能会遇到这些问题。

首先是幂等性的问题。对非幂等的接口进行重试，可能会导致数据的重复提交，比如重复下单或者重复扣款。这会带来严重的业务问题。为了解决这个问题，我们可以使用唯一标识符、状态机、乐观锁或者采用 Token 机制来解决。

其次是难以区分瞬态错误与永久性错误。对永久性错误（如参数错误、权限不足）进行重试，实际上是浪费资源且没有意义。为了解决这个问题，我们可以根据错误码或异常类型，区分瞬态错误和永久性错误。维护已知永久性错误的列表，避免对这些错误进行重试。在复杂的场景中，还可以结合机器学习算法，自动识别错误类型，优化重试逻辑。
我在实践中就设计过一个很有意思的重试机制，避免了重试风暴的重试机制。这个重试机制是为了解决超时问题的，也就是在超时的时候尝试重试。

但是在实践中，如果无脑重试就容易出现一个问题：如果超时本身是因为系统负载很高而引发的，那么重试就会进一步加重系统的负担。因此在这种情况下，反而不应该重试。

------------------------------

477. 如何选择合适的重试间隔时间及重试次数？, page url: https://www.mianshi.icu/question/detail?id=1018
477-1018-答案：
略难的题，因为很多人在设计重试的时候，都是凭借感觉、经验随便设置一个重试间隔，以及重试次数。

在这个问题之下，你可以使用我之前做的一个实验，并且总结出来的规律刷亮点。
从理论上来说，选择重试间隔，其实就是选择一个避开了上一次失败原因的时间点。

当然了，上面这个描述非常理想化，因此在实践中都是综合考虑错误类型、业务场景、用户体验和系统资源等因素。以下是详细的选择思路：


选择合适的重试间隔时间和重试次数的核心要点总结：
从理论上来说，选择重试间隔和重试次数的核心原则就一条：避开上一次导致失败的因素。当然这个原则过于抽象了，所以在实践中我都是综合考虑错误类型、业务场景、用户体验和系统资源等因素来决定重试间隔和重试次数。

首先，重试间隔时间决定了每次重试的频率。合理的间隔时间可以平衡系统压力和用户体验。对于瞬态错误，比如网络抖动、超时，适合较短的重试间隔，例如 100 毫秒到 1 秒，因为这类错误可能很快恢复。而对于服务不可用，比如服务重启、依赖故障，需要较长的重试间隔，例如 5 秒到 30 秒，避免频繁重试加重服务端压力。至于永久性错误，比如参数错误、权限不足，则不需要重试，应该直接返回错误。对于实时性高的场景，比如支付、下单，重试间隔应尽可能短，但不能过于频繁，例如 500 毫秒到 2 秒。非实时性场景，比如日志上传、批量任务，可以采用较长的重试间隔，例如 5 秒到 1 分钟。

其次，重试次数决定了系统在失败情况下的容错能力和用户体验。对于瞬态错误，可以设置较多的重试次数，例如 3 到 5 次。对于服务不可用，设置较少的重试次数，例如 1 到 3 次。对于关键业务，比如支付、下单，可以增加重试次数，例如 5 到 10 次，但需要结合合理的间隔时间。而非关键业务，比如日志上传，可以减少重试次数，例如 1 到 3 次，甚至直接放弃重试。同时，要避免过多重试导致用户等待时间过长，应该结合业务 SLA 设置合理的总重试时间。
我之前在工作中，为了搞清楚重试间隔和重试次数在微服务架构下对调用成功率的影响，做过一个实验。

这个实验原理不复杂，就是在接入了业务的重试策略中，接入可观测性，分别统计直接成功、重试一次成功、重试两次成功到重试十次成功的比例。

我就发现了一些规律。

------------------------------

478. 如果重试达到最大次数后仍然失败，应该如何处理？, page url: https://www.mianshi.icu/question/detail?id=1019
478-1019-答案：
简单题。

其实重试失败了之后没啥好说的，除了记录日志并且告警以外，做不了啥东西了。你在面试中可以坦白说这个，我也不觉得谁还能搞出来什么骚操作。
如果重试达到最大次数后仍然失败，说明问题并非暂时性，继续重试会增加系统负担，影响用户体验。 这时应该采取以下措施：
如果重试达到最大次数仍然失败，说明问题不是暂时的，继续重试只会增加系统负担，影响用户体验。这时我会采取以下措施：

首先，停止重试，记录详细的错误信息，包括错误类型、时间、参数、重试次数，以及每次重试的响应信息等等，以便后续分析和排查问题。可以使用日志系统或监控平台来记录这些信息。

然后，及时通知相关人员，通过邮件、短信或报警系统等方式，及时通知相关的开发人员或运维人员，以便他们尽快介入处理。通知内容应该包含关键的错误信息和上下文，例如受影响的用户、订单号等等。

------------------------------

479. 你们公司哪些业务使用了这个本地消息表解决方案？, page url: https://www.mianshi.icu/question/detail?id=1020
479-1020-答案：
简单题。

其实你可以想到，任何需要在执行了业务，并且需要发送消息的场合，都需要这个解决方案。

------------------------------

480. 如何确保重试机制不会导致级联失败？, page url: https://www.mianshi.icu/question/detail?id=1021
480-1021-答案：
略难的题，这个题目一般只会出现在社招里面。

之所以是略难的题，倒不在于重试，而在于你可能不知道什么是级联失败。在这个问题之下，你还是可以用我提供的那个区分是否可重试的重试策略来刷亮点。
首先你要明白级联失败的含义：级联失败是指在一个服务的失败，引起了另外一个服务的失败。在极端的情况下，可能引起整个系统都失败了。

一般来说，重试会引发级联失败多半是因为系统或者服务本身就是处于失败的边缘，而重试就是最后一根稻草。也有很少数的情况是因为无限制重试导致资源消耗过多，导致服务级联失败。

那么可以考虑使用以下措施来预防出现这种情况。
在重试机制中，为了避免级联失败，也就是一个服务的重试行为导致下游服务过载，引发连锁崩溃的情况，我们可以采取以下措施。

首先，要限制重试行为。设置最大重试次数，比如最多三次，避免无限重试。同时，使用指数退避或随机退避策略，逐步拉长重试间隔，减少对下游服务的冲击。我们还可以在客户端或网关层设置全局限流，限制重试请求的总量，防止短时间内大量重试涌入下游服务。

其次，要保护下游服务。可以使用熔断机制。当检测到下游服务失败率超过阈值时，打开熔断器，停止发送请求，直接返回默认值或错误信息。对于永久性错误，比如参数错误、权限不足，要快速失败，避免无意义的重试。还可以使用限流和排队保护下游服务，在服务端设置限流规则，拒绝超出阈值的请求。将重试请求放入消息队列中，按顺序处理，削峰填谷，避免瞬时高并发。
我在实践中就设计过一个很有意思的重试机制，这个重试机制就能够有效防止系统出现级联故障。

但是在实践中，如果无脑重试就容易出现一个问题：如果超时本身是因为系统负载很高而引发的，那么重试就会进一步加重系统的负担。因此在这种情况下，反而不应该重试。

------------------------------

481. 如何避免重试风暴？, page url: https://www.mianshi.icu/question/detail?id=1022
481-1022-答案：
简单题。

重试风暴是一个比较好理解的概念，风暴很容易就让你想到一大堆的重试请求。同样的，这个问题刷亮点还是可以用我提供的那个重试案例。
重试风暴是指出现一大堆重试，这通常是因为重试失败进一步引发重试引起的。

避免重试风暴的关键在于控制重试请求，防止大量请求同时涌入服务端。以下是几种常用的应对策略：


最佳实践：
避免重试风暴的关键在于控制重试请求，防止大量请求同时涌入服务端。我通常会采取几种常用的应对策略。

首先是指数退避。这个策略的原理是每次重试的等待时间呈指数级增长，比如1秒、2秒、4秒、8秒。这样可以有效地拉长重试间隔，降低重试频率，减轻服务端的压力。不过，需要注意的是，这可能会导致等待时间过长，对一些实时性要求高的场景不太适用。此外，还可以在指数退避的基础上加入随机抖动（抖动窗口），例如重试间隔为4秒，抖动范围为正负1秒，那么实际的间隔就是3到5秒之间。这样可以进一步分散重试时间，避免重试请求过于集中，这是对指数退避的一种优化。

其次，我们可以限制并发重试。通过使用并发控制机制（比如信号量）来限制同一时间的重试请求数量。这种方法能够直接控制并发重试的数量，但需要额外的协调机制，通常适合在客户端内部实现。
我在实践中就设计过一个很有意思的重试机制，这个重试机制就能够有效防止出现重试风暴的问题，它主要针对的是服务调用超时触发的超时。

但是在实践中，如果无脑重试就容易出现一个问题：如果超时本身是因为系统负载很高而引发的，那么重试就会进一步加重系统的负担。因此在这种情况下，反而不应该重试。

------------------------------

482. 重试策略与熔断机制的关系，它们如何互相配合？, page url: https://www.mianshi.icu/question/detail?id=1024
482-1024-答案：
略难的题，能够问出这个问题的面试官还是有两把刷子的。

在这个问题下想要刷亮点，可以考虑设计一个同时使用了重试和熔断的服务治理策略。
重试策略和熔断机制是保障分布式系统高可用性的两种重要手段，它们相互配合，共同提升系统的容错能力，但它们的作用方式和关注点有所不同：


配合方式：
在分布式系统中，重试策略和熔断机制是确保系统高可用性的两种关键手段。它们相互配合，共同提升系统的容错能力，但在作用方式和关注点上有所不同。
我之前就设计过一个类似的熔断策略。

------------------------------

483. 重试策略与降级的关系，它们如何互相配合？, page url: https://www.mianshi.icu/question/detail?id=1025
483-1025-答案：
略难的题，能够问出这个问题的面试官还是有两把刷子的。

在这个问题下想要刷亮点，可以考虑设计一个同时使用了重试和熔断的服务治理策略。
重试策略和降级是保障分布式系统高可用性的两种重要手段，它们相互配合，共同提升系统的容错能力，但它们的作用方式和关注点有所不同：

有两种配合方式，而且这两种配合方式很不一样。
在分布式系统中，重试策略和降级是确保系统高可用性的两种重要手段。它们相互协作，共同提升系统的容错能力，但在作用方式和关注点上有所不同。




------------------------------

484. 重试策略与超时控制的关系，它们如何互相配合？, page url: https://www.mianshi.icu/question/detail?id=1026
484-1026-答案：
略难的题，很多面试官会知道超时了可以重试，但是很少会这么问。

在这个问题之下，显然你可以用我之前一再提及的区分可以重试的超时和频繁超时的案例。

重试策略和超时控制是保障分布式系统高可用性的两种重要手段，它们相互配合，共同提升系统的容错能力，但它们的作用方式和关注点有所不同：


配合方式：

就超时后触发重试来说，这个案例装逼重试策略：区分偶发性超时和频繁超时的重试策略最为合适了。
在分布式系统中，重试策略和超时控制是确保系统高可用性的两种关键手段。它们相互配合，共同提升系统的容错能力，但在作用方式和关注点上有所不同。

重试策略是指，当服务调用失败时，我们会尝试重新发起请求，以应对网络抖动、服务暂时不可用等临时性故障。它的目标是尽可能提高请求的成功率。超时控制则是为服务调用设置一个时间限制，超过这个时间后，请求将被终止，即使服务最终可能成功返回。它的目的是防止客户端长时间阻塞，避免资源浪费，并提高系统的响应速度。

这两者的配合方式主要有两个：首先，当服务调用超时时，重试策略可以尝试重新发起请求，避免因为网络延迟等临时性问题导致的调用失败。其次，在重试过程中，每次重试都应该设置超时时间，并且可以根据具体情况进行调整，例如采用指数递增的方式。这样可以避免在重试过程中无限期地等待，从而提高系统的响应速度。
我在实践中就设计过一个很有意思的重试机制，它主要针对的是服务调用超时触发的超时。

但是在实践中，如果无脑重试就容易出现一个问题：如果超时本身是因为系统负载很高而引发的，那么重试就会进一步加重系统的负担。因此在这种情况下，反而不应该重试。

------------------------------

485. 什么是服务注册与发现？, page url: https://www.mianshi.icu/question/detail?id=1027
485-1027-答案：
简单概念题，在校招和初级工程师的社招中有可能遇到。

在这个问题之下，刷亮点有多个角度。第一个角度是提及在服务注册与发现机制中的关键点，例如说健康检查、容错等问题；第二个角度是可以讨论大规模分布式系统下的服务注册与发现会面临一些什么新的问题；第三个角度是讨论自己公司使用服务注册与发现机制，但是需要有特色；最后一个角度则是讨论在大规模的微服务集群下，服务注册与发现面临的问题。

当然，最后一个大规模微服务集群的点，可以进一步引导出来你怎么解决服务注册与发现的问题，刷一个问题排查和性能优化的人设。
服务注册与发现是微服务架构中的一个核心机制，用于解决服务实例动态变化和服务间通信的问题。一句话来说，服务发现要解决的就是我客户端怎么知道服务端在哪里？

它包括两个部分：服务注册和服务发现。从形态上来说，服务注册与发现有非常多种做法，但是在面试中如果没有特别指出，那么默认都是讨论借助注册中心来实现的服务注册与发现，基本的结构如这样：
服务注册与发现是微服务架构里非常重要的一个机制，它主要解决的是服务实例动态变化和服务间通信的问题。它包含两部分：服务注册和服务发现。

服务注册指的是服务提供者在启动的时候，把自己的信息，比如服务名、IP 地址、端口号等等，注册到一个中心化的组件，我们一般叫它服务注册中心。注册中心会维护一个动态的服务实例列表。
从实现上来说，服务注册与发现的关键点之一是心跳问题，也就是说是健康检查问题。

以服务端和注册中心之间的心跳为例，要综合考虑两个问题。

第一个问题是心跳的频率，也就是间隔时间要多长。过长的间隔时间会导致服务端在崩溃之后，注册中心要过很多才能发现。而如果间隔时间过段，那么心跳本身的就会对注册中心、网络带宽产生压力。
在高并发环境下，服务注册与发现面临一些新的挑战。

最典型的问题就是注册中心本身容易成为瓶颈，并且服务于注册和发现过程的健康检查等的通信，会挤占大量的网络带宽，从而影响正常的请求和响应的通信。

例如说在很早之前业界就发现类似于 zookeeper 这种的中间件，并不适合作为大规模微服务集群的注册中心。

------------------------------

486. 为什么需要服务注册与发现？, page url: https://www.mianshi.icu/question/detail?id=1028
486-1028-答案：
简单题，校招和社招都有可能出现。

服务注册与发现其实就是为了让客户端知道有哪些服务端节点，而在分布式系统中，服务端节点是动态变化的，一会多一个一会少一个，所以需要有一个机制能够正确让客户端知道这些服务端节点的变化情况。

因此你如果要刷亮点也可以围绕服务注册与发现在反映节点动态变化上的效果。
在微服务架构中，服务注册与发现是解决服务之间动态通信问题的核心机制，主要原因如下：


那么为了防止你被面试官进一步追问问倒，我需要提醒你一下，理论上来说，直接在客户端配置服务端节点的 IP 和端口都是可行的。但是这种配置没有办法解决一个问题，就是服务端节点是动态变化的。比如说重新部署了服务端，在重新部署的过程中，各个节点先下线，再重新上线。
在微服务架构中，服务注册与发现是解决服务之间动态通信问题的核心机制，原因主要有以下几点：

首先，服务实例的动态变化需要被有效管理。在微服务架构下，服务实例的数量和位置会因为自动伸缩、故障恢复等情况不断变化。如果没有服务注册与发现机制，客户端就需要手动维护所有服务实例的地址，这在实际场景中几乎是不可能完成的。而通过服务注册与发现，服务实例的地址管理实现了自动化，客户端不需要关心服务实例的具体位置，这大大降低了配置管理的复杂度。

其次，它实现了服务提供者和消费者的解耦。服务注册中心在这里扮演了一个中间人的角色，消费者不需要直接依赖服务提供者的地址，而是通过注册中心动态查找服务。这种解耦的设计让系统更加灵活，服务提供者和消费者可以独立部署，同时支持动态扩容和缩容，适应业务需求的变化。
在实践中，服务注册与发现的实时性和稳定性会直接影响微服务架构的稳定性。

举个例子来说，注册中心会和服务端保持心跳，从而判定服务端节点是否还存活。那么如果要是心跳间隔过长，或者是通过连续多次心跳失败才会认定服务端节点已经崩溃，都会导致客户端不能及时收到服务端崩溃的消息。

------------------------------

487. 服务注册与发现的目的和作用是什么？, page url: https://www.mianshi.icu/question/detail?id=1029
487-1029-答案：
简单题，在校招和社招中都有可能遇到。
这个问题和为什么需要服务注册与发现？基本一样，只是需要稍微修改一点点回答的话术就可以。

服务注册与发现的目的是解决微服务架构中服务实例动态变化带来的服务寻址问题，当然一句话来说就是客户端得知道服务端有哪些节点。
服务注册与发现主要就是为了解决微服务架构中服务实例动态变化带来的寻址问题。它的作用主要体现在以下几个方面：

首先，它实现了服务的自动化管理。服务实例上线后会自动注册，下线后自动注销，不需要手动配置，减少了错误和复杂性。

其次，它解耦了服务提供者和消费者。服务消费者不需要知道服务提供者的具体地址，只需要通过服务名就能访问服务，这样就降低了服务之间的耦合度，方便了服务的独立部署和维护。

------------------------------

488. 在微服务架构中，服务注册与发现的完整流程是什么？, page url: https://www.mianshi.icu/question/detail?id=1030
488-1030-答案：
简单题，在校招和社招中都可以说是非常热门的话题了。

在这个问题之下，你可以稍微深入讨论一下服务注册中所谓准备好才能注册的“准备好的”含义，这实际上在微服务框架和使用中都是一个比较棘手的问题。
在微服务架构中，服务注册与发现是两个紧密相关的过程。基本步骤你差不多看这个图就能看出来了：



以下是完整的流程：
在微服务架构中，服务注册和服务发现是两个紧密相关的过程。服务注册就是服务提供者把自己信息注册到注册中心的过程，而服务发现就是服务消费者从注册中心获取可用服务实例信息并完成调用的过程。

服务注册的流程是这样的：服务提供者启动后，会向注册中心，比如 Eureka、Consul、Nacos 等，发送注册请求，包括服务名、IP 地址、端口号，以及一些元数据，比如版本、权重等等。注册中心会把这些信息存储到注册表中。之后，服务提供者会定期向注册中心发送心跳信号，表明自己仍然可用。如果注册中心在规定时间内没收到心跳，就会把这个实例标记为不可用，或者从注册表中移除。当服务实例发生变化，比如扩容、缩容或者故障恢复时，注册中心会实时更新注册表，确保服务消费者始终能拿到最新的服务实例信息。

服务发现的流程是这样的：服务消费者启动时，会配置注册中心的地址，并通过注册中心获取目标服务的实例列表。当服务消费者需要调用某个服务时，会向注册中心发送服务发现请求，注册中心根据服务名查询注册表，返回可用服务实例的列表。然后，服务消费者会根据返回的实例列表，结合负载均衡策略，比如轮询、随机、权重等，选择一个服务实例。最后，服务消费者使用选定实例的地址信息，通过 HTTP、gRPC 等协议发起调用。另外，如果服务消费者缓存了服务实例列表（客户端缓存模式），注册中心会通过推送或定期拉取的方式更新消费者的缓存，确保调用的准确性。
而在实践中，比较难以处理的问题是服务端节点什么时候才应该要注册到注册中心上。理论上来说，服务节点只有在自己启动成功，准备好了之后才应该注册到注册中心上，但是怎么样才是准备好了，这个是一个比较模糊的问题。从实际上来说，大概有三种情况。

第一种也是很多微服务框架使用的语义，就是当节点监听端口成功，就认为准备好了，这时候就会把信息注册上去。

第二种语义是当节点已经监听端口成功，并且通过异步线程或者进程向着这个节点发送健康检查成功之后，才会把信息注册上去。

------------------------------

489. 在微服务架构中，服务注册的流程是怎样的？, page url: https://www.mianshi.icu/question/detail?id=1031
489-1031-答案：
简单题，在校招和社招中都是一个热门的题目。

你可以深入讨论注册时机的问题来刷亮点。
先看在微服务架构中，服务注册与发现的完整流程是什么？，这是子问题。
在微服务架构中，服务注册是指服务提供者将自身信息注册到服务注册中心的过程，目的是让服务消费者能够发现并调用它。这个过程依赖于服务注册中心，并且需要通过心跳机制来维持。具体来说，服务注册的流程是这样的：

首先，服务提供者启动的时候，会读取自身的配置信息，包括服务名、IP 地址、端口号，还有一些其他的元数据，比如版本号、环境等等。

然后，服务提供者会向服务注册中心，比如 Eureka、Consul、Nacos 等发送一个注册请求，把这些配置信息提交给注册中心。注册中心收到请求后，会验证信息的有效性，然后把服务实例信息存储在注册表中。
而在实践中，比较难以处理的问题是服务端节点什么时候才应该要注册到注册中心上。理论上来说，服务节点只有在自己启动成功，准备好了之后才应该注册到注册中心上，但是怎么样才是准备好了，这个是一个比较模糊的问题。从实际上来说，大概有三种情况。

第一种也是很多微服务框架使用的语义，就是当节点监听端口成功，就认为准备好了，这时候就会把信息注册上去。

第二种语义是当节点已经监听端口成功，并且通过异步线程或者进程向着这个节点发送健康检查成功之后，才会把信息注册上去。

------------------------------

490. 在微服务架构中，服务发现的流程是怎样的？, page url: https://www.mianshi.icu/question/detail?id=1032
490-1032-答案：
简单题，在校招和社招中都很热门。

在这个题目之下，你可以深入讨论服务发现过程中的容错问题，主要考虑的就是两个点：注册中心提供了错误的信息以及客户端和服务端节点之间网络不通的两种情况。
先看这个问题，这是它的子问题：

在微服务架构中，服务发现是指服务消费者通过服务注册中心获取目标服务的可用实例信息，并完成服务调用的过程。它依赖于服务注册中心，并通常与负载均衡机制配合使用。以下是服务发现的完整流程：


这里可以深入讨论两个问题。
在微服务架构中，服务发现是指服务消费者通过服务注册中心获取目标服务的可用实例信息，然后完成服务调用的过程。它依赖于服务注册中心，并且通常会和负载均衡机制配合使用。

服务消费者在启动时，会配置服务注册中心的地址，比如 Eureka、Consul、Nacos 等等。这样，消费者就可以通过注册中心获取目标服务的实例信息，而不用硬编码服务提供者的地址。

当服务消费者需要调用某个服务时，它会向服务注册中心发送服务发现请求。这个请求通常包含目标服务的名称（Service Name）。
但是这并不意味着客户端可以完全相信服务发现的信息。

在实践中，客户端需要进一步考虑处理两个问题。

第一个问题是注册中心提供了错误的信息。最常见的情况就是服务端节点 A 崩溃了，但是注册中心没有及时发现导致客户端这边还会把请求转发过去节点 A 上。

------------------------------

491. 在微服务架构中，服务注册的过程中可能遇到哪些问题？如何解决？, page url: https://www.mianshi.icu/question/detail?id=1033
491-1033-答案：
略难的题。

在这个问题之下，你回忆注册中心的三角形模型，那么会出问题的并且和注册过程有关的，就是注册中心本身有问题，服务提供者有问题，注册过程本身有问题。而如果你要想赢得亮点，就可以深入讨论判定服务提供者崩溃这一个难点，由此引发的需要客户端进行容错的问题。
首先你需要了解在微服务架构中，服务注册的流程是怎样的？，其次你需要了解在微服务架构中，服务发现与注册的过程中可能遇到哪些问题？如何解决？，将里面和服务注册过程有关的问题摘出来。
在微服务架构中，服务注册过程中可能会遇到以下问题：

首先是注册中心相关的问题。注册中心可能因为网络中断、服务器宕机等原因导致不可用，这样服务就无法注册或者心跳无法发送。针对这种情况，我们可以采取一些措施，例如集群化部署注册中心、客户端缓存服务实例列表、服务提供者同时注册多个注册中心等。另外，当服务规模很大时，注册中心可能面临性能瓶颈。这时，可以考虑水平扩展增加注册中心节点、优化注册中心缓存机制以及将服务实例分区管理等措施。

其次是网络问题。网络延迟、网络抖动、网络分区等都可能导致服务注册失败或心跳超时。对此，我们可以采用一些方法，例如在服务注册和心跳发送时增加重试机制，确保短暂的网络问题不影响注册；根据网络情况调整心跳超时时间；以及优化网络配置，例如使用更高效的网络协议。
在整个服务注册的过程中，有一个问题非常难以解决，就是注册中心该如何判定服务端已经崩溃了。

实践中绝大多数服务注册与发现模型都是通过心跳机制来判定服务端是否已经崩溃，但是这种判定机制有两个关键点。

第一个要点是避开偶发性的失败，例如说避开因为网络抖动引发的偶发性心跳失败问题。

------------------------------

492. 在微服务架构中，服务发现的过程中可能遇到哪些问题？如何解决？, page url: https://www.mianshi.icu/question/detail?id=1034
492-1034-答案：
略难的题。

在这个题目之下，你可以深入讨论服务发现不一致引发的问题，从而引出客户端容错机制，刷出亮点并且赢得竞争优势。
你需要先了解在微服务架构中，服务发现的流程是怎样的？，而后从在微服务架构中，服务发现与注册的过程中可能遇到哪些问题？如何解决？里面摘出来和服务发现有关的问题。
在微服务架构中，服务发现过程中可能会遇到以下问题：

首先是注册中心相关的问题。注册中心可能因为网络中断、服务器宕机等原因导致不可用，这样服务就无法注册或者心跳无法发送。针对这种情况，我们可以采取一些措施，例如集群化部署注册中心、客户端缓存服务实例列表、服务提供者同时注册多个注册中心等。另外，当服务规模很大时，注册中心可能面临性能瓶颈。这时，可以考虑水平扩展增加注册中心节点、优化注册中心缓存机制以及将服务实例分区管理等措施。

其次是网络问题。服务消费者和注册中心之间的网络延迟或中断都可能导致服务发现失败。我们可以考虑在每个区域部署独立的注册中心、客户端本地缓存服务实例列表、优化网络配置以及启用实时推送机制等措施。
在服务发现过程中遇到的问题，绝大多数都可以通过客户端的容错来解决。

------------------------------

493. 常用的注册中心中间件有哪些？, page url: https://www.mianshi.icu/question/detail?id=1035
493-1035-答案：
简单题。

如果说要回答出来，那么随便记忆两个中间件的特点就可以了。但是要想装逼成功，那么就有两个思路：一个是你们公司使用的注册中心是什么；一个是注册中心选型上的讨论，并且进一步提及在大规模微服务架构下注册中心选型的注意事项。
前置知识：

在微服务架构中，服务注册与发现是实现服务动态管理和通信的核心机制。以下是常见的注册中心中间件：
常用的注册中心中间件有以下几种。

首先是 Eureka。它的优点是轻量级、部署简单，支持自我保护机制，比较适合中小型微服务架构，而且和 Spring Cloud 集成也很好。缺点是数据同步是最终一致性，社区活跃度下降，功能也比较单一。所以，如果你的技术栈是 Spring Cloud，对一致性要求不高，Eureka 是一个不错的选择。

Consul 的优点是强一致性，支持多数据中心、健康检查和分布式配置管理，功能丰富，还提供友好的 Web UI。缺点是部署和运维比较复杂，高并发场景下性能可能受限。它比较适合中大型微服务架构，跨数据中心部署，对一致性和多功能有需求的场景。
而在做出决定的时候，可以综合考虑这些问题：

首先，是一致性和可用性的权衡。如果业务对数据一致性要求很高，比如订单、支付这类金融系统，我们就需要选择强一致性的注册中心，比如 Consul 或 ZooKeeper。但如果业务更关注高可用性和快速响应，比如电商或者推荐系统，那么像 Eureka 或 Nacos 这样偏向高可用的中间件会更合适。

第二个是服务规模和性能需求。如果是小规模系统，比如只有几十个服务实例，其实大多数注册中心都可以满足需求。但如果是大规模系统，比如成百上千个服务实例，就需要选择性能更高、可扩展性更强的中间件，比如 Consul 或 Nacos。而在高并发场景下，像 Eureka 和 Nacos 这种能支持高并发服务注册和发现的中间件会更有优势。
这里可以进一步讨论一下有关一致性和可用性的问题，也就是在注册中心选型中 CAP 应该选哪个的问题。

------------------------------

494. 你知道注册中心吗？, page url: https://www.mianshi.icu/question/detail?id=1036
494-1036-答案：
简单题，在校招和社招中都有可能问出来。

这算是一个深入讨论服务注册与发现、注册中心这些话题的征兆，所以为了起到良好的引导效果，你只需要尽可能提及注册中心的功能、常见注册中心、注册中心选型以及容错等话题。后续就等着面试官深入追问。
注册中心是微服务架构中的核心组件，常用用于实现服务注册与发现。它在分布式系统中扮演“服务目录”的角色，帮助服务提供者和消费者通过动态寻址实现通信，避免硬编码服务地址的问题。以下是注册中心的核心功能和实际应用：

注册中心是微服务架构中非常核心的组件，它主要用来实现服务注册与发现。在分布式系统中，注册中心就像一个“服务目录”，帮助服务提供者和消费者通过动态寻址实现通信，避免了硬编码服务地址的问题。

注册中心的核心功能包括服务注册、服务发现、健康检查和动态扩缩容。

服务注册是指服务提供者在启动后，将自己的实例信息，比如服务名、IP、端口、元数据等，注册到注册中心，这样消费者就可以通过注册中心找到它。

------------------------------

495. 在微服务架构中，如何实现服务的优雅上下线？, page url: https://www.mianshi.icu/question/detail?id=1037
495-1037-答案：
略难的题，注意这边它问的不是如何上下线，而是如何优雅上下线，所以难点就在于优雅两个字。

如果用正确但是没啥用的话来说，就是优雅上线就是你得准备好才能上线，优雅下线就是你得正确结束了才能下线。而在面试中要赢得竞争优势，就可以结合实践讨论自己在优雅上线和优雅下线的时候，做过一些什么事情。

这里我用两个案例，一个是不够高端的在上线之前要提前加载缓存，另外一个是在服务下线过程中要考虑三件事：分布式事务、分布式锁和定时任务，服务只有正确结束了这三者之后才能下线，不然就容易出现数据不一致的问题。
在微服务架构中，优雅上下线的核心目标是避免因服务实例的上线或下线操作导致请求失败、流量中断或数据丢失。以下是实现优雅上下线的关键步骤和机制：


这里我额外提供一个服务上线和一个服务下线的案例，你可以在面试中使用它们来刷亮点，当然也可以用自己的案例。
在微服务架构中，优雅上下线是为了避免服务实例上线或下线时出现请求失败、流量中断或数据丢失等问题。要做到优雅上下线，关键在于以下几个步骤和机制：

服务上线时，要保证新实例能够平稳地接入流量，避免还没准备好就接收请求导致失败。首先，服务实例启动后要完成初始化操作，比如加载配置、连接数据库、初始化缓存等，并通过健康检查，确保实例已经准备好接收流量。然后，等所有初始化和健康检查都完成后，再向服务注册中心注册，避免未准备好的实例被消费者调用。最后，通过动态调整负载均衡权重或灰度发布策略，逐步引入流量，避免瞬时流量冲击导致服务过载。
举个例子来说，在实践中很多应用都使用到了缓存，那么就需要在服务上线之前先加载好缓存，避免刚上线的时候出现大量的缓存未命中，尤其是本地缓存。

其次，在服务下线的时候，事务、锁和定时任务也是要小心的点。并且一般来说微服务架构中使用的事务可能是分布式事务，使用的锁是分布式锁。
和服务优雅上下线密切相关的一个东西，就是网关。

首先，如果网关支持灰度，那么服务优雅上线可以借助灰度来完成一部分事情。举个例子来说，在服务上线的时候要考虑缓存预加载的问题，但是如果要是有灰度配合，那么可以不执行缓存预加载。而是借助灰度过程来完成缓存预加载。

------------------------------

496. 在微服务架构中，如何实现服务的优雅上线？, page url: https://www.mianshi.icu/question/detail?id=1038
496-1038-答案：
略难的题，这里考察的是优雅上线，所以重点不在于服务上线的过程，而在于上线要优雅。

而所谓的优雅，本质上就是你得“准备好”才能上线。你在回答这个问题的时候，可以从两个角度刷亮点：一个是你在实践中解决过的优雅上线的问题，另外一个则是讨论结合网关来做优雅上线。
你需要先阅读在微服务架构中，如何实现服务的优雅上下线？
在微服务架构中，优雅上线是为了避免服务实例上线时出现请求失败、流量中断或数据丢失等问题。要做到优雅上线，关键在于以下几个步骤和机制：
我自己在实践中就解决过很多优雅上线的问题。
服务优雅上线在某些情况下还可以借助网关来达成。

例如说在缓存预加载这个场景中，简单的做法就是服务上线的时候自己去加载一下。那么另外一种做法则是借助灰度来实现缓存预加载，或者叫做预热。

------------------------------

497. 在微服务架构中，如何实现服务的优雅下线？, page url: https://www.mianshi.icu/question/detail?id=1039
497-1039-答案：
略难的题，这里它问的是优雅下线，而不是下线过程，所以侧重点在于怎么做到优雅。

简单来说，就是优雅下线要考虑的就是该结束的过程要结束，该释放的资源要释放，而后才能下线。你在面试中可以结合自己的实践情况来讨论自己在优雅下线过程中处理过什么样的问题。
就是这个问题在微服务架构中，如何实现服务的优雅上下线？的一部分，你需要先看看前面的问题。
在微服务架构中，优雅下线是为了避免服务实例下线时出现请求失败、流量中断或数据丢失等问题。要做到优雅下线，关键在于以下几个步骤和机制：
总的来说，服务的优雅下线过程就是要考虑结束该结束的过程。

而在这个过程中，要小心三个东西：分布式事务、分布式锁和定时任务。

理论上来说，不管是服务下线，还是普通的单体应用下线，都需要考虑事务的问题，只是说在微服务架构下，更有可能遇到分布式事务的问题。分布式事务如果不能正常结束，那么很可能会遇到数据一致性的问题。

------------------------------

498. 如何选择合适的注册中心中间件？, page url: https://www.mianshi.icu/question/detail?id=1040
498-1040-答案：
略难的题，一般出现在社招中，校招不会问，因为这个东西需要有实战经验才能回答好。

要刷亮点，你可以讨论在注册中心选型选 CAP 中哪一个的点。而如果你在项目经历中，或者在简历中提及过自己落地过微服务架构，那么你就可以结合自己的实践经验来聊自己是如何进行技术选型的。
前置知识：

选择合适的注册中心中间件是微服务架构设计中的重要决策，需根据 业务需求、技术栈、团队能力 以及系统的 非功能性需求（如一致性、高可用性、性能扩展等）进行全面评估。以下是选择时的关键考虑因素及常见中间件对比。
在微服务架构中，选择合适的注册中心中间件需要综合考虑以下因素：

首先，是一致性和可用性的权衡。如果业务对数据一致性要求很高，比如订单、支付这类金融系统，我们就需要选择强一致性的注册中心，比如 Consul 或 ZooKeeper。但如果业务更关注高可用性和快速响应，比如电商或者推荐系统，那么像 Eureka 或 Nacos 这样偏向高可用的中间件会更合适。

第二个是服务规模和性能需求。如果是小规模系统，比如只有几十个服务实例，其实大多数注册中心都可以满足需求。但如果是大规模系统，比如成百上千个服务实例，就需要选择性能更高、可扩展性更强的中间件，比如 Consul 或 Nacos。而在高并发场景下，像 Eureka 和 Nacos 这种能支持高并发服务注册和发现的中间件会更有优势。
这里可以进一步讨论一下有关一致性和可用性的问题，也就是在注册中心选型中 CAP 应该选哪个的问题。

在大规模微服务架构下一般来说注册中心应该选择 AP 模型的中间件。这是因为在微服务架构中，服务的数量和复杂度都显著增加，任何一个服务的中断都可能对整个系统产生重大影响。而 AP 模型强调可用性，即使在网络分区或某些节点故障的情况下，也能保证系统继续提供服务，从而提高了系统的整体稳定性和可靠性。

------------------------------

499. 在微服务架构中，如何处理服务实例的动态变化（如上线、下线、故障）？, page url: https://www.mianshi.icu/question/detail?id=1041
499-1041-答案：
略难的题，一般只会出现在社招中。

其实这种问法会让你觉得摸不着头脑，但是如果你把问题换成如果服务实例动态变化了，注册中心和客户端会怎样，就清晰多了。要在这个问题之下刷亮点，赢得竞争优势，你可以讨论客户端容错策略，以及高并发场景下服务实例频繁变化会给注册中心带来庞大的压力这两个点。
在微服务架构中，服务实例的动态变化是常态。为了保障系统的稳定性和可用性，需要构建一套完善的机制来处理这些变化。具体可以从以下三个方面入手：
在微服务架构中，服务实例的动态变化很常见，比如服务实例因为扩容、缩容或者故障而上下线。为了保障系统的稳定性和高可用性，我们需要一套完善的机制来处理这些变化，主要从以下几个方面入手：

首先是服务注册中心的动态管理。注册中心是处理服务实例动态变化的核心，它主要通过心跳检测和实例状态同步来实现。注册中心会定期检测实例的健康状态，如果超时未响应，就会移除实例。注册中心要把实例状态的变化实时同步给消费者。同步方式有推送和拉取两种，在大规模分布式系统场景下，可以采用增量同步、注册中心分区或分层架构等优化策略。

其次是服务消费者侧的容错策略。服务消费者这边需要一些容错策略来应对服务实例的动态变化。比如：服务消费者可以通过缓存和动态更新来应对实例的变化。还可以采用负载均衡策略，比如轮询、加权轮询、一致性哈希等，来分发流量。以及一些容错机制，比如重试机制、熔断机制、降级处理和限流保护等，确保在部分实例不可用时，服务仍然可以正常运行。
在服务实例节点动态变化的时候，最经常遇到的问题就是注册中心不能及时地将最新的状态同步给客户端，因此客户端容错就非常重要。
而在大规模分布式系统下，如果服务节点动态变化频繁，那么会给注册中心带来庞大的压力。

一方面如果变化是服务节点主动上线下线引起的，那么它们就会触发写操作，更新注册中心中注册的信息。

------------------------------

500. 在服务注册与发现中，有哪些常见的服务健康检查方式？, page url: https://www.mianshi.icu/question/detail?id=1042
500-1042-答案：
略难的题，因为很多人只知道心跳，但是不知道其它乱七八糟的健康检查方式。

在这个问题之下，最好的装逼点还是说自己在实践中用过的一些花里胡哨的健康检查方式。
前置知识：

在服务注册与发现中，服务健康检查是确保服务实例可用性的重要机制。通过健康检查，注册中心可以动态感知服务实例的状态变化（如健康、故障、下线等），从而保障消费者调用的服务始终可用。常见的健康检查方式主要有以下两类：


在实际场景中，单一健康检查方式往往不足以全面反映服务状态，因此通常结合多种方式使用，并通过优化策略提升效率和准确性。例如：
在服务注册与发现中，服务健康检查非常重要，它能确保服务实例是可用的。通过健康检查，注册中心可以动态感知服务实例的状态变化，比如健康、故障、下线等等，这样就能保证消费者调用的服务始终可用。常见的健康检查方式主要有两类：主动健康检查和被动健康检查。

主动健康检查是由注册中心或消费者主动发起探测请求，定期检查服务实例的健康状态。常见的实现方式有几种：基于HTTP的检查，就是向服务实例的健康检查端点，比如/health，发送 HTTP 请求，根据返回的状态码，比如 2xx，来判断健康状态。这种方式简单易用，适合 HTTP 服务，但缺点是只能检测服务的基本可达性，没法深入检测内部状态。此外还有基于TCP、gRPC以及自定义脚本等检查实现方式。

被动健康检查是通过监控服务实例的运行状态或调用结果，间接判断健康状况。常见的实现方式包括心跳检测，服务实例定期向注册中心发送心跳信号，如果在规定时间内没收到心跳，就认为实例不可用。这种方式实现简单，适合大规模服务实例监控，但缺点是无法检测服务内部的业务逻辑状态。此外还有请求失败率监控和日志监控等检查实现方式。
我在实践中设计过一个比较有特色的健康检查方式，它本质上是心跳，但是并不是简单的固定间隔的心跳机制。

在使用健康检查来判定服务端节点是否完好的时候，有两个关键点。

第一个要点是避开偶发性的失败，例如说避开因为网络抖动引发的偶发性心跳失败问题。

------------------------------

501. 在微服务架构中，服务发现与注册的过程中可能遇到哪些问题？如何解决？, page url: https://www.mianshi.icu/question/detail?id=1046
501-1046-答案：
略难的问题。

要想回答好这个问题，你需要仔细琢磨注册中心模型中的三个顶点和三条边都有可能出现什么问题，而后逐一分析。

而后你可以进一步结合自己在实践中遇到的和服务注册与发现有关的问题来刷亮点，这里你可以使用我准备的案例。
在微服务架构中，服务发现与注册是核心流程，但在实际运行中可能遇到多种问题。这些问题主要集中在注册中心的高可用性、网络稳定性、服务健康状态、数据一致性、以及负载均衡策略等方面。以下是详细的问题及对应的解决方案：
在微服务架构中，服务发现与注册的过程中可能遇到以下问题：

首先是注册中心相关的问题。注册中心可能因为网络中断、服务器宕机等原因导致不可用，这样服务就无法注册或者心跳无法发送。针对这种情况，我们可以采取一些措施，例如集群化部署注册中心、客户端缓存服务实例列表、服务提供者同时注册多个注册中心等。另外，当服务规模很大时，注册中心可能面临性能瓶颈。这时，可以考虑水平扩展增加注册中心节点、优化注册中心缓存机制以及将服务实例分区管理等措施。

其次是网络问题。网络延迟、网络抖动、网络分区等都可能导致服务注册失败或心跳超时。对此，我们可以采用一些方法，例如在服务注册和心跳发送时增加重试机制，确保短暂的网络问题不影响注册；根据网络情况调整心跳超时时间；以及优化网络配置，例如使用更高效的网络协议。

早期我在实践中就处理过和服务注册与发现的问题。

在服务注册与发现的过程中，经常会遇到两个问题。

第一个问题是注册中心提供了错误的信息。最常见的情况就是服务端节点 A 崩溃了，但是注册中心没有及时发现导致客户端这边还会把请求转发过去节点 A 上。

------------------------------

502. 在微服务架构中，服务注册时注册了什么元数据？, page url: https://www.mianshi.icu/question/detail?id=1047
502-1047-答案：
简单题，在校招和社招中都有可能遇到。

最简单的回答就是注册了服务端节点的定位信息，但是要想刷亮点就要讨论为了支持服务治理而注册的一些额外的信息。最好的装逼方式就是结合自己的牛逼的服务治理方案，讲自己的有特色的服务治理措施是如何利用这些注册的元数据的。
在微服务架构中，服务注册时需要提供一些关键的元数据信息，这些信息有助于服务消费方合理发现并调用服务，同时也有助于服务的管理运维。这些元数据可以大致分为服务识别信息、服务实例地址信息、服务实例状态信息和扩展信息四个方面。以下是常见的关键元数据信息及其作用：
在微服务架构中，服务注册时需要提供一些关键的元数据信息，这些信息不仅帮助服务的消费者能够精准地发现并调用服务，同时也为服务的管理和运维提供了重要支持。通常，这些元数据可以分为以下几类：

首先是服务标识符，它的作用是唯一标识服务，方便服务的发现和管理。主要包括服务名称、服务版本、命名空间或者分组等信息。
举个例子来说，常见的基于权重的负载均衡算法大多数都是通过注册中心来了解不同服务端节点的权重。

例如说有 ABC 三个节点，那么这三个节点在注册的时候，从配置文件或者环境变量中读到自己的权重，而后写入到注册中心。

客户端在执行服务发现的时候就能从注册中心中获得这部分权重信息，用于执行基于权重的负载均衡算法。而即便是动态调整权重的负载均衡算法，也要通过这个过程拿到节点的初始权重。

------------------------------

503. 在微服务架构中，如果一个服务的注册信息过期了，会发生什么？如何应对？, page url: https://www.mianshi.icu/question/detail?id=1048
503-1048-答案：
略难的题，主要是如果你长期只是做增删改查，那么你基本没有机会遇到这种问题，因为大部分时候微服务框架已经帮你解决了。

基本上注册信息过期了就代表客户端拿到了错误的注册信息，所以要么就是调用不通，要么就是流量控制如负载均衡等策略执行有问题。你要想刷亮点，就可以使用自己在实践中使用过的客户端容错措施。
事实上，你要记住一个点，就是注册中心和客户端都不知道注册信息过期了这一回事，它是我们开发者眼中的观点。因此过期了本质上就是服务端没有更新自己注册的信息，那么注册中心上的就是错误的信息了。

在在微服务架构中，服务注册时注册了什么元数据？中区分了三类注册的数据：

导致注册信息过期的常见原因：
在微服务架构中，如果一个服务的注册信息过期了，就意味着客户端会拿到不准确的注册信息，有可能出现调用不通，或者负载不均衡等问题。

而解决方案要从三个角度入手：

在注册中心层面，一方面要考虑引入高可用机制，防止注册中心故障或者高负载导致无法更新注册信息。另外一方面要考虑重新调整心跳机制和健康检查机制，尽快发现服务端节点注册信息的变化。
在注册信息过期这个场景下，引发的大多数问题都可以通过客户端容错来解决。

------------------------------

504. 请解释一下服务注册与发现中的心跳机制。, page url: https://www.mianshi.icu/question/detail?id=1049
504-1049-答案：
简单题，心跳这个东西本身还是很清晰简单的，所以在校招和社招中都有可能遇到。

你在这个问题之下，要刷亮点有两个方向：一个是自己设计的有特色的心跳机制，另外一个则是综合讨论心跳机制在大规模分布式系统下面临的挑战。
服务注册与发现中的心跳机制是用于动态维护服务实例可用性的一种机制。它的主要作用是确保注册中心能够实时感知服务实例的存活状态，从而动态更新服务实例列表。服务实例通过定期向注册中心发送信号，表明自身仍然健康，从而实现注册中心对服务状态的实时感知。
心跳机制是服务注册与发现中非常重要的一个机制，它主要用来动态维护服务实例的可用性。简单来说，就是服务实例定期向注册中心发送心跳信号，告诉注册中心“我还活着”，这样注册中心就能实时掌握服务实例的健康状态，并动态更新服务实例列表。
我在实践中设计过一个比较有特色的健康检查方式，它本质上是心跳，但是并不是简单的固定间隔的心跳机制。

在使用健康检查来判定服务端节点是否完好的时候，有两个关键点。

第一个要点是避开偶发性的失败，例如说避开因为网络抖动引发的偶发性心跳失败问题。
不过，心跳机制在实际应用中也需要一些优化策略，尤其是在大规模分布式系统中。

首先，要合理设置心跳间隔和超时时间。心跳间隔不能太长，否则故障检测的延迟会比较高；也不能太短，否则会增加网络开销。超时时间也要根据网络环境和服务的重要性来设置，避免误判或者延迟检测。

其次，在高并发场景下，可以批量处理心跳信号，减少注册中心的压力。比如，可以将多个心跳请求合并成一个请求，或者采用异步处理的方式。

------------------------------

505. 在服务注册与发现的过程中，为什么需要心跳机制？, page url: https://www.mianshi.icu/question/detail?id=1050
505-1050-答案：
简单题，在校招和社招中都有可能遇到。

在这个问题下要装逼，其实可以讨论一下自己实践中遇到的心跳机制使用不当的问题，以及对应的解决方案。这里我给你提供了一个很简单的因为错误配置心跳间隔引发的线上故障，你如果有合适的案例，也可以使用自己的案例。
在服务注册与发现的过程中，心跳机制的主要作用是动态维护服务实例的可用性状态，确保注册中心能够实时感知服务实例的健康状况。以下是具体原因：
心跳机制在服务注册与发现中扮演着至关重要的角色，它主要负责动态维护服务实例的可用性状态，确保注册中心能够实时了解每个服务实例的健康状况。

具体来说，心跳机制的作用主要体现在以下几个方面：

首先，它能够动态维护服务实例列表。在微服务架构中，服务实例的状态是动态变化的，随时可能因为扩容、缩容、故障或者网络问题而上线或下线。心跳机制通过服务实例定期向注册中心发送心跳信号，表明自己仍然在线，注册中心则根据心跳信息动态更新服务列表，确保服务消费者始终能够调用到可用的服务实例。如果没有心跳机制，注册中心就无法及时感知服务实例的状态变化，服务消费者就可能调用到已经下线或不可用的实例，导致服务调用失败。
之前我遇到过一个心跳机制配置不对引发的问题，这个问题其实很简单。就是同事在配置一个新服务的时候，不小心多加了一个0，导致心跳间隔变成了预期中的 10 倍。

------------------------------

506. 在微服务架构中，如果服务端与注册中心的心跳检测失败，应该如何处理？, page url: https://www.mianshi.icu/question/detail?id=1051
506-1051-答案：
略难的题，一般出现在社招中。

其实大多数的服务注册与发现有关的问题，你都可以从服务端、注册中心和客户端三个角度思考，这个问题也一样。在这个问题之下，你要刷亮点赢得竞争优势，那么可以使用我设计的一个有特色的心跳机制，它就是为了区分偶发性心跳失败而设计的。
在微服务架构中，如果服务端与注册中心的心跳检测失败，可能会导致注册中心误判服务实例不可用，从而将其从服务列表中移除，进而影响服务调用的稳定性。针对这种情况，可以从以下几个方面进行处理：
为了避免这种情况，我们需要从服务端、注册中心和服务消费者三个方面入手，采取一些应对策略。

首先，服务端应对策略。一个比较简单的做法是增加重试机制。当心跳检测失败时，服务实例可以尝试重新发送心跳信号，避免因为短暂的网络问题被误判。当然，重试的间隔和次数需要合理设置。另外，如果服务实例确实要下线，比如手动停机或出现故障，应该在停机前主动注销注册信息，避免注册中心误判。主动健康检查也是一个不错的选择，比如服务实例可以定期检查自身的内存、线程池状态，或者提供一个/health接口给注册中心进行探测。

其次，注册中心应对策略。首先，心跳超时时间的设置要合理，通常建议设置为心跳间隔的三倍。其次，可以引入延迟移除机制。当检测到心跳超时后，先不要立即移除服务实例，而是将其标记为“过渡性不可用”，并尝试重新与服务实例通信。如果通信恢复，就重新标记为可用；如果重试失败，再彻底移除。另外，为了提高注册中心的可靠性，最好采用多节点部署，避免单点故障。在选择注册中心时，还需要根据业务需求选择合适的一致性模型。如果对实时性要求比较高，可以选择最终一致性模型，比如 Eureka，但要注意可能会短暂返回过时的服务实例列表。如果对数据一致性要求比较高，可以选择强一致性模型，比如 Consul。
我在实践中设计过一个比较有特色的健康检查方式，它本质上是心跳，但是并不是简单的固定间隔的心跳机制。

在使用健康检查来判定服务端节点是否完好的时候，有两个关键点。

第一个要点是避开偶发性的失败，例如说避开因为网络抖动引发的偶发性心跳失败问题。

------------------------------

507. 在微服务架构中，如果服务消费者无法从注册中心获取服务实例信息，应该如何处理？, page url: https://www.mianshi.icu/question/detail?id=1052
507-1052-答案：
略难的题，罕见的面试题，校招中几乎不会遇到。

这个问题其实有一个隐含的假设，即服务注册是没有问题的，所以你的回答只需要围绕注册中心和客户端来进行就可以。
在微服务架构中，如果服务消费者无法从注册中心获取到服务实例信息，可能会导致请求失败并影响业务连续性。服务消费者无法从注册中心获取服务实例信息，可能由以下原因导致：
在微服务架构中，如果服务消费者无法从注册中心获取到服务实例信息，那服务调用肯定会出现问题，严重的话还会影响业务的连续性。我们可以从服务消费者和注册中心两个层面来采取应对措施。

服务消费者这边，首先要维护本地缓存，即使注册中心暂时不可用，也可以使用缓存中的服务实例信息。缓存需要定期更新，比如每 5 分钟更新一次。

其次，要引入重试机制，当获取服务实例信息失败时，可以进行重试，但要注意设置最大重试次数和合理的重试间隔。如果重试仍然失败，可以触发降级逻辑，比如返回默认值、跳转到备用服务或者使用本地计算。还可以使用熔断机制，在多次调用失败后快速返回错误，避免浪费资源。

------------------------------

508. 为什么注册中心也支持客户端心跳？如果客户端与注册中心的心跳失败，会有什么问题？如何处理？, page url: https://www.mianshi.icu/question/detail?id=1053
508-1053-答案：
略难的题。

从服务注册与发现的模型上来说，可以认为注册中心和服务端的心跳是必不可少的， 因为需要确保注册中心能够发现服务端已经崩溃。但是客户端和注册中心的心跳看上去就不是很必要，然而在实践中，这个心跳也很常见。
其实说一些注册中心是保守的做法，就我个人接触到的服务注册与发现模型来说，大部分都有客户端到注册中心的心跳机制。

在微服务架构中，注册中心不仅需要维护服务提供者的状态，有时还可能需要维护服务消费者（客户端）的状态。一些注册中心（如 Consul、Nacos）支持服务消费者主动向注册中心汇报心跳，其主要目的包括：

如果客户端（服务消费者）与注册中心的心跳失败，并不会像服务提供者心跳失败那样直接导致服务不可用，但仍然会带来一些问题：
在微服务架构里，注册中心通常用来维护服务提供者的状态，但有时也需要维护服务消费者的状态。像 Consul、Nacos 这样的注册中心就支持服务消费者主动汇报心跳。这样做主要有几个好处：

首先，可以保持消费者和注册中心的连接，减少连接建立的开销，同时让注册中心能够及时感知消费者的在线状态。其次，通过心跳，消费者可以快速发现注册中心是否可用，以便及时采取应对措施。另外，一些注册中心的高级特性也依赖于心跳机制，比如，有些注册中心可以根据心跳信息将服务变更主动推送到消费者，减少消费者轮询注册中心的频率。

如果客户端（也就是服务消费者）与注册中心的心跳失败了，虽然不像服务提供者心跳失败那么严重，不会直接导致服务不可用，但还是会带来一些麻烦。一个是无法及时感知服务变更。消费者可能继续调用已经失效的实例，或者无法发现新增的实例。另一个是注册中心无法掌握消费者的状态，比如无法准确统计在线消费者数量，这会影响监控和管理。此外，一些依赖心跳的高级特性也会失效。

------------------------------

509. 你们公司哪些业务使用了这个本地消息表解决方案？, page url: https://www.mianshi.icu/question/detail?id=1054
509-1054-答案：
简单题。

其实你可以想到，任何需要在执行了业务，并且需要发送消息的场合，都需要这个解决方案。
判定能不能用这个方案的标准很简单，你的业务里面有一个本地事务，而后还要发送一条消息。只要满足这两个条件，你就可以用这个方案。

这里罗列一些适合使用本地消息表的业务场景：
我们公司的新业务已经强制要求接入这个项目了，不允许自己再开发解决方案。而老的业务也是在改造过程中，尤其是一些自研的本地消息表不稳定、经常出问题的业务，也要求切换了。

------------------------------

510. 在这个项目里面，你观测了什么数据？, page url: https://www.mianshi.icu/question/detail?id=1055
510-1055-答案：
简单题。

在这个问题之下，你可以通过前后对比来凸显自己在可观测性上取得的成就。前就是没有你这个统一方案的时候，各个业务方自己的方案错漏百出，一大堆的问题。后就是接入你这个方案之后给他们带来便利。
在这个面试项目中，观测的数据主要是两块：

从记录的内容上来说，包括：
在这个项目中，我们重点观测的数据主要分为两大块：首先是本地事务的执行情况，包括事务是否成功执行以及执行后的消息发送状态；其次是补偿任务的执行情况，确保在事务处理中可能出现的问题能够得到及时且有效的处理。

为了全面记录这些关键信息，我们采用了多种技术手段。从记录的内容上来说，主要涵盖了日志，trace 和 metric 数据。

日志主要是提供排查错误的细节信息，通过详尽的日志记录，我们可以追踪到事务执行和消息发送的每一个细节，为问题的排查提供了有力的依据。
实际上，当初推动我们提供这样一个解决方案的动机之一就是统一对本地消息表的监控。

------------------------------

511. *什么是OSI七层模型？, page url: https://www.mianshi.icu/question/detail?id=1056
511-1056-答案：
简单题，高频题，校招和初级岗位面试中常见。

OSI七层模型（Open Systems Interconnection Model，开放系统互联模型）是由国际标准化组织（ISO）制定的标准化的网络通信参考模型，用于指导不同系统之间的通信。它将网络通信过程划分为七个层次，每一层都有特定的功能和协议，确保数据在网络中可靠传输。
OSI七层模型，也就是开放系统互联模型，是由国际标准化组织制定的网络通信参考模型，主要是在不同系统之间实现网络通信的标准化。它把整个通信过程拆分成了七个层次，每层都有各自的功能和对应的协议，确保数据能在网络里进行可靠的传输。

最顶层是应用层，这里直接面向用户，为应用程序提供网络服务接口，需要处理用户和网络之间的交互，比如使用 HTTP、HTTPS、DNS等协议，提供网页浏览、文件传输、域名解析等服务。

再往下一层是表示层，主要负责数据格式转换，比如加密、解密、压缩、解压缩，目标是让接收方能够正确理解数据格式，也能确保不同系统之间的数据兼容。

------------------------------

512. *什么是TCP/IP四层模型？, page url: https://www.mianshi.icu/question/detail?id=1057
512-1057-答案：
简单题，高频题，校招和初级岗位面试中常见。
TCP/IP 四层模型是实际互联网通信的参考标准，TCP/IP模型将网络通信分为四个层次，每一层都有特定的功能和协议，协同完成数据的传输。

从上到下，TCP/IP模型分为以下四层：
TCP/IP 四层模型是互联网通信里事实参考标准。它把网络通信按功能分成四层，每层都有自己的职责和协议，相互配合完成数据传输。

最上层叫做应用层，这一层直接和用户打交道，负责处理像文件传输、电子邮件、网页浏览这类服务。比如我们平时用的 HTTP、FTP、SMTP、DNS 和远程登录协议 SSH 都归在这一层。

接下来是传输层，它主要负责在两个主机之间建立可靠的通信，确保数据不会乱序出现或者丢失。我觉得这里最常用的就有 TCP 和 UDP，前者重视可靠性，后者则追求传输速度更快一些。

------------------------------

513. *OSI七层模型和TCP/IP四层模型的区别是什么？, page url: https://www.mianshi.icu/question/detail?id=1058
513-1058-答案：
简单题，校招和初级岗位面试中常见。
前置知识：

OSI七层模型和TCP/IP四层模型都用于描述网络通信的过程，但它们在抽象程度、设计目标以及实际应用上存在明显差异。下面是对两者区别的综合说明：


举例说明
OSI 七层模型和 TCP/IP 四层模型都是用来描述网络通信过程的，但它们在抽象程度、设计目标和实际应用上有明显不同。

OSI 模型将整个通信过程划分成七个层次：物理层、数据链路层、网络层、传输层、会话层、表示层和应用层。这样的分层方式非常细致，方便我们在理论上对网络的各种功能进行教学和讨论。而 TCP/IP 模型则更贴近现实，把整个过程简化为四个层次：网络接口层、网际层（也叫因特网层）、传输层和应用层。实际上，OSI 模型中的会话层和表示层的功能都被合并到了 TCP/IP 的应用层，而物理层和数据链路层则合称为网络接口层，这样的设计更符合当前互联网协议的实际实现和业务需求。

从设计理念上讲，OSI 模型追求的是严格的分层，每一层都有非常明确的职责和接口标准，所以更多用作理论上的指导和教学参考，这种做法虽然标准化，但在实际部署中往往显得过于理想化。相比之下，TCP/IP 模型是根据互联网协议实际演进形成的，侧重于实用性和高效性，虽然层次划分比较粗，但它更真实地反映了各个网络协议在实际通信中的作用，因此也成为了目前互联网中广泛应用的事实标准。

------------------------------

514. *什么时候用TCP？, page url: https://www.mianshi.icu/question/detail?id=1059
514-1059-答案：
简单题，校招和初级岗位面试中常见。
前置知识：
TCP协议是面向连接的，提供可靠的数据传输，保证数据包的顺序性和完整性。因此，常用在需要数据可靠传输的场景，例如网页浏览、电子邮件、文件传输等。
随着移动互联网和IoT（物联网）的普及，TCP的面向连接特性已成为劣势。例如，当移动端设备频繁切换网络（如从Wi-Fi到移动网络，再切换回Wi-Fi）时，通过IP地址+端口来标识连接的TCP协议，需要重新建立连接，这将导致延迟增加或连接中断。

------------------------------

515. *什么时候用UDP？, page url: https://www.mianshi.icu/question/detail?id=1060
515-1060-答案：
简单题，校招和初级岗位面试中常见。
前置知识：
UDP是无连接的，传输速度快、延迟低，但不可靠，适用于对实时性要求高且可容忍少量丢包的应用，如视频会议、在线游戏、DNS查询和VoIP。
随着移动互联网和物联网的普及，对传输层协议提出了更高的要求，不仅需要较好的实时性，还需兼具可靠性和安全性。为此，基于UDP的QUIC协议应运而生。QUIC结合了UDP的高效性、TCP的可靠性和TLS的安全性，解决了传统TCP协议的队头阻塞和握手延迟等问题，同时支持连接迁移等特性，使其更适应移动互联网的复杂网络环境。作为HTTP/3的基础协议，QUIC已被广泛应用于国内外大厂中，大幅提升了网页加载速度和视频播放的流畅性，显著改善了用户体验。

------------------------------

516. *说说什么时候用TCP？什么时候用UDP？, page url: https://www.mianshi.icu/question/detail?id=1061
516-1061-答案：
简单题，校招和初级岗位面试中常见。
前置知识：
UDP和TCP各有其独特的特点和适用场景。

UDP是无连接的，传输速度快、延迟低，但不可靠，适用于对实时性要求高且可容忍少量丢包的应用，如视频会议、在线游戏、DNS查询和VoIP。
随着移动互联网和物联网的普及，对传输层协议提出了更高的要求，不仅需要较好的实时性，还需兼具可靠性和安全性。为此，基于UDP的QUIC协议应运而生。QUIC结合了UDP的高效性、TCP的可靠性和TLS的安全性，解决了传统TCP协议的队头阻塞和握手延迟等问题，同时支持连接迁移等特性，使其更适应移动互联网的复杂网络环境。作为HTTP/3的基础协议，QUIC已被广泛应用于国内外大厂中，大幅提升了网页加载速度和视频播放的流畅性，显著改善了用户体验。

------------------------------

517. *TCP 是用来解决什么问题的？, page url: https://www.mianshi.icu/question/detail?id=1062
517-1062-答案：
简单题，校招和初级岗位面试中常见。
TCP旨在解决在不可靠的网络（如IP网络）上提供可靠数据传输的问题。它通过一系列机制确保数据在传输过程中的可靠性、完整性和有序性，具体解决以下关键问题：
在网络通信里，TCP就是为了解决“如何在像IP网络这样不可靠的网络环境下，提供一个可靠的数据传输通道”的问题。它通过一系列机制来确保数据在传输过程中的可靠性、完整性和有序性。

首先，TCP 解决的是数据传输的可靠性问题。IP 网络本身不保证数据包一定能送达，可能会出现数据包丢失的情况。TCP 通过确认应答（ACK）和超时重传机制来解决这个问题。IP 网络也不保证数据包的送达顺序，数据包到达接收端的顺序可能和发送时的顺序不一样。TCP 通过序列号和排序机制来解决这个问题。由于网络延迟或者重传机制，同一个数据包可能会多次到达接收端。TCP 通过序列号和去重机制来解决这个问题。

其次，TCP 也关注数据完整性。它在每个数据包中都包含校验和，用来检测数据在传输过程中是否发生错误。

------------------------------

518. *到底什么是TCP连接？, page url: https://www.mianshi.icu/question/detail?id=1063
518-1063-答案：
简单题，校招和初级岗位面试中常见。
前置知识：
TCP连接是通信双方（通常是客户端和服务器）之间基于TCP协议建立的一种虚拟的全双工通信管道。通信双方可以同时通过该“管道”进行数据传输。该虚拟管道借助TCP协议提供的一系列机制来保证数据的可靠传输，包括确认应答、超时重传、校验和、流量控制以及拥塞控制等。通信双方在通信开始前，通过三次握手来建立该虚拟管道。在通信结束后，通过四次挥手来关闭该虚拟管道。

------------------------------

519. *TCP是面向连接的，这句话的意思是什么？, page url: https://www.mianshi.icu/question/detail?id=1064
519-1064-答案：
简单题，校招和初级岗位面试中常见。
前置知识：
“TCP是面向连接的”这句话，简单来说就是指在发送和接收数据之前，通信双方（客户端和服务器）必须先通过三次握手建立一个TCP连接，以确保彼此都准备就绪、同意使用TCP进行通信。

建立TCP连接后，双方会在操作系统中维护这个连接的状态信息，包括：连接的标识五元组（源IP、源端口、目的IP、目的端口、协议类型）、序列号和、确认号、窗口大小及计时器等。并采取确认应答、超时重传、校验和、流量控制、拥塞控制等机制确保数据的完整、可靠、有序传输。

------------------------------

520. *TCP三次握手和四次挥手的详细过程是什么？为什么需要这样设计？, page url: https://www.mianshi.icu/question/detail?id=1065
520-1065-答案：
简单题，在校招和初级工程面试中，概率极大。

前置知识：
TCP三次握手是指建立一个可靠的TCP连接的过程。它的主要目的是让通信双方（客户端和服务器）确认彼此的接收和发送能力，并协商一些关键参数，确保数据传输的可靠性。

最开始，服务端要启动之后，要监听某个端口。

当客户端准备连接服务端的时候，发送 SYN 报文，并且带上自己的初始化序列号（ISN），而后进入 SYN－SENT 状态。这是第一次握手。
从这个过程上也可以看到，三次握手这个过程虽然可靠性很强，但是性能很差。一方面三次握手过程增加了连接建立的延迟，尤其是在网络状况不佳的情况下；另外一方面是每次建立连接都需要消耗系统资源（如文件描述符、内存等）。
当然，我们也可以从一个更加本质的角度去理解三次握手和四次挥手的过程。

对于 TCP 连接来说，初始化的时候有两个关键的点：确认网络是连通，同步初始化序列号，即 ISN。

TCP 是一个全双工通信的协议，这就意味着服务端要确认客户端的 ISN，客户端也要确认服务端的 ISN。因此从设计上来说是四个步骤：客户端发送 SYN 和 ISN，服务端确认，服务端发送 SYN 和 ISN，客户端确认。只不过这逻辑上的第二和第三个步骤，可以合并为一个步骤，也就是服务端在确认的时候，也顺手把自己的 ISN 返回去了。

------------------------------

521. 什么是TCP的三次握手和四次挥手？, page url: https://www.mianshi.icu/question/detail?id=1066
521-1066-答案：
简单题，在校招和初级工程面试中，概率极大。尤其是如果你是一个没啥面试项目的应届生，那么差不多就是必问题了。

在这个问题之下，装逼的最佳方式有两个，一个是讨论三次握手机制意味着 TCP 创建连接非常麻烦，所以需要池化技术；另外一个是讨论为啥恰好是三次握手，而不是两次握手，也不是四次握手。
TCP三次握手和四次挥手的详细过程是什么？为什么需要这样设计（mianshi.icu）？的另一种问法。
TCP三次握手是指建立一个可靠的TCP连接的过程。它的主要目的是让通信双方（客户端和服务器）确认彼此的接收和发送能力，并协商初始的序列号，确保数据传输的可靠性。

最开始，服务端要启动之后，要监听某个端口。

当客户端准备连接服务端的时候，发送 SYN 报文，并且带上自己的初始化序列号（ISN），而后进入 SYN－SENT 状态。这是第一次握手。
从这个过程上也可以看到，三次握手这个过程虽然可靠性很强，但是性能很差。一方面三次握手过程增加了连接建立的延迟，尤其是在网络状况不佳的情况下；另外一方面是每次建立连接都需要消耗系统资源（如文件描述符、内存等）。
当然，我们也可以从一个更加本质的角度去理解三次握手和四次挥手的过程。

对于 TCP 连接来说，初始化的时候有两个关键的点：确认网络是连通，同步初始化序列号，即 ISN。

TCP 是一个全双工通信的协议，这就意味着服务端要确认客户端的 ISN，客户端也要确认服务端的 ISN。因此从设计上来说是四个步骤：客户端发送 SYN 和 ISN，服务端确认，服务端发送 SYN 和 ISN，客户端确认。只不过这逻辑上的第二和第三个步骤，可以合并为一个步骤，也就是服务端在确认的时候，也顺手把自己的 ISN 返回去了。

------------------------------

522. *TCP如何建立连接？, page url: https://www.mianshi.icu/question/detail?id=1067
522-1067-答案：
简单题，在校招和初级工程面试中，概率极大。尤其是如果你是一个没啥面试项目的应届生，那么差不多就是必问题了。

在这个问题之下，装逼的最佳方式有两个，一个是讨论三次握手机制意味着 TCP 创建连接非常麻烦，所以需要池化技术；另外一个是讨论为啥恰好是三次握手，而不是两次握手，也不是四次握手。
详细介绍一下 TCP 的三次握手机制？的另一种问法。
TCP三次握手是指建立一个可靠的TCP连接的过程。它的主要目的是让通信双方（客户端和服务器）确认彼此的接收和发送能力，并协商一些关键参数，确保数据传输的可靠性。

最开始，服务端要启动之后，要监听某个端口。

当客户端准备连接服务端的时候，发送 SYN 报文，并且带上自己的初始化序列号（ISN），而后进入 SYN－SENT 状态。这是第一次握手。
从这个过程上也可以看到，三次握手这个过程虽然可靠性很强，但是性能很差。一方面三次握手过程增加了连接建立的延迟，尤其是在网络状况不佳的情况下；另外一方面是每次建立连接都需要消耗系统资源（如文件描述符、内存等）。
当然，我们也可以从一个更加本质的角度去理解三次握手的过程。对于 TCP 连接来说，初始化的时候有两个关键的点：确认网络是连通，同步初始化序列号，即 ISN。

------------------------------

523. *建立TCP连接时，如果发送SYN后就宕机了，之后会发生什么？, page url: https://www.mianshi.icu/question/detail?id=1068
523-1068-答案：
简单题，在校招和初中级工程师岗位中有可能遇到。
前置知识：
如果客户端在发送 SYN 报文后立即宕机，可能会发生以下两种情况：

如果服务端因某些原因没有收到SYN报文，那么什么也不会发生。客户端和服务端之间不会建立连接。

如果服务器收到客户端的 SYN 报文，那么服务端会回复 SYN-ACK 报文并进入 SYN_RCVD 状态（即半连接状态），等待客户端发送 ACK 报文以完成三次握手。由于客户端宕机，服务器将无法收到客户端的 ACK 报文。

------------------------------

524. *TCP的三次握手可以改成两次吗？为什么？, page url: https://www.mianshi.icu/question/detail?id=1069
524-1069-答案：
简单题，在校招和初中级工程师岗位中有可能遇到。
前置知识：

等价问题：

三次握手的核心目的是确认双方都具备发送和接收数据的能力，并协商状态参数，从而建立可靠的全双工通信信道（即TCP连接）。如果改成两次握手，会导致以下问题：
不可以改为两次。

三次握手的核心目的是确认双方都具备发送和接收数据的能力，并且能够协商好各自的状态参数，这样才能真正建立一个可靠的全双工通信通道。

要是只用两次握手，只建立客户端到服务端的单向通信信道，无法建立服务端到客户端的单向通信信道。因为服务器没办法知道客户端是否收到了自己的 SYN-ACK 报文，一旦客户端根本没准备好接收数据，服务器这边贸然发数据过去就有可能丢失或导致连接异常。

------------------------------

525. *TCP如何释放连接？, page url: https://www.mianshi.icu/question/detail?id=1070
525-1070-答案：
简单题，在校招和初中级工程师岗位中有可能遇到。
前置知识：
TCP释放连接的过程通常被形象地称为四次挥手，通信双方（客户端和服务器）通过四个步骤来安全地终止连接的过程。它的目的是确保双方都能确认数据传输已经完成，并释放资源。

第一次挥手：客户端发送一个FIN包，表示客户端没有数据发送了，希望关闭连接。这个包的序列号seq = x，FIN标志位为1。而后客户端进入 FIN_WAIT_1 状态。

第二次挥手：服务端收到FIN包后，发送一个ACK（确认）包，确认收到客户端的FIN包。这个包的确认号ack = x + 1，表示期望收到客户端的下一个序列号。这个时候服务端会进入 CLOSE_WAIT 状态。客户端收到服务端的 ACK 报文，进入了 FIN_WAIT_2 状态。
TCP连接是全双工的，意味着数据可以在两个方向上独立传输。因此，每个方向的连接都需要单独关闭。第一次和第二次挥手关闭了客户端到服务端的连接，第三次和第四次挥手关闭了服务端到客户端的连接。如果只进行三次挥手，可能会导致其中一个方向的数据未完全传输完毕就被关闭。

------------------------------

526. *TCP四次挥手如果没有最后一次会怎么样？会导致什么后果？, page url: https://www.mianshi.icu/question/detail?id=1071
526-1071-答案：
简单题，在校招和初中级工程师岗位中有可能遇到。
前置知识：

如果TCP四次挥手过程中缺少最后一次ACK（即主动关闭方对被动关闭方FIN的确认），将导致以下后果：


总结
如果在 TCP 四次挥手的过程中缺少最后一次 ACK，也就是说主动关闭方没有确认被动关闭方的 FIN，那么两边的连接状态就会出现差异。主动关闭方会觉得自己已经把该做的事情都做完了，很可能已经释放了资源；而被动关闭方却还在等那个 ACK，认为连接并没有彻底关闭。这样一来，就有可能引发后续的通信混乱，比如被动关闭方还会不断重传 FIN 或者尝试其他操作，而主动关闭方根本就不再理会。

更糟糕的是，被动关闭方因为没有真正收到 ACK，会一直待在 LAST_ACK 状态，占用系统资源，而且还要不断重传 FIN，这样会带来额外的网络开销。要是连接数量多起来，这些没能完成关闭的连接就会累积，可能导致资源紧张甚至影响服务器稳定性。

------------------------------

527. *为什么TCP四次挥手需要有 TIME_WAIT 状态?, page url: https://www.mianshi.icu/question/detail?id=1072
527-1072-答案：
简单题，在校招和初中级工程师岗位中有可能遇到。
前置知识：

TIME_WAIT状态是TCP四次挥手断开连接过程中主动关闭方的最后一个状态，它存在的核心是为了保证连接的可靠关闭，避免旧连接的数据包干扰新连接。具体原因如下：
TIME_WAIT 状态是出现在 TCP 四次挥手断开连接时，主动关闭的一方在发送最后的 ACK 之后，会进入的一个等待状态。它存在的主要目的，是为了确保连接能够被可靠地关闭，并且避免旧连接的数据包影响到新连接。

首先，在四次挥手的最后一步，主动关闭方会发送一个 ACK 给被动关闭方。如果这个 ACK 因为网络原因丢失，而主动关闭方又过早地释放了连接并复用了端口，那么被动关闭方重发的 FIN 可能会到达一个已经被复用的端口，导致原本处于关闭状态的连接出现混乱。为了解决这个问题，主动关闭方在发送 ACK 之后会进入TIME_WAIT状态，并等待 2×MSL（报文最大生存时间）再彻底释放连接，这样可以留出足够的时间让对方重传 FIN 并再次收到 ACK。

------------------------------

528. *除了四次挥手，TCP释放连接的方式还有哪些？, page url: https://www.mianshi.icu/question/detail?id=1073
528-1073-答案：
简单题，在校招和初中级工程师岗位中有可能遇到。
除了标准的四次挥手，TCP释放连接的方式还包括以下几种：
在 TCP 里，除了最常见的四次挥手，还有几种比较特殊的方式来释放连接。比如说，如果通信过程中出现严重错误（像端口未打开或连接异常），一方会发送 RST 报文来强制结束连接。它的特点是能够马上生效，不需要四次挥手，但往往用于异常情况下的快速释放，也有可能造成数据丢失。

还有就是是超时关闭，如果 TCP 连接在一定时间里毫无响应，或者在建立连接时就长时间没反应，TCP协议会因为超时自动释放连接。具体的超时时间往往由操作系统决定，比较常见于对方主机崩溃、网络出现严重故障或者 Keepalive 探测一直得不到回应等情况。

------------------------------

529. *TCP的可靠传输是如何保证的？, page url: https://www.mianshi.icu/question/detail?id=1074
529-1074-答案：
简单题，在校招和初中级工程师岗位中有可能遇到。
TCP 的可靠传输是通过一系列机制共同作用来保证的，主要包括：
TCP 的可靠传输主要依赖于多种机制共同配合。

首先，TCP 通过三次握手机制来确保通信双方的发送和接收能力都处于正常状态，而数据传输完成之后，则会通过四次挥手来优雅地释放连接，保证资源能够被正确回收和释放。

其次，序列号和确认应答 (ACK) 能够让接收方告知发送方哪些数据已经成功接收，如果超时没有收到 ACK，发送方就会重传相应的数据。为了避免发送方等到超时才重传，TCP 还引入了快速重传机制，一旦接收方收到失序的数据包，就会立刻发送重复 ACK，当发送方累积收到 3 个重复 ACK 时，就能迅速重传丢失的数据包。

------------------------------

530. *TCP是如何保证可靠传输的？, page url: https://www.mianshi.icu/question/detail?id=1075
530-1075-答案：
简单题，在校招和初中级工程师岗位中有可能遇到。
TCP的可靠传输是如何保证的（mianshi.icu）？的另一种问法。
TCP 的可靠传输主要依赖于多种机制共同配合。

首先，TCP 通过三次握手机制来确保通信双方的发送和接收能力都处于正常状态，而数据传输完成之后，则会通过四次挥手来优雅地释放连接，保证资源能够被正确回收和释放。

其次，序列号和确认应答 (ACK) 能够让接收方告知发送方哪些数据已经成功接收，如果超时没有收到 ACK，发送方就会重传相应的数据。为了避免发送方等到超时才重传，TCP 还引入了快速重传机制，一旦接收方收到失序的数据包，就会立刻发送重复 ACK，当发送方累积收到 3 个重复 ACK 时，就能迅速重传丢失的数据包。

------------------------------

531. *TCP滑动窗口的作用是什么？, page url: https://www.mianshi.icu/question/detail?id=1076
531-1076-答案：
简单题，在校招和初中级工程师岗位中有可能遇到。
TCP 滑动窗口（Sliding Window）是 TCP 协议中用于流量控制与拥塞控制的关键机制，它能够动态管理发送方可持续发送的数据总量，从而确保在不同网络条件下都能达到高效且可靠的传输效果。它的主要作用和特点可概括如下：
TCP 滑动窗口机制主要用于实现流量控制和拥塞控制。它能帮助发送方动态管理可持续发送的数据总量，既能确保在网络顺畅时保持较高吞吐量，也能够在接收方处理不过来或者网络拥塞时，及时收缩速率，从而实现更高的传输效率与可靠性。

首先，流量控制主要是由接收方主导的。接收方通过ACK报文告诉发送方自己当前的缓冲区还有多大的容量，也就是所谓的窗口大小。发送方根据这个信息来控制发送数据的量。这样一来，就能有效地防止接收方因为处理能力不足而导致数据被淹没的情况发生。比如说，如果接收方的缓冲区已经满了，它就会把窗口大小调小甚至设为零，这样发送方就会暂停发送数据，直到接收方有处理能力为止。

其次，拥塞控制主要是为了感知网络的状态，并根据网络的状况来动态地调整数据发送的速率。发送方会结合接收方提供的窗口大小和当前网络拥塞情况下的拥塞窗口大小来决定实际发送的数据量。具体来说，发送方会取这两个窗口中的较小值作为实际的发送窗口。在网络状况良好的情况下，发送方会逐步扩大窗口大小，从而提高吞吐量；而如果检测到丢包或者延迟增加的情况，发送方则会迅速收缩窗口大小，以防止网络进一步拥塞。

------------------------------

532. *TCP超时重传机制是为了解决什么问题？, page url: https://www.mianshi.icu/question/detail?id=1077
532-1077-答案：
简单题，在校招和初中级工程师岗位中有可能遇到。
TCP 超时重传机制（Retransmission Timeout，RTO）主要是为确保数据包在可能丢失或延迟过长的网络环境中依然能够被可靠送达接收方，从而实现 TCP 的可靠传输目标。具体来说，它旨在解决以下核心问题：
TCP 超时重传机制最核心的目的是在不稳定的网络环境下，保证数据包能够按预期到达接收方，避免发生数据包丢失却无人察觉的情况。因为在网络中，数据包可能会因为各种故障或拥塞被丢弃，或者虽然成功到了接收方，但回送的 ACK 又在途中丢失，如果发送方一直收不到确认，就会进入超时重传流程，把原来的数据包再发一遍，直到对方真的收到并确认为止。

TCP 超时重传机制一大好处是它能够根据不同的网络条件进行动态调整。网络时延和拥塞程度往往不稳定，如果只使用固定的超时时间，要么会让发送方频繁误判丢包、白白重传，要么会让发送方等得太久，降低效率。TCP 会采样网络往返时延，通过算法估计出一个相对合理的超时时间，这样就能兼顾到网络波动，既不至于过早重传，也不会无限期地等待确认。

------------------------------

533. *什么是TCP的流量控制和拥塞控制？, page url: https://www.mianshi.icu/question/detail?id=1078
533-1078-答案：
简单题，在校招和初中级工程师岗位中有可能遇到。
TCP 的流量控制和拥塞控制是两大核心机制，它们像“交通信号灯”一样协同工作，既保证数据可靠传输，又维持网络的高效运行。虽然目标都是控制发送速率，但解决的问题和实现方式有本质区别。


两者如何协同工作？

发送方的实际发送窗口取 接收窗口（rwnd） 和 拥塞窗口（cwnd） 的较小值。例如：
TCP 的流量控制和拥塞控制其实就像“交通信号灯”，一方面要确保接收方不会被海量数据淹没，另一方面也要让整个网络在高负载时还能持续工作下去。

流量控制主要是为了让发送方别发得太猛，超过了接收方当前能处理的数据量。接收方每次在 ACK 里带上一个接收窗口大小，比如说接收方可以告诉发送方：“我现在最多还能收 5MB”，那发送方就会据此控制自己的发送速率，如果接收方的缓冲快满了，窗口就可能被降到 0，发送方也就只能暂时停下来，等待接收方处理完再说。其实流量控制这里不怎么考虑网络因素，更注重接收方本身的处理速度和缓冲能力。

相比之下，拥塞控制就处理的是网络层面的问题。它的核心目的是让发送方别一次性把所有数据都塞进网络里，引起大规模排队和丢包。TCP 通过一个叫做拥塞窗口的东西来限制发送速率，这个窗口值会根据网络里探测到的各种信号动态变化，常见算法包括慢启动、拥塞避免和快重传快恢复等等。慢启动时，拥塞窗口从很小的值（比如 1 个数据包）开始，每次收到 ACK 就翻倍，让发送量迅速加大，直到发现网络快到上限了，这时候会进入拥塞避免，每次只小幅度地增加窗口，避免过度冲击网络。如果监测到丢包，特别是重复 ACK，可以根据快重传或快恢复的机制，先重传丢失的数据并小幅降速，而不必回到最小值；但要是检测到超时重传这种更严重的情况，就只能把拥塞窗口重置为 1，重新开始慢启动。

------------------------------

534. *详细介绍一下TCP拥塞控制的几种算法, page url: https://www.mianshi.icu/question/detail?id=1079
534-1079-答案：
简单题，在校招和初中级工程师岗位中有可能遇到。
TCP 拥塞控制的核心目标是通过动态调整发送速率，既要充分利用网络带宽，又要避免因数据过量注入而导致的网络拥塞。为了实现这一目标，典型的 TCP 拥塞控制机制包含以下关键阶段与算法。


核心思想与协作方式：
TCP 拥塞控制是指通过动态地调节发送速率来充分利用网络带宽，同时又要避免过度发送导致网络拥塞。

它一般会经历几个相互配合的阶段。

先说慢启动：刚开始不清楚网络的具体容量，只有从一个小的拥塞窗口（例如 1 MSS）开跑，每次收到 ACK 都以指数方式递增，这样能尽快逼近网络极限，但一旦途中发现丢包或超时，就说明碰到拥塞了，于是把慢启动阈值（ssthresh）调为当前拥塞窗口的一半，然后把拥塞窗口重置到 1 MSS，相当于从头再来。

------------------------------

535. *你对TCP的粘包和拆包有了解吗？, page url: https://www.mianshi.icu/question/detail?id=1080
535-1080-答案：
简单题，在校招和初中级工程师岗位中有可能遇到。

TCP 的粘包和拆包问题并非 TCP 本身的设计缺陷，而是由 TCP 的传输特性和应用层协议的交互方式共同导致的。
粘包的意思是，发送方发送的多个数据包在接收方被合并成了一个数据包。比如，发送方分别发送了两条消息“Hello”和“World”，但接收方可能一次性收到“HelloWorld”。而拆包则是相反的情况，发送方发送了一条完整的消息，比如“HelloWorld”，但接收方可能分两次接收到“Hello”和“World”。这两种情况都会导致接收方无法正确解析数据。

TCP 的粘包和拆包问题其实并不是 TCP 本身的缺陷，而是由它的传输特性和应用层协议的交互方式共同导致的。粘包和拆包的产生原因主要有以下几个方面。

首先，TCP 是一个面向字节流的协议，它只负责把数据可靠地传输到对方，但并不关心数据的边界。数据在传输过程中，TCP 会根据网络状况和缓冲区大小动态调整数据包的大小，这就可能导致粘包或拆包。
要解决粘包和拆包问题，关键在于在应用层定义清晰的数据边界。常见的解决方法有以下几种：

第一种是消息定长。也就是说，每条消息的长度是固定的，接收方只需要按照固定的长度读取数据就可以了。这种方法实现起来很简单，解析速度也很快，但缺点是灵活性差，无法适应不同长度的消息。

第二种方法是添加分隔符。在每条消息的末尾加一个特定的分隔符，比如换行符 \n 或者空字符 \0，接收方通过识别分隔符来区分消息。这种方法比较灵活，适合不同长度的消息，但需要确保分隔符不会出现在消息内容中，或者对消息内容进行转义处理。
这种“消息头+消息体”的协议设计模式方式非常常见，比如我之前参与的DBproxy项目中，需要对接MySQL协议。而MySQL协议大体上也遵循这个协议模式。

------------------------------

536. *HTTP2.0和 HTTP3.0 有什么区别？, page url: https://www.mianshi.icu/question/detail?id=1081
536-1081-答案：
简单题，校招和初级岗位面试中常见。
HTTP/2.0 与 HTTP/3.0 的主要差异集中在以下几个方面：
HTTP/2.0 和 HTTP/3.0 的区别，主要集中在底层协议、并行传输能力、握手效率、头部压缩、网络适应性等层面。

HTTP/2.0 是基于 TCP 的，一旦某个数据包丢失就必须等重传，这会造成整个连接里的所有数据流都被阻塞，也就是所谓的队头阻塞问题。HTTP/3.0 用的是基于 UDP 的 QUIC 协议，可以把每个数据流都独立分配 ID，只要某个流丢包，就只重传那个流，不会拖慢其它流，这在高丢包或弱网环境下会表现得更好。

HTTP/2.0 需要先完成 TCP 三次握手，然后再进行 TLS 握手，如果要恢复会话，也会多一次 RTT。而 QUIC 把传输层和加密层放在一起，相当于只需要一次握手就能建立安全连接，比 HTTP/2.0 快了大约一半。更进一步，QUIC 还支持 0-RTT 恢复，如果客户端保存过相关密钥信息，重连时几乎可以立刻发送数据，尤其在网络波动或需要频繁断网重连的场景下非常有效。不过，要注意 0-RTT 可能带来重放攻击风险，服务端通常要配合额外的安全逻辑做校验。

------------------------------

537. *HTTP/1.1、HTTP/2.0和HTTP/3.0的主要区别是什么？, page url: https://www.mianshi.icu/question/detail?id=1082
537-1082-答案：
简单题，在校招和初中级工程师岗位中有可能遇到。
前置知识：
HTTP/2.0相比HTTP/1.1带来了多项性能优化和效率提升。

首先，HTTP/2.0支持多路复用，允许在单个连接中同时发送多个请求和响应，解决了HTTP/1.1中的队头阻塞问题。

其次，HTTP/2.0使用HPACK算法压缩请求和响应的头部，减少了冗余数据，提高了传输效率。

------------------------------

538. *HTTPS的加密过程是怎样的？, page url: https://www.mianshi.icu/question/detail?id=1083
538-1083-答案：
简单题，在校招和初中级工程师岗位中有可能遇到。
前置知识：
整个 HTTPS 的通信过程还是比较复杂的，粗略的来说，可以分成四步。

第一步是 DNS 解析，将域名转换成 IP。

第二步是建立 TCP 连接，也就是三次握手过程。
从上面的过程也能看出来，TLS握手的安全性源于其多重保障机制：

首先，通过密钥交换和随机数生成确保数据加密，防止窃听；

其次，利用数字证书和证书链验证服务器身份，杜绝中间人攻击；

------------------------------

539. *HTTPS协议中，S指的是什么？, page url: https://www.mianshi.icu/question/detail?id=1084
539-1084-答案：
简单题，在校招和初中级工程师岗位中有可能遇到。
前置知识：

HTTPS 中的 "S" 代表 Secure，表示安全。它是 HTTP 协议运行在 SSL/TLS 协议上的实现，也可以理解为 HTTP over SSL/TLS。相比普通的 HTTP 协议，HTTPS 增加了一层加密与安全机制，用于保障数据传输的安全性和完整性，主要通过以下特性实现：


总结
HTTPS 里的 “S” 指的就是 Secure，也就是安全的意思。它其实就是 HTTP 协议加上了 SSL/TLS 协议，用来对数据进行加密、身份验证和完整性校验。

相比起普通的 HTTP，HTTPS 会在传输过程中先通过非对称加密（例如 RSA 或 DH）来安全地交换对称加密密钥，然后再用对称加密算法（例如 AES）对实际数据进行高速加密。这样，即使有人在传输过程中截获数据，也无法解读出真正的内容。

除了基础的加密，HTTPS 还依赖服务器的数字证书来验证网站身份。浏览器会去检查证书的签发机构是否可信，域名是否匹配，证书的有效期等等，这样用户就能确定自己访问的网站是真实可信的，而不是被中间人攻击者假冒的钓鱼网站。

------------------------------

540. *HTTP 请求报文的格式是什么？, page url: https://www.mianshi.icu/question/detail?id=1085
540-1085-答案：
简单题，在校招和初级工程师岗位中有可能遇到，再往上就很难遇到了。
前置知识：
HTTP请求报文由四部分组成：

第一部分是请求行，包含请求方法、URI和HTTP版本，如GET /index.html HTTP/1.1；

第二部分是请求头部，它包含一系列键值对，传递附加信息，如Host: example.com；
HTTP 的这种报文格式对后续很多应用层协议协议产生了深远的影响。

------------------------------

541. *HTTP响应报文的格式是什么？, page url: https://www.mianshi.icu/question/detail?id=1086
541-1086-答案：
简单题，在校招和初级工程师岗位中有可能遇到，再往上就很难遇到了。


前置知识：
HTTP响应报文由四部分组成：

第一部分是状态行，包含HTTP版本、状态码和状态消息，如HTTP/1.1 200 OK；

第二部分是响应头部，它包含一系列键值对，传递附加信息，如Host: example.com；
HTTP 的这种报文格式对后续很多应用层协议协议产生了深远的影响。

------------------------------

542. *HTTP的报文结构是怎样的？, page url: https://www.mianshi.icu/question/detail?id=1087
542-1087-答案：
简单题，在校招和初级工程师岗位中有可能遇到，再网上就很难遇到了。

在实践中这个问题其实没太大用处，毕竟我们现在已经没啥机会直接操作 HTTP 请求响应的字节流了，也就是在浏览器中通过控制台查看请求响应的时候有一点用处而已。

而后有一个非常高端的装逼方式，即讨论 HTTP 报文设计对后续很多应用层协议设计的影响，比如说各种 RPC 协议设计的影响。
HTTP 请求报文和响应报文的格式是什么（mianshi.icu）？的另一种问法。
HTTP请求报文由四部分组成：

第一部分是请求行，包含请求方法、URI和HTTP版本，如GET /index.html HTTP/1.1；

第二部分是请求头部，它包含一系列键值对，传递附加信息，如Host: example.com；
HTTP 的这种报文格式对后续很多应用层协议协议产生了深远的影响。

------------------------------

543. Cookie、Session、Token 之间有什么区别？, page url: https://www.mianshi.icu/question/detail?id=1088
543-1088-答案：
简单题，校招和初级岗位面试中常见。
前置知识：


在 Web 应用中，Cookie、Session 和 Token 是常见的用户认证和状态管理方案。它们的主要区别如下：


选择建议
Cookie、Session 和 Token 在 Web 应用中分别扮演了不同角色，用来实现用户认证和状态管理。

Cookie 是一种将数据存储在客户端浏览器中的方式，每次请求时，浏览器都会自动将它包含在 HTTP 请求头中，适合存储一些相对不敏感且体量较小的文本数据。

Session 的核心数据则存储在服务器端，客户端只需保存一个 Session ID（通常放在 Cookie 里），服务器根据这个 ID 就能检索到对应的会话信息，比如用户资料和权限等。

------------------------------

544. *什么是 CDN？它是如何加速访问的？, page url: https://www.mianshi.icu/question/detail?id=1089
544-1089-答案：
简单题，校招和初级岗位面试中常见。

CDN（内容分发网络，Content Delivery Network）是一种分布式网络架构，通过在全球范围内部署多个边缘节点服务器，将网站资源（如图片、视频、静态文件）缓存并分发到离用户最近的服务器，以提高网站或应用访问速度和可靠性的技术。它的核心目标是通过减少数据传输的物理距离和优化网络路径，使用户能够就近、快速、稳定地获取资源。

CDN的工作流程大致如下：

CDN 通过以下几种关键技术手段实现加速访问：
CDN（Content Delivery Network），也就是内容分发网络，其实就是在全球不同地区部署了大量“边缘节点”服务器，把网站上的静态资源（比如图片、视频、CSS、JavaScript 等）预先缓存起来。这样，当用户访问这些内容时，就能就近从最接近的节点获得资源，无须每次都回到源站取数据，整个访问过程会变得又快又稳定。

在具体运作上，用户在访问网站时，首先会通过 DNS 解析域名，CDN 的智能 DNS 根据用户 IP 返回最近的节点地址。

然后，如果该节点本身已经缓存了用户需要的文件，那就会直接把这些文件提供给用户。要是没有缓存，就会去源站拉取资源，保存到节点里，再把文件返回给用户。等到下次访问时，同一资源就可以直接从缓存里取，进一步提升访问速度。

------------------------------

545. *什么是跨域问题？有哪些解决跨域问题的方式？, page url: https://www.mianshi.icu/question/detail?id=1090
545-1090-答案：
简单题，校招和初级岗位面试中常见。

跨域问题通常是指在一个网页中向不同协议、不同域名或不同端口的服务器发起请求时被浏览器限制或阻拦的现象。该问题是由于浏览器的同源策略（Same Origin Policy）而产生的，其目的是保护用户数据安全，防止恶意网站通过脚本访问其他网站的私有数据。同源策略要求两个URL的协议、域名和端口都相同才能互相访问资源。例如：


然而，在现代Web应用中，不同域之间的数据交互非常普遍，例如：


因此，跨域问题成为Web开发中常见的挑战，需要采用特定的解决方案来实现跨域资源共享。
跨域问题指的是，当一个网页想要请求不同协议、不同域名或不同端口的资源时，浏览器因为同源策略而拒绝或阻止这些请求。这个同源策略是为了保护用户数据，避免恶意网站通过脚本访问其他站点的私有数据。

举个例子，如果我访问的站点是 https://www.example.com，就只能请求同源的 https://www.example.com:443，但如果换成了 http://www.example.com 或者 https://api.example.com，浏览器就会认为它不再是同源，从而触发跨域限制。

随着现代 Web 应用越来越多地使用前端与后端分离、微服务分布、以及第三方资源调用等模式，跨域就成了一个常见的问题，需要通过一些手段来实现跨域资源共享。

------------------------------

546. *什么是DNS？, page url: https://www.mianshi.icu/question/detail?id=1091
546-1091-答案：
简单题，校招和初级岗位面试中常见。


DNS（Domain Name System，域名系统）是互联网的重要基础服务，用于将人类可读的域名（例如 www.example.com）转换成计算机可识别的 IP 地址（例如 1.1.1.1），它本质上是一种分布式数据库系统。形象地说，它就像“互联网的电话簿”，在网络上，IP 地址用于标识设备，但对人类而言，记忆域名要比记忆 IP 地址更加方便。因此，DNS 协议的出现就是为了将可读的域名与计算机可识别的 IP 地址之间进行映射。


以浏览器访问www.example.com为例，大致会经历如下步骤：
DNS，也就是域名系统，可以理解为互联网的电话簿，用来把我们熟悉的域名转换成 IP 地址。人类记域名方便，机器则需要 IP 地址来进行通信，所以 DNS 的出现就解决了这个两难问题。它实际上是一个分布式数据库系统，每个部分都存储着域名和 IP 的对应关系，用户在访问网站时不至于手动输入或记住那些冗长的 IP。
此外，DNS 可以配置很多不同的记录类型，比如 A 记录用来把域名映射到 IPv4，AAAA 记录对应 IPv6，CNAME 记录用来设置别名，MX 记录则是指示邮件服务器。还有 TXT、NS、PTR 等记录可以用来做验证域名所有权、防垃圾邮件和反向解析等。通过这些不同类型的记录，DNS 不仅可以完成域名解析，还能实现邮件路由、服务发现、负载均衡、故障转移等重要功能。

------------------------------

547. *DNS的解析过程是怎样的？, page url: https://www.mianshi.icu/question/detail?id=1092
547-1092-答案：
简单题，校招和初级岗位面试中常见。
前置知识：
以访问www.example.com”为例，当我在浏览器里输入“www.example.com”并按回车后，浏览器首先会查看自己的缓存里有没有这个域名对应的 IP 地址。如果有缓存，浏览器就会直接用这个 IP 去访问，从而很快打开页面。

要是浏览器里没有缓存，就需要调用操作系统的层面进行查询，比如检查 Hosts 文件或者操作系统的 DNS 缓存，看看有没有对应的 IP 映射。

如果操作系统这边也没有找到，那就只能去问本地 DNS 服务器（通常是由ISP 提供）。本地 DNS 服务器也有自己的缓存，如果它也找不到答案，那就需要向上级继续查询，开始递归查询的过程。

------------------------------

548. *DNS的作用是什么？, page url: https://www.mianshi.icu/question/detail?id=1093
548-1093-答案：
简单题，校招和初级岗位面试中常见。
前置知识：
DNS 的最基本功能是做域名解析，既可以把一个域名转换成对应的 IP 地址，也可以通过 IP 地址反查出对应的域名，这两种方式分别叫做正向解析和反向解析。正向解析相当于让我们在浏览器里输入一个域名时，DNS 自动帮我们找到正确的服务器 IP。而反向解析则是用 PTR 记录，根据一个 IP 地址来确认实际对应的域名，这在某些特殊场景下对于安全或者审计都很有用。

在流量管理方面，DNS 会通过负载均衡或地理位置策略把访问请求指向不同的服务器，从而让全球用户都能尽量就近访问，或者是分散在多台服务器上避免单点瓶颈。如果一台主服务器出现崩溃，DNS 还能切换到备用 IP，继续让业务不中断，提高整体的容灾能力。

性能上，DNS 通过多级缓存提高了大规模访问下的响应速度。浏览器、操作系统、本地 DNS 服务器都会短时间保存解析结果，不必每次都发起完整的 DNS 查询，这对减少时延特别有帮助。

------------------------------

549. *什么是 DNS 缓存？如何避免 DNS 污染？, page url: https://www.mianshi.icu/question/detail?id=1094
549-1094-答案：
简单题，校招和初级岗位面试中常见。

DNS 缓存是指在域名解析过程中，将域名与 IP 地址的映射关系临时存储在不同层级的缓存中，以加快后续相同域名解析的速度，并减少对上游 DNS 服务器的查询请求。
DNS 缓存是指在域名解析过程中，将域名与 IP 地址的对应关系临时存储在各个层级（包括浏览器、操作系统和本地 DNS 服务器）的缓存里，以此加快后续相同域名的解析速度，也能减少对上游 DNS 服务器的查询次数。

浏览器通常会先在自己的缓存中查找域名解析记录，若没有或缓存过期，就会再询问操作系统的 DNS 缓存；如果还是没有，就会向本地 DNS 服务器查询。这个过程层层递进，大幅缩短域名解析的响应时间，同时也能减轻网络的负担。

不过 DNS 缓存有可能会导致一些问题，比如数据过时。如果某一个域名的 IP 地址已经更换，而 DNS 缓存还没过期，那么用户就可能访问到错误的服务器。

------------------------------

550. *什么是 DNS 缓存？, page url: https://www.mianshi.icu/question/detail?id=1095
550-1095-答案：
简单题，校招和初级岗位面试中常见。
前置知识：
DNS 缓存是指在域名解析过程中，将域名与 IP 地址的对应关系临时存储在各个层级（包括浏览器、操作系统和本地 DNS 服务器）的缓存里，以此加快后续相同域名的解析速度，也能减少对上游 DNS 服务器的查询次数。

------------------------------

551. *如何避免 DNS 污染？, page url: https://www.mianshi.icu/question/detail?id=1096
551-1096-答案：
简单题，校招和初级岗位面试中常见。
前置知识：
所谓 DNS 污染，指的是攻击者用篡改的 DNS 解析结果来把域名指向假冒或恶意网站，比如钓鱼页面，这样一来用户以为自己访问的是正确的站点，实际上已经落入了陷阱。这些钓鱼页面可能会窃取用户的敏感信息，或者执行一些恶意操作。

为了防止 DNS 污染，可以先考虑使用加密的 DNS 协议，比如 DNS over HTTPS（DoH）或者 DNS over TLS（DoT）。它们能把 DNS 查询过程加密，避免在传输过程中被篡改或截获。

另外也可以启用 DNSSEC，它会对从权威 DNS 服务器返回的解析结果进行数字签名，让客户端能验证数据的完整性和真实性。

------------------------------

552. *请解释SQL与NoSQL数据库的区别及各自的应用场景。, page url: https://www.mianshi.icu/question/detail?id=1097
552-1097-答案：
简答题，校招和初级岗位面试中常见。
SQL 与 NoSQL 的主要区别：


应用场景：
SQL数据库与 NoSQL 数据库在数据模型、事务一致性以及查询方式等方面都有显著区别。

首先就数据模型而言，SQL 采用预先设计好的表结构，并通过外键等方式在多表之间建立关系，让复杂业务逻辑和 JOIN 操作变得相对直接。比较常用有MySQL、PostgreSQL等。而 NoSQL 的数据模型更加灵活，可以使用文档、键值、列族或者图结构来存储信息，也往往是无固定模式的，这就特别适合于不断变化或非结构化的数据场景。比较常用的有MongoDB（文档型）、Redis（键值型）等。

在事务管理上，SQL 数据库常常遵循 ACID 原则，能够提供强一致性和复杂查询能力，适合金融、支付等必须确保数据准确严谨的业务场景。不过，有些 NoSQL 解决方案，比如MongoDB等开始支持类似 ACID 的事务功能，只是有的实现并不彻底，有的只在特定场景下开启。大多数 NoSQL 产品还是通过 BASE 思路来强调基本可用、软状态和最终一致性，从而在高并发或高可用的业务里表现出色。

------------------------------

553. *关系型数据库与非关系型数据库的区别是什么？你能举例说明一下吗？, page url: https://www.mianshi.icu/question/detail?id=1098
553-1098-答案：
简答题，校招和初级岗位面试中常见。
请解释SQL与NoSQL数据库的区别及各自的应用场景（mianshi.icu）的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
关系型数据库与非关系型数据库在数据模型、事务一致性以及查询方式等方面都有显著区别。

首先就数据模型而言，关系型数据库采用预先设计好的表结构，并通过外键等方式在多表之间建立关系，让复杂业务逻辑和 JOIN 操作变得相对直接。比较常用有MySQL、PostgreSQL。而非关系型数据库的数据模型更加灵活，可以使用文档、键值、列族或者图结构来存储信息，也往往是无固定模式的，这就特别适合于不断变化或非结构化的数据场景。比较常用的有MongoDB（文档型）、Redis（键值型）等。

在事务管理上，关系型数据库常常遵循 ACID 原则，能够提供强一致性和复杂查询能力，适合金融、支付等必须确保数据准确严谨的业务场景。不过，有些非关系型数据库解决方案，比如MongoDB等开始支持类似 ACID 的事务功能，只是有的实现并不彻底，有的只在特定场景下开启。大多数 NoSQL 产品还是通过 BASE 思路来强调基本可用、软状态和最终一致性，从而在高并发或高可用的业务里表现出色。

------------------------------

554. *你知道数据库设计的三大范式吗？, page url: https://www.mianshi.icu/question/detail?id=1099
554-1099-答案：
简答题，校招和初级岗位面试中常见。
什么是数据库的第一、第二、第三范式（mianshi.icu）？的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
数据库的三大范式主要用于解决数据冗余和数据异常等问题，以保证数据一致性。

第一范式强调字段的原子性，如果一个字段里混合了多个独立的信息，就会在后续的查询和更新中带来额外复杂度。比如在学生表里，把多个电话号码塞进同一个字段肯定会增加处理难度，最好的做法就是把这些号码分开存，或者单独拆成一张电话表。

接下来，第二范式就是在满足第一范式的前提下，确保所有非主键字段都完全依赖于主键，如果主键是多个字段组成的复合主键，就不能出现某个非主键字段只依赖其中一个字段的情况。举个例子，订单明细表可能以“订单ID+产品ID”作为复合主键，但如果产品名称只跟产品ID有关系，而与订单ID无关，那么它就不应该放在这张表中，而是挪到一个专门的产品表里，否则就会重复存储商品名称，一旦商品信息变动，还得去所有记录里更新。
在实际应用里，范式化可以减小冗余、维持数据一致性，也能让业务逻辑更加清晰。不过，如果一个场景读多写少，为了减少多表查询的性能损耗，也会考虑做一些反范式化，把常用的信息冗余在同一张表里，降低查询的复杂度。比如电商订单列表页要显示用户姓名、订单号和商品名称，可以在订单表里冗余用户姓名以避免频繁地去关联用户表或商品表。

------------------------------

555. *什么是数据库的第一、第二、第三范式？, page url: https://www.mianshi.icu/question/detail?id=1100
555-1100-答案：
简答题，校招和初级岗位面试中常见。






总而言之，三大范式（1NF、2NF、3NF）各自解决了在数据原子性、依赖完整性以及传递依赖方面的问题，帮助我们在关系型数据库中更好地降低冗余、提升数据一致性，从而构建高内聚、低耦合的数据模型。实际应用中需结合业务特点灵活取舍——核心事务系统优先范式化，再根据业务特点和访问模式适度反范式化，最终在数据一致性与系统性能间取得平衡。
数据库的三大范式主要用于解决数据冗余和数据异常等问题，以保证数据一致性。

第一范式强调字段的原子性，如果一个字段里混合了多个独立的信息，就会在后续的查询和更新中带来额外复杂度。比如在学生表里，把多个电话号码塞进同一个字段肯定会增加处理难度，最好的做法就是把这些号码分开存，或者单独拆成一张电话表。

接下来，第二范式就是在满足第一范式的前提下，确保所有非主键字段都完全依赖于主键，如果主键是多个字段组成的复合主键，就不能出现某个非主键字段只依赖其中一个字段的情况。举个例子，订单明细表可能以“订单ID+产品ID”作为复合主键，但如果产品名称只跟产品ID有关系，而与订单ID无关，那么它就不应该放在这张表中，而是挪到一个专门的产品表里，否则就会重复存储商品名称，一旦商品信息变动，还得去所有记录里更新。
在实际应用里，范式化可以减小冗余、维持数据一致性，也能让业务逻辑更加清晰。不过，如果一个场景读多写少，为了减少多表查询的性能损耗，也会考虑做一些反范式化，把常用的信息冗余在同一张表里，降低查询的复杂度。比如电商订单列表页要显示用户姓名、订单号和商品名称，可以在订单表里冗余用户姓名以避免频繁地去关联用户表或商品表。

------------------------------

556. *什么是反范式化？为什么需要反范式化？它的应用场景是什么？, page url: https://www.mianshi.icu/question/detail?id=1101
556-1101-答案：
简答题，校招和初级岗位面试中常见。
什么是反范式化？

反范式化（Denormalization）是指在数据库设计中，刻意违反范式化原则，通过增加冗余字段或合并表结构等方式来提升查询效率、简化查询过程。它与范式化的目标相反：范式化力求减少数据冗余、加强数据一致性，而反范式化则“以空间换时间”，在一定程度上牺牲数据一致性和占用更多存储空间，换取更高的查询速度或更方便的查询方式。

为什么需要反范式化？
反范式化主要是一种“以空间换时间”的做法，跟我们平常坚持的数据库设计三大范式正好相反。范式化是为了减少冗余、确保数据一致性，但在高并发或者需要实时分析的场景下，大量的表关联操作会成为系统瓶颈。

反范式化就是主动在数据库里增加一些冗余字段或合并表结构，一定程度上牺牲数据的一致性和占用更多存储空间，换来查询效率的提升，或者让查询逻辑变得更简单。

举个例子，电商后台每天都要做销售统计，如果按照严谨的范式化思路，把订单、产品、用户等信息全部拆分开，每次要做统计分析时都可能涉及多张表的关联和聚合操作，性能压力相当大。反范式化的解决方式，可以是把日销量、累计销量这些信息先期预计算并冗余在一个汇总表里，需要用的时候，直接读取就行了，效率会高很多。当然，这并不是随心所欲地冗余所有信息，而是对高频场景或查询压力大的需求做选择性冗余。

------------------------------

557. *什么是主键和唯一键？它们的区别是什么？, page url: https://www.mianshi.icu/question/detail?id=1102
557-1102-答案：
简答题，校招和初级岗位面试中常见。
基本概念

主要区别

简单示例
主键其实就是用来标识每行数据身份的核心字段或字段组合，必须保证主键值的非空性和唯一性。非空性是指插入主键值数据时必须显式指定值，但如果设置了自增（AUTO_INCREMENT），插入数据时就不用手动指定值，数据库会自动分配下一个可用的数值，另外通常不建议在插入后再去修改主键。唯一性是指主键值数据整能够唯一标识表中的一行，此外数据库一般会自动给它创建聚簇索引。

相比之下，唯一键虽然也要保证相应字段的值的唯一性，但它更偏向业务层面的唯一性控制，不一定要当成每行数据的主识别字段。它允许为空，而且在不同的数据库里，对空值的处理也不一样，有的可以插入多个空值，例如MySQL。有的只能插一次，例如SQL Server 。一个表中可以同时存在多个唯一键，每个唯一键都会有对应的非聚簇索引来提升检索效率，这样就能在业务里确保不同属性都能做到唯一。

两者主要区别，首先是每张表只能有一个主键，但可以有多个唯一键。

------------------------------

558. *什么是外键？在什么情况下使用外键？, page url: https://www.mianshi.icu/question/detail?id=1103
558-1103-答案：
简答题，校招和初级岗位面试中常见。
基本概念

外键是指在一个表（往往称为子表）中的字段或字段组合，用来引用另一个表（主表）的主键或唯一键，从而在数据库层面建立表与表之间的关联关系。它能确保子表中的对应值一定在主表中存在，避免插入“无效”或“孤立”的数据，这样能够有效地维护数据的一致性与完整性。


主要作用：
外键其实就是在一张表（子表）中设置一个或多个字段，用来引用另一张表（主表）的一个过多个主键或唯一键，这样就能在数据库层面把两张表关联起来。它最大的作用就是确保子表里引用的数据一定能在主表中找到对应记录，避免出现那种找不到主表记录的"孤儿数据"。

外键的主要作用体现在以下几个方面。首先，它能明确表之间的关系，比如“订单表”通过外键关联“用户表”的主键，就能清楚地表示这个订单是哪个用户的。其次，外键约束可以防止在子表里插入主表不存在的引用，比如不允许在订单表里插入一个不存在的用户ID。最后，通过设置外键的级联操作，比如 CASCADE，当主表里的记录被删除或更新时，子表里的相关记录也会同步更新或删除，避免出现“孤儿数据”。

不过使用外键也是要权衡的。它确实能帮我们在数据库层面严格把控数据质量，让表之间的关系更清晰，开发人员一看表结构就能明白数据之间的关联。但是在高并发的系统中，每次插入或更新数据时都要做外键检查，这个开销是不能忽视的。特别是在分布式架构下，数据可能分散在不同节点上，实现跨节点的外键约束就更麻烦了。

------------------------------

559. *什么是外键？它在数据库设计中起什么作用？, page url: https://www.mianshi.icu/question/detail?id=1104
559-1104-答案：
简答题，校招和初级岗位面试中常见。
什么是外键？在什么情况下使用外键（mianshi.icu）？的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
外键其实就是在一张表（子表）中设置一个或多个字段，用来引用另一张表（主表）的一个过多个主键或唯一键，这样就能在数据库层面把两张表关联起来。它最大的作用就是确保子表里引用的数据一定能在主表中找到对应记录，避免出现那种找不到主表记录的"孤儿数据"。

外键在数据库设计中的作用体现在以下几个方面。首先，它能明确表之间的关系，比如“订单表”通过外键关联“用户表”的主键，就能清楚地表示这个订单是哪个用户的。其次，外键约束可以防止在子表里插入主表不存在的引用，比如不允许在订单表里插入一个不存在的用户ID。最后，通过设置外键的级联操作，比如 CASCADE，当主表里的记录被删除或更新时，子表里的相关记录也会同步更新或删除，避免出现“孤儿数据”。

不过使用外键也是要权衡的。它确实能帮我们在数据库层面严格把控数据质量，让表之间的关系更清晰，开发人员一看表结构就能明白数据之间的关联。但是在高并发的系统中，每次插入或更新数据时都要做外键检查，这个开销是不能忽视的。特别是在分布式架构下，数据可能分散在不同节点上，实现跨节点的外键约束就更麻烦了。

------------------------------

560. *数据库中的事务指的是什么？, page url: https://www.mianshi.icu/question/detail?id=1105
560-1105-答案：
基础题。这个问题一般出现在面试官想要考察你的数据库事务的功底的时候，作为一个开场问题。要想回答好这个问题，你需要提前准备好整个事务的面试方案，做好引导工作。事务的相关知识非常多，你要尽可能在回答这个问题的时候将你能赢得竞争优势的点回答出来。
什么是数据库事务（mianshi.icu）？的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
（基本回答）数据库事务，简单来说就是一组不可分割的操作，确保数据库从一个状态迁移到另外一个状态，不会出现任何的中间状态。

（引导 ACID）数据库事务 ACID 四个特性，分别是原子性、一致性、隔离性、持久性。
在实践中使用事务的时候，要注意数据库设置的隔离级别、数据库事务的超时设置。
与数据库事务类似的另外一个东西是分布式事务。不过虽然两者都是事务，但是实践中实现方式、应用场景都很不一样。

------------------------------

561. *数据库中事务的四大特性（ACID）是什么？, page url: https://www.mianshi.icu/question/detail?id=1106
561-1106-答案：
基础题。基本上在讨论数据库或者数据库事务的时候，大概率会问这个问题。但是如果你只能回答出 ACID 的基本定义，那么你是没有办法赢得竞争优势的。
什么是 ACID（mianshi.icu）？的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
ACID 是指数据库事务满足的四个特性：

首先，原子性（Atomic）指的是强调事务内部的所有操作，要么同时成功，要么同时失败。

其次，一致性（Consistency）指的是数据库必须从一个一致的状态，迁移到另外一个一致的状态，而不能处于中间状态。或者说，事务开始之前和事务开始之后，必须满足数据库的完整性约束和满足业务规则。
这四个特性里面，比较值得探讨的是隔离性。
进一步来说，在分布式事务里面，是基本没有办法保证 ACID 这四个特性的。

------------------------------

562. *数据库事务隔离级别有哪些？各自会产生什么问题？, page url: https://www.mianshi.icu/question/detail?id=1107
562-1107-答案：
基础高频题。基本上在讨论数据库或者数据库事务的时候，大概率会问这个问题。

你在回答的时候，一方面要回答出来隔离级别的基本定义，一方面也要回答出来不同隔离级别可能引发的问题。而后你有几个选择：一个选择是将话题引导到 MySQL 实现隔离级别的底层机制上；一个选择是将话题延伸到快照读上；最后一个选择是进一步讨论分布式事务中的隔离级别问题。
什么是隔离级别（mianshi.icu）？的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
数据库的隔离级别分成四种：

首先是读未提交（Read Uncommitted），这是最低的隔离级别，允许一个事务读取另一个事务尚未提交的数据。这种情况下可能会出现脏读，即读取到其他事务可能最终会回滚的数据。

其次是读已提交（Read Committed），在这个隔离级别下，一个事务只能读取已经被其他事务提交的数据。它避免了脏读，但可能会出现不可重复读，即一个事务在其运行期间多次读取相同记录时，可能会发现记录已被其他已提交的事务修改。
此外在数据库中还有一种快照读的概念。它是指同一个事务内部，读到的数据永远是一样的，也就是在可重复读的基础上进一步解决了幻读问题。在 MySQL 中的可重复读隔离级别下，利用 MVCC 中的 Read View 机制，实现了快照读的效果。
在分布式事务里面，倒是很少讨论隔离级别这个问题。现在的热门的 TCC 之类的分布式事务，几乎完全没有考虑到任何隔离级别的问题。

------------------------------

563. *如何选择隔离级别？说说具体场景下可能导致的问题, page url: https://www.mianshi.icu/question/detail?id=1108
563-1108-答案：
简答题，校招和初级岗位面试中常见。
前置知识：
数据库的隔离级别分成四种：

首先是读未提交（Read Uncommitted），这是最低的隔离级别，允许一个事务读取另一个事务尚未提交的数据。这种情况下可能会出现脏读，即读取到其他事务可能最终会回滚的数据。

其次是读已提交（Read Committed），在这个隔离级别下，一个事务只能读取已经被其他事务提交的数据。它避免了脏读，但可能会出现不可重复读，即一个事务在其运行期间多次读取相同记录时，可能会发现记录已被其他已提交的事务修改。
在分布式事务里面，倒是很少讨论隔离级别这个问题。现在的热门的 TCC 之类的分布式事务，几乎完全没有考虑到任何隔离级别的问题。

------------------------------

564. *什么是幻读？举例说明一下, page url: https://www.mianshi.icu/question/detail?id=1109
564-1109-答案：
简答题，校招和初级岗位面试中常见。
前置知识：


幻读是一种特殊的并发问题，在同一事务内，当多次执行相同的范围查询时，会发现结果集发生了变化，好像“幻影”般出现或消失某些行。这种现象通常出现在其他并发事务在该范围内插入或删除数据的情况下，而非修改已存在的数据内容（修改会引起不可重复读）。

幻读通常发生于执行范围查询时。例如，一个事务在统计符合某个条件的记录时，另一事务插入了新记录或删除了部分记录，导致同一事务内连续多次查询时结果不一致。
幻读是一种比较特殊的并发问题。它主要发生在同一个数据库事务里，如果我多次执行同样的范围查询，结果发现查出来的结果集数量不一样了，多了或者少了，就像出现了幻觉一样，这就是幻读。
要避免幻读，有以下几种办法：

一种是把事务隔离级别调到“可串行化”。这样的话，我的事务在查的时候，别人就不能改了，但是这样性能会比较差。

------------------------------

565. *什么是索引？为什么使用索引可以加速查询？, page url: https://www.mianshi.icu/question/detail?id=1111
565-1111-答案：
简答题，校招和初级岗位面试中常见。
前置知识：

索引加速查询的工作机制
（总体介绍）数据库索引是一种数据结构，用于快速查找数据库表中的数据。它类似于书籍的目录，可以帮助我们快速找到需要的信息。

索引的主要作用是加速查找，当执行查询操作时，如果没有索引，就会涉及到全表扫描，每一行都得判断一遍；而有了索引后，数据库只需要扫描索引里相关的数据，就能直接跳转到符合条件的记录。

索引之所以可以加速查找是因为索引数据结构的有序性可以把查找的时间复杂度降低到 O(log n) 或更低，并且索引文件通常比整个表要小很多，所以所需的磁盘 I/O 操作也大大减少，有时候如果查询的所有字段都已经包含在索引里面，那就可以实现索引覆盖或覆盖索引优化，直接从索引中获取想要的数据，完全避免回表操作。
除了使用过索引以外，我也擅长针对业务设计索引，以及进行索引优化、查询优化。大部分情况下，业务的性能不好，很可能是索引方面没有设计好。
索引的应用非常广泛，除了我们最熟悉关系型数据库会使用到索引结构，别的中间件也会使用到索引结构。只是说这些中间件的索引从设计和实现上来说会和关系型数据库很不一样。

------------------------------

566. *用过数据库索引吗，讲一下索引的作用，以及使用时的注意事项, page url: https://www.mianshi.icu/question/detail?id=1112
566-1112-答案：
简答题，校招和初级岗位面试中常见。
前置知识：
数据库索引主要作用是加速查询，此外优化排序和分组、提升连接效率。当然，索引还能帮助保证数据的完整性约束。比如唯一索引或者主键索引，它们能防止出现重复的数据（像邮箱、身份证号这种），或者保证业务的幂等性，确保整个业务逻辑的完整性。

在使用数据库索引的时候需要注意以下几点：

首先，尽量避免使用会导致索引失效的SQL语法。比如在 WHERE 子句中使用函数或者表达式，像 "WHERE YEAR(create_time)=2023" 这样的语句，索引就失效了。此时可以使用范围查询，像 "BETWEEN '2023-01-01' AND '2023-12-31'"来替代。另外，在使用通配符的时候也要小心，像 "LIKE '%北京'" 这样的语句是无法使用索引的，而 "LIKE '北京%'" 就可以。还有隐式类型转换，如果查询条件与索引列的类型不一致，也会导致索引失效。比如，如果 id 是一个数字字段，但我在查询的时候用了字符串 "WHERE id='100' "，索引就失效了。

------------------------------

567. *在哪些场景下表中的索引会失效？, page url: https://www.mianshi.icu/question/detail?id=1114
567-1114-答案：
如果你平时没有想过，那么面试官问出来就很容易卡住。实际上这个问题并不难，但是可能的情况比较多，因此你在回答的时候，不一定能够把面试官想要你回答的所有的点都回答出来。但是并不要紧，你只需要答出三五个点就差不多了，足以证明你对这个问题有比较深刻的理解。

而后在平时注意收集一些索引失效的案例就可以了，尽量在回答的时候用上，就非常完美。
为什么定义了索引，但是最终数据库却没有用上（mianshi.icu）？的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
（基本原因）在以下场景中表中的索引会失效：

在实践中，遇到慢查询，或者 SQL 优化的时候，首先要确定的就是数据库究竟有没有用索引，用的是哪个索引，是否符合我们的预期。
（进一步引导）索引失效的问题不仅仅在于查询会很慢，还可能引发更加严重的问题，比如说表锁。正常在 MySQL 的 Innodb 引擎中，对数据加锁是借助索引来实现的。如果没有命中任何索引，那么 Innodb 引擎就会使用表锁。

------------------------------

568. *索引失效的原因有哪些？, page url: https://www.mianshi.icu/question/detail?id=1115
568-1115-答案：
如果你平时没有想过，那么面试官问出来就很容易卡住。实际上这个问题并不难，但是可能的情况比较多，因此你在回答的时候，不一定能够把面试官想要你回答的所有的点都回答出来。但是并不要紧，你只需要答出三五个点就差不多了，足以证明你对这个问题有比较深刻的理解。

而后在平时注意收集一些索引失效的案例就可以了，尽量在回答的时候用上，就非常完美。
为什么定义了索引，但是最终数据库却没有用上（mianshi.icu）？的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
（基本原因）索引失效的原因有很多：

在实践中，遇到慢查询，或者 SQL 优化的时候，首先要确定的就是数据库究竟有没有用索引，用的是哪个索引，是否符合我们的预期。
（进一步引导）索引失效的问题不仅仅在于查询会很慢，还可能引发更加严重的问题，比如说表锁。正常在 MySQL 的 Innodb 引擎中，对数据加锁是借助索引来实现的。如果没有命中任何索引，那么 Innodb 引擎就会使用表锁。

------------------------------

569. *数据库中索引的种类有哪些？, page url: https://www.mianshi.icu/question/detail?id=1116
569-1116-答案：
简答题，校招和初级岗位面试中常见。
以下是数据库中常见索引类型：
数据库中索引主要以下几种：

首先，从逻辑功能来看，常见的有主键索引、唯一索引、普通索引、联合索引、覆盖索引、前缀索引、全文索引、空间索引等。

其次，从底层数据结构来看，常见的有 B+树索引、哈希索引、倒排索引等。

------------------------------

570. *索引的底层原理是什么？如何运作的？, page url: https://www.mianshi.icu/question/detail?id=1117
570-1117-答案：
简答题，校招和初级岗位面试中常见。
前置知识：

索引的底层原理是在数据库中引入“额外的数据结构来加速数据查找”，常见的索引结构包括 B+ 树、哈希表和位图索引。通过将无序的数据动态组织成一种有序（或可散列）的形式，索引可显著减少全表扫描的次数，从而提高查询效率。而这一切以“牺牲一定的存储空间和增加写操作成本”为代价来换取查询速度的提升。常用做索引的数据结构主要有以下几种：

索引的作用：
索引的底层原理，其实就是在数据库里增加一些额外的数据结构，用来加快数据的查找速度。常见的索引结构有 B+ 树、哈希表和位图索引。通过把原本无序的数据变成有序的形式或者哈希的形式，索引能大大减少全表扫描的次数，提高查询的效率。当然，这是以牺牲一定的存储空间和增加写操作的成本为代价的。

下面我以B+ 树索引为例，介绍一下索引运作方式：

B+ 树是多叉树，它由根节点、很多层内部节点和叶子节点组成。它会限制每个节点的键值数量在一个范围内，保证所有叶子节点的深度都一样，这样树就比较平衡。内部节点并不存储数据，只存储了索引键的数据，因此更加紧凑。同样的大小的磁盘或者内存，能够装下更多的内部节点。叶子节点通常存储着指向实际数据行的指针，或者记录的物理地址，而且所有的叶子节点都通过双向链表连在一起，非常适合范围查询，性也非常稳定。

------------------------------

571. *数据库索引为什么通常用B+树实现而不是B树？, page url: https://www.mianshi.icu/question/detail?id=1118
571-1118-答案：
中等难度，应届生和社招都可能遇到。主要考察对B+树和B树的理解以及索引设计的实际应用。回答时要避免简单罗列B+树特点，而要从数据库实际应用场景出发，阐述B+树的优势。重点在于解释B+树在范围查询和磁盘IO方面的性能。可以通过引出哈希索引、聚簇索引等话题，引导面试官进一步探讨。




前置知识：

数据库索引通常采用 B+ 树结构，主要基于以下几个关键原因：


实际应用案例
数据库索引通常采用 B+ 树结构，这主要有以下原因：

首先，B 树和 B+ 树的结构差异是一个关键因素。在 B 树中，每个节点不仅存储索引键，还存储实际的数据记录，这就导致数据分散在所有节点中。而 B+ 树的设计则是将所有数据仅存储在叶子节点，非叶子节点只存储索引键。这种结构使得 B+ 树在进行范围查询时更加高效，因为所有的叶子节点通过链表相连，形成一个有序链表，这样就能快速遍历。

其次，B+ 树提供了更稳定的查询性能。所有的查询都必须到达叶子节点，这样查询路径的长度是一致的，因此查询性能更加稳定，时间复杂度也稳定在 O(log n)。相比之下，B 树的查询可能在非叶子节点结束，虽然在最佳情况下是 O(1)，但平均和最坏情况下的性能不如 B+ 树。数据库系统通常更倾向于稳定的性能表现。

------------------------------

572. *如何选择索引列的顺序？, page url: https://www.mianshi.icu/question/detail?id=1120
572-1120-答案：
简答题，校招和初级岗位面试中常见。
在设计复合索引时，列顺序直接影响查询性能，需综合考虑以下原则：
在设计复合索引的时候，列的顺序非常重要，它直接影响到查询的性能。我们需要综合考虑下面几个原则：

首先是等值条件优先，范围条件靠后。也就是说，把等值匹配的列，比如 =, IN 这些，放在索引的最左边（满足最左匹配原则）；范围查询的列，比如 >, <, BETWEEN 这些，放在索引的最后面。因为范围条件之后的索引列就不能用来做查询过滤了，但是可以用来排序或者做覆盖查询。举个例子，如果查询条件是 WHERE user_id=100 AND order_date > '2023-01-01'，那么索引就应该是 (user_id, order_date)。

其次，对于等值条件，要按照选择性从高到低排列。选择性高的列，也就是唯一值比较多的字段，要放在前面，这样可以更快地过滤掉大量不相关的记录。比如说，(手机号, 性别) 就比 (性别, 手机号) 要好，即使查询同时使用了这两列。

------------------------------

573. *什么是索引覆盖？, page url: https://www.mianshi.icu/question/detail?id=1121
573-1121-答案：
简答题，校招和初级岗位面试中常见。

索引覆盖是一种数据库查询优化技术，也是查询执行的一种状态。它指的是在执行某个查询时，数据库引擎可以仅通过访问索引就获取到该查询所需的全部数据，而无需回表访问实际的数据行。换句话说，如果查询所需的所有字段（包括 SELECT 列表中的字段、WHERE 子句中的过滤条件字段、ORDER BY 或 GROUP BY 中的排序字段等）都包含在某个索引中（即满足最左前缀原则），那么这个索引就“覆盖”了该查询，这种情况称为索引覆盖。

索引覆盖的优势：


注意事项：
索引覆盖其实是一种很巧妙的查询优化技术，它的核心思想是让查询所需的所有数据都能直接从索引中获取，这样就不用再去访问实际的数据行了。

换句话说，如果我们的查询只需要索引里已经包含的字段，不管是 SELECT 列表中的字段，还是 WHERE 子句的过滤条件，或者是 ORDER BY、GROUP BY 用到的排序字段，只要这些字段都在索引里面并且能够满足最左前缀匹配，那么这个索引就算是"覆盖"了这个查询。
同时，我们说索引覆盖效果好的一个前提条件是：索引能装进去内存里面。如果索引本身也要从磁盘里面加载，或者触发了内存 swap，那么覆盖索引有一定的效果，但是效果就没有预期中的好了。

------------------------------

574. *覆盖索引和索引覆盖的含义及区别？, page url: https://www.mianshi.icu/question/detail?id=1123
574-1123-答案：
简答题，校招和初级岗位面试中常见。
前置知识：
“覆盖索引”和“索引覆盖”本质上是同一查询优化技术的不同表述。

如果一个索引中的列包含了某个查询所需要的所有字段（包括 SELECT 语句中需要返回的字段、WHERE 子句的过滤条件、以及 ORDER BY 或 GROUP BY 中用到的排序字段等），数据库在执行这一查询时，可以直接从索引中返回结果，而不必再去访问实际的数据行，那么该索引就是覆盖了这个查询的“覆盖索引”，而这个查询就是被“索引覆盖”的查询。
同时，我们说覆盖索引或者索引覆盖效果好的一个前提条件是：索引能装进去内存里面。如果索引本身也要从磁盘里面加载，或者触发了内存 swap，那么覆盖索引有一定的效果，但是效果就没有预期中的好了。

------------------------------

575. *数据库索引有哪些作用？, page url: https://www.mianshi.icu/question/detail?id=1125
575-1125-答案：
简答题，校招和初级岗位面试中常见。
前置知识：
数据库索引有以下几个作用：

首先是加速查询。索引通过建立一个数据之间的有序映射，帮助数据库快速定位到我们要找的数据，避免了全表扫描，这样查询效率就大大提高了。打个比方，如果我们在一个有上百万条数据的表里找“name=alex”的记录，要是用上了覆盖索引，就能直接返回数据，而且时间复杂度也能从 O(n) 降到 O(log n)，这个提升是非常明显的。我在平时优化慢查询的时候，也是先看查询有没有命中索引，索引创建是否合适。

其次是优化排序和分组。索引本身是有序的，所以当我们进行排序或者分组操作的时候，就可以利用这个特性，避免对整个表的数据进行排序和分组，这样也能节省很多时间。

------------------------------

576. *使用数据库索引时需要注意什么？, page url: https://www.mianshi.icu/question/detail?id=1126
576-1126-答案：
简答题，校招和初级岗位面试中常见。
前置知识：
在使用数据库索引的时候需要注意以下几点：

首先，尽量避免使用会导致索引失效的SQL语法。比如在 WHERE 子句中使用函数或者表达式，像 "WHERE YEAR(create_time)=2023" 这样的语句，索引就失效了。此时可以使用范围查询，像 "BETWEEN '2023-01-01' AND '2023-12-31'"来替代。另外，在使用通配符的时候也要小心，像 "LIKE '%北京'" 这样的语句是无法使用索引的，而 "LIKE '北京%'" 就可以。还有隐式类型转换，如果查询条件与索引列的类型不一致，也会导致索引失效。比如，如果 id 是一个数字字段，但我在查询的时候用了字符串 "WHERE id='100' "，索引就失效了。

其次，确认索引是否生效，我会经常使用 EXPLAIN 命令来分析执行计划。通过查看 EXPLAIN 的结果，可以知道查询是否命中了索引。如果 type 字段是 ref 或者 range，那就说明用到了索引；如果 type 字段是 ALL，那就表示进行了全表扫描，索引没有生效。此外，我会尽量使用覆盖索引，也就是查询所需的所有数据直接从索引中获取，这样可以避免回表查询，提高查询效率。如果 EXPLAIN 的 Extra 列显示 "Using index"，那就说明使用了覆盖索引。

------------------------------

577. *设计数据库索引时需要注意什么？, page url: https://www.mianshi.icu/question/detail?id=1127
577-1127-答案：
简答题，校招和初级岗位面试中常见。
前置知识：
在设计数据库索引时，我通常会关注很多细节，确保整个方案综合考虑了查询性能和写入效率。

首先，我会根据具体的查询场景选择合适的索引存储结构，比如说，B+树索引它非常适合范围查询（例如 WHERE age > 20）和排序操作（例如 ORDER BY create_time），利用数据的有序性帮助快速定位目标数据；而如果查询都是等值查询，比如 WHERE id = 100，哈希索引可能更合适，不过它就不支持范围查询和排序。对于简单的文本关键词搜索，全文索引可能是一个更好的选择。

另外，我在选择索引类型时，也会根据具体业务场景来做决策。如果只是单纯为了加速查询，普通索引就可以满足要求；但如果需要确保数据的唯一性，比如邮箱或身份证号这类场景，那么唯一索引就是必选方案。

------------------------------

578. Redis的数据类型有哪些？分别适用于什么场景？, page url: https://www.mianshi.icu/question/detail?id=1129
578-1129-答案：
简单题，在校招和初中级岗位面试中非常常见。

回答这个问题，你一边要把 Redis 有的数据结构都列举一边，一边要指出这些数据结构对应的底层实现。而后，你要进一步举几个例子，论述不同的数据结构可以用于不同的场景，最好就是用你项目经历中有特色的 Redis 案例。

最后你通过总结一般的选择数据结构的原则来刷出最后一个亮点。
Redis有哪些数据结构（mianshi.icu）？的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
常用的数据结构有五种。

第一种是 String，其底层实现是简单字符串（SDS），简单来说就是一个字节数组。大多数的缓存用的都是这种数据结构。

第二种是 List，底层实现有 ziplist，quicklist。一般存储数组类的数据就很适合。我曾经用 List 结构实现过滑动窗口算法。
从实践中来说，使用 Redis 最重要的问题就是选择合适的数据结构。我个人总结了一些选择数据结构的原则。

第一个是首先要考虑的是功能性，也就是数据结构支持的操作要能满足业务场景需要，这可以说是一切的前提了。

第二个则是要考虑性能问题。性能问题包含两方面，一个是内存使用量要尽可能少，另外一个是读写操作要尽可能快。尤其是在使用一些复杂数据结构的时候，千万要小心过大的数据结构拖累 Redis 性能。比如说最典型的遍历操作，如果要是遍历巨大的 List，那么会严重拖累 Redis 的性能。

------------------------------

579. Redis 中常见的数据类型有哪些？, page url: https://www.mianshi.icu/question/detail?id=1130
579-1130-答案：
简单题，在校招和初中级岗位面试中非常常见。

回答这个问题，你一边要把 Redis 有的数据结构都列举一边，一边要指出这些数据结构对应的底层实现。而后，你要进一步举几个例子，论述不同的数据结构可以用于不同的场景，最好就是用你项目经历中有特色的 Redis 案例。

最后你通过总结一般的选择数据结构的原则来刷出最后一个亮点。
Redis有哪些数据结构（mianshi.icu）？的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
常用的数据结构有五种。

第一种是 String，其底层实现是简单字符串（SDS），简单来说就是一个字节数组。大多数的缓存用的都是这种数据结构。

第二种是 List，底层实现有 ziplist，quicklist。一般存储数组类的数据就很适合。我曾经用 List 结构实现过滑动窗口算法。
从实践中来说，使用 Redis 最重要的问题就是选择合适的数据结构。我个人总结了一些选择数据结构的原则。

第一个是首先要考虑的是功能性，也就是数据结构支持的操作要能满足业务场景需要，这可以说是一切的前提了。

第二个则是要考虑性能问题。性能问题包含两方面，一个是内存使用量要尽可能少，另外一个是读写操作要尽可能快。尤其是在使用一些复杂数据结构的时候，千万要小心过大的数据结构拖累 Redis 性能。比如说最典型的遍历操作，如果要是遍历巨大的 List，那么会严重拖累 Redis 的性能。

------------------------------

580. *介绍一下Redis的Hash类型, page url: https://www.mianshi.icu/question/detail?id=1131
580-1131-答案：
简单题，在校招、初中级岗位面试中非常常见。考察对Redis数据类型的深入理解，重点介绍其基本概念及内部编码（底层实现），并结合实际应用介绍使用场景。
前置知识：
Redis 的 Hash 类型是一种键值对集合，每个 Hash 中包含多个字段（field），每个字段都有一个对应的值（value）。Hash 类型的键是一个字符串，而字段和值也是字符串。

------------------------------

581. *介绍一下Redis的SortedSet（zSet）类型, page url: https://www.mianshi.icu/question/detail?id=1132
581-1132-答案：
简单题，在校招、初中级岗位面试中非常常见。考察对Redis数据类型的深入理解，重点介绍其基本概念及内部编码（底层实现），并结合实际应用介绍使用场景。
前置知识：
Redis 的 zSet（有序集合）是一种特殊的数据结构，结合了集合（Set）和排序（Sorted）的特性。zSet 中的每个元素都有一个唯一的成员（member）和一个与之关联的分数（score），用于对成员排序。分数相同的成员会按照字典顺序排序。

------------------------------

582. *介绍一下Redis的Set类型, page url: https://www.mianshi.icu/question/detail?id=1133
582-1133-答案：
简单题，在校招、初中级岗位面试中非常常见。考察对Redis数据类型的深入理解，重点介绍其基本概念及内部编码（底层实现），并结合实际应用介绍使用场景。
前置知识：
Redis 的 Set 类型是一种无序的字符串集合，具有唯一性，即集合中的每个元素都是唯一的，不能重复。Set 类型支持多种操作，如添加、删除、查找、集合运算等。

------------------------------

583. *你用Redis中的zset类型实现过哪些功能？, page url: https://www.mianshi.icu/question/detail?id=1136
583-1136-答案：
基础题，校招和初中级岗位中常考。主要考验你对Redis有序集合的实际应用能力。重点阐述如何利用zset的score特性和有序性解决实际问题。
前置知识：
我用Redis的zset类型实现过热门帖子排行榜功能。实现方式大致如下：

------------------------------

584. *介绍一下Redis中SortedSet（ZSet）类型的底层实现, page url: https://www.mianshi.icu/question/detail?id=1137
584-1137-答案：
略难的题，高频题，在各个层级的面试里面都是高频题目。
前置知识：
Redis 中的 ZSet 类型使用 ziplist（压缩列表）和 skiplist（跳跃表） + dict（字典） 作为底层实现。

当 ZSet 中的元素数量较少（默认阈值：128个）且每个元素的值都比较短（默认阈值：64字节）时，Redis 会使用 ziplist 作为 ZSet 的底层实现。ziplist 的优点是，内存空间连续，内存利用率高，存储开销小。但 ziplist 也存在明显的局限性，例如插入和删除元素时需要移动大量数据，性能较低；查找元素需要遍历整个列表，时间复杂度为 O(N)；不适合存储大量或大尺寸的元素。

------------------------------

585. *Redis中Set和SortedSet区别是什么？, page url: https://www.mianshi.icu/question/detail?id=1138
585-1138-答案：
基础题，校招和初中级岗位常考。

考察对Redis数据结构的理解。回答时要清晰区分Set和SortedSet的定义和特性，以及如何利用这些特性解决实际问题。回答后可以引导到具体的应用场景，比如如何使用Set和SortedSet实现不同的功能，展示对Redis数据结构的全面理解。
前置知识：
Set 和 Sorted Set (zSet) 是 Redis 中两种不同的数据结构，它们的主要有以下区别：

首先是有序性，Set 是无序集合，元素没有特定的排列顺序。Sorted Set 是有序集合，每个元素都关联一个分数（score），元素根据分数进行排序。分数相同的元素按照字典顺序排列。

其次是底层实现，Set 使用 intset (当存储整数且元素数量较少时) 或 hashtable 作为底层实现。而 Sorted Set 使用 ziplist (当数据量较小时) 或 skiplist 作为底层实现。

------------------------------

586. 什么是Redis持久化？请描述RDB和AOF的区别。, page url: https://www.mianshi.icu/question/detail?id=1139
586-1139-答案：
略难的题，Redis 面试一般比较少考察持久化机制，因为在实践中就不推荐开启持久化，只有在一些很罕见的情况下才会考虑开启持久化功能。

你在这个问题下，可以深入阐述讨论 AOF 的刷盘机制，以及 COW 机制来赢得竞争优势。
Redis 的持久化机制是如何运作的？（mianshi.icu）的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
Redis 持久化是指将内存中的数据保存到磁盘，以确保数据在 Redis 服务器重启或崩溃后不会丢失。

Redis有两种主要的持久化机制：RDB（Redis DataBase）和AOF（Append Only File）。

RDB通过创建数据库快照，将内存中的数据以二进制形式保存到磁盘上的RDB文件中。触发方式包括手动（SAVE或BGSAVE命令）和自动（配置文件设置）。优点是文件体积小、恢复速度快，适合灾难恢复。缺点是可能存在数据丢失风险（取决于快照间隔），且生成快照时会有很大的性能开销。
这里值得一提的是 AOF 的刷盘机制，它支持三种策略：always 每次写操作后立即同步、everysec 每秒同步一次、no 由操作系统决定。
另外一个很有意思的点是RDB持久化中采用的COW（Copy-On-Write）思路是指在创建数据库快照时，并不直接复制整个数据集，而是仅在数据被修改时才复制相应的页面。这种机制通过延迟复制，减少了不必要的写操作和内存消耗，提高了效率。

------------------------------

587. Redis的持久化机制有哪些？各有什么特点？, page url: https://www.mianshi.icu/question/detail?id=1140
587-1140-答案：
略难的题，Redis 面试一般比较少考察持久化机制，因为在实践中就不推荐开启持久化，只有在一些很罕见的情况下才会考虑开启持久化功能。

你在这个问题下，可以深入阐述讨论 AOF 的刷盘机制，以及 COW 机制来赢得竞争优势。
Redis 的持久化机制是如何运作的？（mianshi.icu）的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
Redis有两种主要的持久化机制：RDB（Redis DataBase）和AOF（Append Only File）。

RDB通过创建数据库快照，将内存中的数据以二进制形式保存到磁盘上的RDB文件中。触发方式包括手动（SAVE或BGSAVE命令）和自动（配置文件设置）。优点是文件体积小、恢复速度快，适合灾难恢复。缺点是可能存在数据丢失风险（取决于快照间隔），且生成快照时会有很大的性能开销。

RDB 本身也是进行全量主从复制的核心结构。
这里值得一提的是 AOF 的刷盘机制，它支持三种策略：always 每次写操作后立即同步、everysec 每秒同步一次、no 由操作系统决定。
另外一个很有意思的点是RDB持久化中采用的COW（Copy-On-Write）思路是指在创建数据库快照时，并不直接复制整个数据集，而是仅在数据被修改时才复制相应的页面。这种机制通过延迟复制，减少了不必要的写操作和内存消耗，提高了效率。

------------------------------

588. Redis 的 RDB 和 AOF 有什么区别？各自的优缺点是什么？, page url: https://www.mianshi.icu/question/detail?id=1141
588-1141-答案：
略难的题，Redis 面试一般比较少考察持久化机制，因为在实践中就不推荐开启持久化，只有在一些很罕见的情况下才会考虑开启持久化功能。

你在这个问题下，可以深入阐述讨论 AOF 的刷盘机制，以及 COW 机制来赢得竞争优势。
Redis 的持久化机制是如何运作的？（mianshi.icu）的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
RDB通过创建数据库快照，将内存中的数据以二进制形式保存到磁盘上的RDB文件中。触发方式包括手动（SAVE或BGSAVE命令）和自动（配置文件设置）。优点是文件体积小、恢复速度快，适合灾难恢复。缺点是可能存在数据丢失风险（取决于快照间隔），且生成快照时会有很大的性能开销。

RDB 本身也是进行全量主从复制的核心结构。
这里值得一提的是 AOF 的刷盘机制，它支持三种策略：always 每次写操作后立即同步、everysec 每秒同步一次、no 由操作系统决定。
另外一个很有意思的点是RDB持久化中采用的COW（Copy-On-Write）思路是指在创建数据库快照时，并不直接复制整个数据集，而是仅在数据被修改时才复制相应的页面。这种机制通过延迟复制，减少了不必要的写操作和内存消耗，提高了效率。

------------------------------

589. *Redis 在生成 RDB 文件时如何处理请求？, page url: https://www.mianshi.icu/question/detail?id=1142
589-1142-答案：
略难的题，Redis 面试一般比较少考察持久化机制，因为在实践中就不推荐开启持久化，只有在一些很罕见的情况下才会考虑开启持久化功能。


前置知识：

Redis 在生成 RDB 文件时，为了尽可能减少对正常请求的影响，采用了以下处理方式：


具体流程
Redis 在生成 RDB 文件时，主进程会先通过 fork()系统调用创建子进程来负责 RDB 文件的生成及替换，然后再继续处理客户端的读写请求。如果主进程在处理写请求时修改了某个内存页面，并且此时父进程和子进程共享该内存页面，操作系统会使用 COW（Copy-On-Write）机制将该页面复制到新的内存地址，以确保子进程仍然可以访问未修改的原始数据。
COW思路在计算机系统中非常常见，广泛应用于文件系统、虚拟内存管理、进程创建等领域，通过共享和延迟复制资源，优化性能和资源利用率。

------------------------------

590. *Redis是什么？它的主要用途有哪些？, page url: https://www.mianshi.icu/question/detail?id=1143
590-1143-答案：
简单题，校招和初级岗位常考。主要考察对Redis的基本概念和应用场景的理解。一般可以看做是深入讨论 Redis 的前奏。
前置知识：


Redis（Remote Dictionary Server）是一个开源的、高性能的基于内存的键值存储系统，通常被用作数据库、缓存和消息中间件。它支持多种数据结构，如字符串、哈希、列表、集合和有序集合等，，并提供了丰富的操作命令。因其高效的读写性能和灵活的使用场景而受到广泛欢迎。
Redis，全称是 Remote Dictionary Server（远程字典服务器），是一个开源的、高性能的基于内存的键值存储系统。它通常被用作数据库和缓存，有时也用作消息中间件。Redis 支持多种数据结构，比如字符串、哈希、列表、集合和有序集合等，并且提供了丰富的操作命令。由于它高效的读写性能和灵活的使用场景，Redis 在业界得到了广泛的欢迎。

在实际应用中，Redis 的用途非常广泛。最常见的场景是作为缓存层，用于加速数据访问。通过缓存热点数据，Redis 能够减少对后端数据库的频繁访问，从而显著提高系统的响应速度。在 Web 应用中，Redis 还可以存储用户的会话信息，比如登录状态和购物车数据，支持快速读写和过期管理。在分布式系统中，它能够实现会话的共享和一致性，避免会话数据丢失或重复存储。

其次，Redis 的有序集合非常适合用于实现排行榜功能，支持快速计算和更新排名，并支持范围查询，比如获取排行榜前N行数据。

------------------------------

591. Redis 是单线程的，为什么能这么快？, page url: https://www.mianshi.icu/question/detail?id=1144
591-1144-答案：
略难的题，之所以略难是因为这个题目很容易错，在校招和初中级岗位中比较常见。

Redis 之所以那么快，最关键就是两个：纯内存操作和IO多路复用，剩下的都不值一提。很容易犯的错误就是说 Redis 那么快是因为用了单线程模型，这个有点扯淡，要是单线程是原因，那么你的 Kafka 干嘛不用单线程？
Redis 为什么那么快？（mianshi.icu）的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
Redis 高性能的最主要因素有两个：高效的 IO 多路复用和纯内存操作。

首先，IO 多路复用是高性能中间件的标配，Redis 基于 IO 多路复用可以支撑非常多的连接，并且高效读写数据。

其次 Redis 是基于内存的数据库，大部分操作都是内存操作，天然就非常快。作为对比，Kafka 这种 IO 密集型的应用，性能始终不如 Redis。
但是我要澄清一个问题。就是我们业界里面有一种错误的说法，说 Redis 的单线程模型是 Redis 高性能的原因。

这句话其实不对，倒果为因了。虽然 Redis 的主线程是单线程的，确实在一定程度上提高了 Redis 的性能。

------------------------------

592. *为什么 Redis 设计为单线程？6.0 版本为何引入多线程？, page url: https://www.mianshi.icu/question/detail?id=1145
592-1145-答案：
简单题，在校招、初中级岗位面试中非常常见。

回答这个问题的关键点就是指出 Redis 的线程模型的演进，而后强调一下多线程的模式不到逼不得已不要使用。
前置知识：
Redis 并非完全意义上的单线程。虽然命令处理是单线程的，但它的持久化、异步删除等操作是使用额外的线程或进程来执行的。这种设计使得 Redis 能够充分利用多核 CPU 的性能，同时保持单线程模型带来的简单性和可预测性。

------------------------------

593. *Redis 和 Memcached 有哪些区别？, page url: https://www.mianshi.icu/question/detail?id=1146
593-1146-答案：
简单题，校招和初级岗位面试中常见。

需要理解 Redis 和 Memcached 的基本概念、特性和应用场景。回答时要清晰地描述 Redis 和 Memcached 的主要区别。这是一个很好的引导题，可以引导到很多关联面试题。

Redis 和 Memcached 都是常用的内存缓存系统，但它们的设计目标、功能特性和适用场景有所不同。以下是它们的主要区别：
Redis 和 Memcached 都是挺常用的内存缓存系统，但它们的设计目标、功能特性和适用场景不太一样。二者主要有以下区别：

首先是数据类型方面，Redis 支持多种数据类型，包括字符串（String）、列表（List）、集合（Set）、有序集合（Sorted Set）、哈希（Hash）、位图（Bitmap）、HyperLogLog 和地理空间索引（Geospatial Index）等等。而 Memcached 就只支持字符串（String）类型。

其次是在持久化方面，Redis 是支持持久化的，可以将内存中的数据保存到磁盘中，防止数据丢失。Redis 提供了两种持久化方式：一种是 RDB（Redis Database），定期将内存中的数据生成快照保存到磁盘中；另一种是 AOF（Append Only File），将每次写操作追加到日志文件中，重启时通过重放日志文件恢复数据。Memcached 就不支持持久化，数据只存储在内存中，重启后数据会丢失。

------------------------------

594. Redis 的应用场景有哪些？, page url: https://www.mianshi.icu/question/detail?id=1147
594-1147-答案：
基础题，高频题。一般可以看做是深入讨论 Redis 的前奏。

回答这个问题的关键要看你平时准备了 Redis 哪些案例。而后你在整个回答的过程中你就可以稍微提及这些案例，但是不需要阐述细节，等待面试官进一步追问。而且还要注意的是，你选择的案例不能太大众化，要比你当前求职的岗位稍微高级那么一点点。
你用 Redis 解决过什么问题？（mianshi.icu）的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
最典型的使用场景就是用 Redis 来作为缓存，一般来说根据缓存的数据不同，可以是缓存字符串，或者是 List 结构、哈希结构。一般来说引入 Redis 作为缓存之后，性能会有数量级的提升。
我还用 Redis 处理过更加复杂的问题，比如说榜单问题。这个榜单问题的核心就是利用 ZSet 来排序，对应的 score 则是根据排序规则来计算。比如说根据时间、点赞和评论数等计算一个热度，把这个热度作为 score。每次获取前几名，也就是执行一次 Range 而已。
我还用 Redis 来设计过分布式锁。使用 Redis 来设计分布式锁的关键点有两个：使用 SETNX 来排他性的设置一个值；要考虑续约的问题。也就是在设置了一定的过期时间之后，如果在快要过期了业务还没执行完毕，那么要考虑续约的问题，防止分布式锁因为过期而失效。

------------------------------

595. *Redis的过期策略和内存淘汰策略有哪些？, page url: https://www.mianshi.icu/question/detail?id=1148
595-1148-答案：
简单题，在校招和初中级岗位中比较常见。

本来 Redis 的过期删除其实没啥好说的，但是因为 Redis 有过一个 BUG 导致后面面试题就经常问了。你在回答的时候可以通过抽象总结一般的缓存如何解决过期问题来赢得竞争优势。
前置知识：
Redis 的过期策略主要有定时删除和延迟删除两种策略。

定时删除的关键是随机抽查，点到即止。也就是说定时删除的时候，会从一个随机位置开始，遍历后续的 key。如果 key 已经过期了就直接删除。而在遍历的过程中，遍历够了一定数量的 key，就会停下来。这主要是为了避免消耗太多时间和 CPU 资源在清理过期 key 上。

延迟删除是指在读取 key 的时候，如果要是 key 已经过期了，就会直接删除，并且返回 key 不存在的响应。
Redis 的这种过期删除策略，在大部分缓存实现里面都能见到，比如说本地缓存的实现差不多也是借助定时删除和延迟删除的策略。

------------------------------

596. *Redis 中有哪些内存淘汰策略？, page url: https://www.mianshi.icu/question/detail?id=1149
596-1149-答案：
简单题，在校招和初中级岗位中比较常见。

重点解释每种策略的特点和适用场景。可以结合实际应用场景，比如如何选择合适的策略来优化缓存性能。
当 Redis 的内存使用达到上限时，会触发内存淘汰策略，用于决定哪些键应该被删除以释放内存。可以通过配置文件中的 maxmemory-policy 参数进行设置。以下是 Redis 提供的几种内存淘汰策略：


LRU 与 LFU 的区别：
Redis 中有以下内存淘汰策略：

首先是 noeviction，这是默认策略。它不会淘汰任何数据，当内存使用达到上限时，不再接受新的写入操作，直接返回错误。这种策略适用于对数据完整性和一致性要求非常高的场景，确保不丢失任何数据。

其次是 allkeys-lru和volatile-lru，这两个策略分别是从所有键和设置了过期时间的键中移除最近最少使用的键，适合所有键的访问频率差不多的场景。
选择内存淘汰策略时，需要根据具体的应用场景和数据访问模式进行权衡。

如果对数据完整性要求非常高，可以选择 noeviction。

如果键的访问频率都差不多的场景，可以考虑 allkeys-lru；在此基础上希望只淘汰设置了过期时间的键，可以选择 volatile-lru 。

------------------------------

597. Redis数据过期后的删除策略是什么？, page url: https://www.mianshi.icu/question/detail?id=1150
597-1150-答案：
简单题，在校招和初中级岗位中比较常见。

本来 Redis 的过期删除其实没啥好说的，但是因为 Redis 有过一个 BUG 导致后面面试题就经常问了。你在回答的时候可以通过抽象总结一般的缓存如何解决过期问题来赢得竞争优势。
Redis 是如何删除过期数据的？（mianshi.icu）的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
Redis 数据过期后采取的是定时删除和延迟删除两种策略。
Redis 的这种策略，在大部分缓存实现里面都能见到，比如说本地缓存的实现差不多也是借助定时删除和延迟删除的策略。

------------------------------

598. ***Redis淘汰策略有哪些？你项目用的是哪个？为什么？, page url: https://www.mianshi.icu/question/detail?id=1151
598-1151-答案：
基础题，在校招和初中级岗位面试中比较常见。
前置知识：
Redis 中有以下内存淘汰策略：

首先是 noeviction，这是默认策略。它不会淘汰任何数据，当内存使用达到上限时，不再接受新的写入操作，直接返回错误。这种策略适用于对数据完整性和一致性要求非常高的场景，确保不丢失任何数据。

其次是 allkeys-lru和volatile-lru，这两个策略分别是从所有键和设置了过期时间的键中移除最近最少使用的键，适合所有键的访问频率差不多的场景。

------------------------------

599. *进程和线程的区别是什么？它们各自的使用场景是什么？, page url: https://www.mianshi.icu/question/detail?id=1152
599-1152-答案：
基础题，在校招和初级工程师面试中比较常见。

要在这个问题之下刷出亮点，最好就是讲清楚沿着进程、线程和协程三者的演进趋势。
前置知识：
进程和线程是操作系统中两个重要的概念，两者主要有以下区别。

首先从定义上来说，进程是资源分配的基本单位，拥有独立的地址空间，而线程是进程内部的执行单元，是 CPU 调度和分派的基本单位，用于实现并发执行。

其次从资源管理上来说，进程间的资源是隔离的，而线程共享进程的资源，上下文切换较快。
整体上来说，这体现了一个趋势：就是在当下任务越来越重，并且计算机性能也越来越好的时候，我们需要更细粒度、更轻量级的机制。

比如说进程到线程，我们就有了多线程编程，这样可以在共享别的资源的时候，独立调度不同的线程，充分利用 CPU 的计算能力。

------------------------------

600. 进程间通信的方式有哪些？各有什么特点？, page url: https://www.mianshi.icu/question/detail?id=1153
600-1153-答案：
简单题，在校招和初中级岗位面试中有点可能会遇到。

在一些情况下，面试官问到这个问题，其实并不是想问操作系统的内容，而是想要借助这个话题问你 RPC 和微服务架构相关的内容。

所以你在这个问题之下，除了提及教科书上的内容，一定要提及分布式环境下的通信方式。
进程间通信方式有哪些？（mianshi.icu）的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
进程之间的通信方式有很多。

第一种是共享内存。也就是多个进程共享同一块内存区域，实现高效的数据交换；

第二种是管道。管道可以认为是一种数据结构，可以在进程之间传递数据，多用于父子进程之间；
而在分布式环境下，还有更多的通信方式：

------------------------------

601. *什么是进程上下文切换？它为什么开销大？, page url: https://www.mianshi.icu/question/detail?id=1154
601-1154-答案：
简单题，在校招中有可能遇到，社招不太会遇到。

在这个问题之下，可以综合对比线程是如何切换，并评价两者之间的性能差异原因，从而刷出亮点。
前置知识：


进程上下文切换的开销主要来自以下几个方面：
进程切换的步骤有几个。

第一个步骤是中断处理。也就是说进程切换其实是依赖中断来触发的，而后进入中断处理，开始执行切换的代码。

第二个步骤是保存当前进程的上下文。
从进程切换的这些步骤也能看出来，它的性能是比较差。

------------------------------

602. *进程上下文切换时需要保存什么？, page url: https://www.mianshi.icu/question/detail?id=1155
602-1155-答案：
简单题，在校招中有可能遇到，社招不太会遇到。
进程上下文切换是操作系统在多任务环境中实现进程调度的重要机制。当操作系统决定将 CPU 从一个进程切换到另一个进程时，需要保存当前进程的状态，并加载下一个进程的状态。为了确保进程能够在切换后正确恢复执行，操作系统需要保存以下主要内容：
进程上下文切换时需要保存以下主要内容：

首先是CPU 寄存器状态，例如通用寄存器、程序计数器（PC）、栈指针（SP）、基址指针（BP）、状态寄存器（FLAGS）等。

其次是进程状态信息，例如进程状态（如运行、就绪、阻塞等）、优先级信息、调度信息（时间片、调度策略等）等信息。

------------------------------

603. *什么是缓存穿透？如何解决？, page url: https://www.mianshi.icu/question/detail?id=1156
603-1156-答案：
基础题，高频面试题。校招和初中级岗位面试中常考。

需要理解缓存穿透的概念和解决方案，要解释每种方案的原理、优缺点和适用场景，并注意结合实际案例。还可以引导到缓存雪崩、缓存击穿等相关问题，展示对缓存问题的系统性理解。
缓存穿透是指查询一个不存在的数据，由于缓存中没有该数据，请求会直接穿透缓存到达数据库。查询数据库时，也没有该数据，此时通常的做法是不缓存不存在的数据直接返回结果。但这也埋下了祸根，如果大量请求查询不存在的数据，会对数据库造成很大的压力，甚至导致数据库崩溃。


在实际应用中，通常需要组合使用多种方案来缓解缓存穿透问题。比如，先提前预热缓存，再使用参数校验，在查询缓存之前，过滤掉明显不合法的参数。接着用布隆过滤器过滤掉大部分无效请求，然后用缓存默认值/特殊值来处理少量的穿透请求，最后结合请求限流来防止恶意攻击。与此同时，设置缓存命中率监控，及异常告警，并定期根据业务特点调整缓存策略。
缓存穿透是一个常见的问题，指的是查询一个不存在的数据。当请求的数据在缓存中找不到时，请求就会直接穿透缓存，访问数据库。如果数据库中也没有这个数据，通常的做法是不缓存不存在的数据，直接返回结果。但这样一来，如果有大量请求查询不存在的数据，就会对数据库造成很大的压力，甚至可能导致数据库崩溃。

造成缓存穿透的原因有几个方面。首先是无效请求，比如用户请求的数据在数据库中根本不存在，像请求一个错误的 ID。其次是恶意攻击，攻击者可能故意发送大量不存在的请求，试图消耗系统资源。

为了解决这个问题，可以考虑以下两种解决方案。

------------------------------

604. *什么是缓存击穿？如何解决？, page url: https://www.mianshi.icu/question/detail?id=1157
604-1157-答案：
基础题，高频面试题。校招和初中级岗位面试中常考。

需要理解缓存击穿的概念和解决方案，要解释每种方案的原理、优缺点和适用场景，并注意结合实际案例。还可以引导到缓存雪崩、缓存穿透等相关问题，展示对缓存问题的系统性理解。
缓存击穿是指某个热点数据在缓存中过期或被删除时，短时间内大量并发请求同时访问该数据，导致请求直接穿透到数据库，造成数据库压力骤增，甚至可能导致数据库及整个系统崩溃。


在实际应用中，通常需要组合使用多种方案来解决缓存击穿问题。比如，首先对热点数据进行预热，在系统启动时就将这些数据加载到缓存中，避免冷启动时的击穿风险。接着，使用互斥锁来限制只有一个线程可以访问数据库并重建缓存。同时，为了进一步提升性能，可以引入本地缓存，将热点数据缓存在应用服务器本地，减少对分布式缓存的依赖。此外，还可以考虑将热点 key 设置为永不过期，并通过后台线程定期更新，以保证数据的一致性。最后，建立缓存命中率监控和异常告警机制，并定期根据业务特点调整缓存策略，例如调整互斥锁的超时时间、本地缓存的容量等，以达到最佳的性能和稳定性。
缓存击穿是指某个热点数据在缓存中过期或被删除时，短时间内大量并发请求同时访问该数据，导致请求直接穿透到数据库，造成数据库压力骤增，甚至可能导致数据库及整个系统崩溃。

缓存击穿产生的原因主要有两个。一是热点数据过期，也就是某个 key 的访问频率非常高，成为了热点 key，但这个 key 在缓存中过期了。二是高并发请求，在缓存失效的瞬间，多个请求同时到达，导致数据库瞬间承受大量请求。

为了解决这个问题，有几种方案可以考虑。

------------------------------

605. *什么是缓存雪崩？如何解决？, page url: https://www.mianshi.icu/question/detail?id=1158
605-1158-答案：
基础题，高频面试题。校招和初中级岗位面试中常考。

需要理解缓存雪崩的概念和解决方案，要解释每种方案的原理、优缺点和适用场景，并注意结合实际案例。还可以引导到缓存击穿、缓存穿透等相关问题，展示对缓存问题的系统性理解。

缓存雪崩是指在某一时刻，缓存中大量的 key 同时过期失效，导致大量请求直接访问数据库，引起数据库压力剧增，甚至崩溃。以下是对缓存雪崩的详细解析及解决方案。


在实际应用中，通常需要组合使用多种方案来解决缓存雪崩问题。比如，首先对缓存的过期时间进行随机化，避免大量 key 同时过期。同时，构建多级缓存架构，引入本地进程缓存，减少对分布式缓存的依赖。针对可能出现的系统故障，引入熔断降级机制，当缓存或数据库出现异常时，能够快速切换到降级方案。此外，搭建高可用的缓存集群，通过主从架构和哨兵机制提高系统的可靠性。最后，建立监控告警系统，实时监控缓存命中率、响应时间和错误率，及时发现并处理潜在的性能问题。定期进行压力测试和预案演练，不断优化缓存策略，提高系统的整体稳定性和性能。
缓存雪崩就是在同一时间段内，大量缓存 key 集中过期，导致大量请求直接访问数据库，瞬间巨大的流量压垮数据库，造成系统整体性能下降甚至宕机的现象。

这种情况产生的原因主要有两方面。一是大量缓存key的过期日期相同，可能同一批次加入缓存中的。二是缓存服务宕机，缓存服务器发生故障，导致所有缓存失效。

为了解决这个问题，有几种方案可以考虑。

------------------------------

606. *什么是缓存穿透、缓存雪崩、缓存击穿？如何解决？, page url: https://www.mianshi.icu/question/detail?id=1159
606-1159-答案：
基础题，高频面试题。校招和初中级岗位面试中常考。

需要理解缓存穿透、缓存雪崩、缓存击穿的概念和解决方案，要解释每种方案的原理、优缺点和适用场景，并注意结合实际案例。
前置知识：
缓存穿透是指查询一个不存在的数据。当请求的数据在缓存中找不到时，请求就会直接穿透缓存，访问数据库。如果数据库中也没有这个数据，通常的做法是不缓存不存在的数据，直接返回结果。但这样一来，如果有大量请求查询不存在的数据，就会对数据库造成很大的压力，甚至可能导致数据库崩溃。

可以考虑使用布隆过滤器，过滤掉无效请求。也可以对不存在的key缓存默认值/特殊值，当发现缓存默认值/特殊值时，不再查询数据库，并直接返回。

缓存雪崩是指在同一时间段内，大量缓存 key 集中过期，导致大量请求直接访问数据库，瞬间巨大的流量压垮数据库，造成系统整体性能下降甚至宕机的现象。

------------------------------

607. 如何保证 Redis 与数据库的数据一致性？, page url: https://www.mianshi.icu/question/detail?id=1160
607-1160-答案：
简单题，这个应该说是最基础的数据一致性的问题了。

回答这个问题，不能仅仅回答先更新数据库这种，而是详尽分析各种可行的策略，最终可以引出结论——不管怎么做，都没有办法做到强一致性，只能追求最终一致性。
在使用 Redis 作为缓存时，如何解决数据一致性问题？（mianshi.icu）的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
如果不是寻求强一致性的话，还是有一些办法来解决的。
第三种方式就是所谓的延迟双删了，也就是前一个解决方案的基础上多了一次删除。简单来说就是更新 DB 之后立刻删除缓存，而后睡眠一段时间再次删除缓存，这样就能避开前一种方式造成的不一致。但是极端情况下，也有可能业务在读取了 DB 的老数据之后，隔了好久才回写缓存，刚好绕开了两次删除，那么也会出现数据不一致的问题。真的出现了，就认命，反正缓存是会过期的，到时候重新加载出来就是正确的数据了。
总的来说，可以说在分布式系统下，但凡涉及到多个中间件的数据一致性，就难以保证强一致性，只能是保证最终一致性，并且尽可能把不一致的时间缩短。不一致的根源有两个：并发更新和部分失败。

------------------------------

608. *Redis 支持事务吗？如何实现？, page url: https://www.mianshi.icu/question/detail?id=1161
608-1161-答案：
略难的题，在中高级面试中常见。需要理解 Redis 事务的概念和实现方式。回答时要清晰地解释 Redis 事务的基本概念和、实现方式和应用场景。还可以引导到 Redis 事务的局限性，例如不支持回滚。也可以与MySQL事务进行对比。
Redis 支持事务，但其事务特性与传统关系型数据库有所不同。Redis 事务提供了一种将多个命令打包成一个原子操作序列的机制，然后按顺序执行，从而保证这些命令的原子性。

Redis 事务通过以下命令来实现：


Redis 事务的工作原理：
Redis 支持事务，但其事务特性与MySQL事务有所不同。Redis 事务提供了一种将多个命令打包成一个原子操作序列的机制，然后按顺序执行，从而保证这些命令的原子性。

Redis 事务的工作原理是这样的。首先，客户端发送 WATCH 命令，监视一个或多个 key。如果在事务执行之前这些 key 被其他客户端修改，那么事务将被中断，EXEC 命令会返回 nil。接着，客户端发送 MULTI 命令，标记事务的开始。然后，客户端将需要执行的命令依次添加到事务队列中。在 MULTI 和 EXEC 之间的所有命令都会被放入队列中，但不会立即执行。最后，客户端发送 EXEC 命令，Redis 服务器会按顺序执行事务队列中的所有命令。如果客户端发送 DISCARD 命令，则取消事务，放弃执行事务队列中的所有命令。

Redis 事务有一些局限性。首先，它不支持回滚操作。如果事务中的某个命令失败，其他的命令仍然会继续执行。其次，Redis 事务只能保证单个 Redis 实例上的原子性，无法保证分布式环境下的数据一致性。此外，Redis 事务只支持简单的命令序列，不支持复杂的事务操作（例如，嵌套事务、条件判断等）。最后，长时间的事务操作可能会阻塞其他客户端的请求，因此应尽量缩短事务的执行时间。
Redis 事务与MySQL事务相比，有如下几个特性。

首先是顺序性，事务中的命令按照添加的顺序依次执行。

其次是原子性，Redis 事务不保证原子性，但保证命令序列的隔离执行，即在事务执行期间，不会有其他客户端的命令插入。当事务中的某个命令失败，事务并不会被中断，而是继续执行后续其他命令，并且 Redis 事务不支持回滚（rollback），所以事务的原子性得不到保证。

------------------------------

609. *你用过Redis 的 Lua 脚本吗？如何使用？, page url: https://www.mianshi.icu/question/detail?id=1162
609-1162-答案：
简单题，在校招初中级岗位中常见。

回答时要清晰地解释 Lua 脚本在 Redis 中的作用和使用方法，要详细解释 Lua 脚本在 Redis 中的应用场景，例如实现原子性的计数器、分布式锁等。
Lua 是一种轻量级的脚本语言，Redis 通过内置的 Lua 解释器来执行 Lua 脚本。Redis 支持使用 Lua 脚本进行服务器端编程，这是一种强大的扩展 Redis 功能的方式，允许在服务器端原子性地执行复杂的操作。Lua 脚本可以帮助减少网络开销，提高性能，并实现一些复杂的原子操作。

为什么需要 Lua 脚本：


Lua 脚本的优势：
是的，我使用过 Redis 的 Lua 脚本功能。

Redis 通过内置的 Lua 解释器来执行 Lua 脚本，这是一种强大的扩展 Redis 功能的方式，允许在服务器端原子性地执行复杂的操作。Lua 脚本可以帮助减少网络开销，提高性能，并实现一些复杂的原子操作。

Lua 脚本的基本语法与 Lua 语言一致，我曾用Lua脚本实现过分布式锁、计数器、限流器等功能。

------------------------------

610. *Redis 的 Pipeline 功能是什么？, page url: https://www.mianshi.icu/question/detail?id=1163
610-1163-答案：
简单题，在校招和初中级岗位面试中常见。

重点回答 Pipeline 的概念及优缺点，例如 Pipeline 无法保证原子性。可以引导到 Redis 的其他性能优化技巧，例如使用连接池、优化数据结构等，展示对 Redis 性能优化的深入理解。
Redis Pipeline（管道）允许客户端在一次网络请求中批量发送多个命令，而不必等待每个命令的响应。这种方式通过减少客户端与 Redis 服务器之间的往返通信次数，降低了网络延迟，并显著提高网络传输效率和执行性能。

Pipeline 的工作流程如下：


这种方式避免了传统模式中每发送一个命令都需要等待响应的情况，从而提高了执行效率。
Redis Pipeline（管道）允许客户端将多个 Redis 命令打包在一起，通过单个网络请求发送到 Redis 服务器。接着，Redis 服务器会依次执行这些命令。最后，客户端在发送完所有命令后，再一次性接收所有命令的响应。这种方式避免了传统模式中每发送一个命令都需要等待一个网络往返时间（RTT）的情况，从而降低网络延迟，显著提高网络传输效率和执行性能。

------------------------------

611. *什么是 Redis 的主从复制？如何实现高可用？, page url: https://www.mianshi.icu/question/detail?id=1164
611-1164-答案：
略难的题，在 Redis 面试中比较常见，各个层级的程序员面试中都很常见。

所有的主从复制都差不多是一个样，你会一个就会全部了。

你在这个问题之下，只需要回答到 Redis Sentinel 就可以了，细节可以等进一步追问。而后你可以通过总结 Redis 这种对等集群和主从集群混合的模式，在 Kafka 等中间件中也很常见，从而展示你对系统设计有深刻理解。
前置知识：
Redis 的主从复制分成两种模式：全量复制和增量复制。

在全量复制中，从节点从主节点那里复制整个数据集。这种策略适用于从节点初次加入或者数据丢失的情况。主节点会生成一个 RDB 快照，并将其发送给从节点，从节点加载这个快照来进行复制。全量复制的缺点是，如果数据集非常大，将会占用大量的网络带宽和时间，所以要尽可能规避全量复制。

这一个步骤的核心难点是快速生成 RDB 文件，并且不要影响 Redis 正常对外提供服务。同时 RDB 生成需要时间，从节点加载也需要时间，所以实际上从节点最终加载的数据是比较老的，还需要进一步使用增量复制来跟上节奏。
也不仅仅是 Redis 使用两种形态的复制，在所有的主从结构中，基本上都会支持两种复制模式。

------------------------------

612. Redis 主从复制的实现原理是什么？, page url: https://www.mianshi.icu/question/detail?id=1165
612-1165-答案：
简单题，所有的主从复制都差不多是一个样，你会一个就会全部了。

在回答这个问题的时候，你也不太需要把所有的细节都回答出来，可以等着面试官追问。
Redis 是如何进行主从复制的？（mianshi.icu）的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
Redis 的主从复制分成两种模式：全量复制和增量复制。

在全量复制中，从节点从主节点那里复制整个数据集。这种策略适用于从节点初次加入或者数据丢失的情况。主节点会生成一个 RDB 快照，并将其发送给从节点，从节点加载这个快照来进行复制。全量复制的缺点是，如果数据集非常大，将会占用大量的网络带宽和时间，所以要尽可能规避全量复制。

这一个步骤的核心难点是快速生成 RDB 文件，并且不要影响 Redis 正常对外提供服务。同时 RDB 生成需要时间，从节点加载也需要时间，所以实际上从节点最终加载的数据是比较老的，还需要进一步使用增量复制来跟上节奏。
也不仅仅是 Redis 使用两种形态的复制，在所有的主从结构中，基本上都会支持两种复制模式。

------------------------------

613. * Redis的哨兵机制是什么？, page url: https://www.mianshi.icu/question/detail?id=1166
613-1166-答案：
简单题，在校招、初中级岗位面试中常见。

回答时要清晰地解释哨兵机制的作用。可以引导到 Redis 的其他高可用方案，例如集群模式，展示对 Redis 高可用方案的全面理解。
前置知识：

Redis 哨兵（Sentinel）机制是 Redis 提供的一种高可用性解决方案。实际应用中整个Redis集群会被划分为哨兵集群和主从集群。其中，主从集群对外提供数据存取服务，哨兵集群则负责监控和管理主从集群，并为客户端提供配置信息等，但不对外提供数据存取服务。哨兵节点其实就是一个运行在特殊模式下的Redis进程，它有自己的命令集。它的核心功能包括监控、自动故障转移、通知和配置提供，确保在主从集群的主节点发生故障时，自动检测故障并执行主从切换，从而保证 Redis 服务的连续性和可靠性。

哨兵机制的工作流程如下：
Redis 哨兵（Sentinel）机制是 Redis 提供的一种高可用性解决方案。在实际应用中，整个 Redis 集群会被划分为哨兵集群和主从集群。主从集群对外提供数据存取服务，而哨兵集群则负责监控和管理主从集群，并为客户端提供配置信息，但不直接提供数据存取服务。哨兵节点实际上是一个运行在特殊模式下的 Redis 进程，它有自己的命令集。它的核心功能包括监控、自动故障转移、通知和配置提供，确保在主从集群的主节点发生故障时，能够自动检测故障并执行主从切换，从而保证 Redis 服务的连续性和可靠性。

哨兵机制的工作流程主要包括监控和故障检测。哨兵集群中的每个哨兵节点会定期向主从集群中的主节点和从节点发送 PING 命令，以检测节点的可用性。如果在指定时间内未收到响应，哨兵会将该主从集群节点标记为“主观下线（SDOWN）”。当有足够数量的哨兵节点认为主从集群中同一个主节点主观下线时，就会认为该节点“客观下线（ODOWN）”。接下来，当主从集群的主节点客观下线时，哨兵集群会通过 Raft 算法选举出一个领头哨兵，负责执行故障转移操作。

在故障转移过程中，领头哨兵会从主从集群剩余的从节点中选择一个合适的节点作为新的主节点。选择的优先级通常是优先级最高的从节点（通过 slave-priority 配置），如果优先级相同，则选择复制偏移量（replication offset）最大的从节点。如果复制偏移量也相同，则选择 run ID 最小的从节点。然后，领头哨兵会向所有主从集群的从节点发送命令，使其复制新的主节点，并更新所有客户端的配置，以确保它们连接到新的主节点。故障转移完成后，哨兵会将新的主从集群主节点信息通知给客户端，确保客户端能够重新连接到新的主节点。

------------------------------

614. *请解释Redis的主从复制及哨兵机制。, page url: https://www.mianshi.icu/question/detail?id=1167
614-1167-答案：
简单题，在校招、初中级岗位面试中常见。

所有的主从复制都差不多是一个样，你会一个就会全部了。

回答时要清晰地解释哨兵机制的作用。也不太需要把所有的细节都回答出来，可以等着面试官追问。也可以引导到 Redis 的其他高可用方案，例如集群模式，展示对 Redis 高可用方案的全面理解。
前置知识：
Redis 的主从复制分成两种模式：全量复制和增量复制。

在全量复制中，从节点从主节点那里复制整个数据集。这种策略适用于从节点初次加入或者数据丢失的情况。主节点会生成一个 RDB 快照，并将其发送给从节点，从节点加载这个快照来进行复制。全量复制的缺点是，如果数据集非常大，将会占用大量的网络带宽和时间，所以要尽可能规避全量复制。

这一个步骤的核心难点是快速生成 RDB 文件，并且不要影响 Redis 正常对外提供服务。同时 RDB 生成需要时间，从节点加载也需要时间，所以实际上从节点最终加载的数据是比较老的，还需要进一步使用增量复制来跟上节奏。
也不仅仅是 Redis 使用两种形态的复制，在所有的主从结构中，基本上都会支持两种复制模式。

------------------------------

615. 什么是Redis集群？它如何实现数据的分片和高可用性？, page url: https://www.mianshi.icu/question/detail?id=1168
615-1168-答案：
简单题，在各层级的面试中都有可能遇到，一般来说你面试的公司规模越大越有可能问到。

Redis Cluster 实际上也没啥特殊的，就是一个对等结构和主从结构的混合架构，唯一稍微特殊一点的就是引入了槽和槽分配的概念，你只要把这部分捋清楚就好了。而后，在面试中可以从跨槽问题、混合架构两个角度刷亮点。如果你在实践中有使用类似的技术，那么更加能给面试官留下深刻的印象。

你了解 Redis Cluster 吗？（mianshi.icu）的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
Redis Cluster 是一个对等结构和主从结构的混合架构。Redis Cluster 由多个节点组成，这些节点之间地位是平等的，也就是说它们构成了一个对等结构。

但是从细节上来说，每一个节点都是一个主从集群，也就是说每一个节点都是类似于 Redis Sentinel 模式，并借此来保证高可用。

Redis Cluster 借鉴一致性哈希的思想，利用 CRC16 将 key 分散到 16384 个槽（哈希槽就相当于一致性哈希中的虚拟节点）上面，而后再次将这些槽分配给不同的节点。可以平均分，也可以不是平均分。
但是 Redis Cluster 并不是毫无缺点，最大的问题就是难以处理跨槽的问题。

这最典型的例子就是 pipeline。例如说在 pipeline 里面要处理分散在不同槽上的多个 key，那么pipeline 就会返回错误，这需要客户端进行处理。而有些语言的 Redis 客户端其实没有那么智能。
Redis Cluster 这种对等集群和主从集群的混合模式，在别的中间件里面也能看到类似的设计，甚至于可以说现代的大规模分布式软件的高可用都是通过这种设计来保证的。

------------------------------

616. *什么是 Redis 集群模式？它是如何分片的？, page url: https://www.mianshi.icu/question/detail?id=1169
616-1169-答案：
简单题，在各层级的面试中都有可能遇到，一般来说你面试的公司规模越大越有可能问到。

Redis Cluster 实际上也没啥特殊的，就是一个对等结构和主从结构的混合架构，唯一稍微特殊一点的就是引入了槽和槽分配的概念，你只要把这部分捋清楚就好了。而后，在面试中可以从跨槽问题、混合架构两个角度刷亮点。如果你在实践中有使用类似的技术，那么更加能给面试官留下深刻的印象。
你了解 Redis Cluster 吗？（mianshi.icu）的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
Redis Cluster 是一个对等结构和主从结构的混合架构。Redis Cluster 由多个节点组成，这些节点之间地位是平等的，也就是说它们构成了一个对等结构。

但是从细节上来说，每一个节点都是一个主从集群，也就是说每一个节点都是类似于 Redis Sentinel 模式，并借此来保证高可用。

Redis Cluster 借鉴一致性哈希的思想，利用 CRC16 将 key 分散到 16384 个槽（哈希槽就相当于一致性哈希中的虚拟节点）上面，而后再次将这些槽分配给不同的节点。可以平均分，也可以不是平均分。
但是 Redis Cluster 并不是毫无缺点，最大的问题就是难以处理跨槽的问题。

这最典型的例子就是 pipeline。例如说在 pipeline 里面要处理分散在不同槽上的多个 key，那么pipeline 就会返回错误，这需要客户端进行处理。而有些语言的 Redis 客户端其实没有那么智能。
Redis Cluster 这种对等集群和主从集群的混合模式，在别的中间件里面也能看到类似的设计，甚至于可以说现代的大规模分布式软件的高可用都是通过这种设计来保证的。

------------------------------

617. Redis 集群的实现原理是什么？, page url: https://www.mianshi.icu/question/detail?id=1170
617-1170-答案：
简单题，在各层级的面试中都有可能遇到，一般来说你面试的公司规模越大越有可能问到。

Redis Cluster 实际上也没啥特殊的，就是一个对等结构和主从结构的混合架构，唯一稍微特殊一点的就是引入了槽和槽分配的概念，你只要把这部分捋清楚就好了。而后，在面试中可以从跨槽问题、混合架构两个角度刷亮点。如果你在实践中有使用类似的技术，那么更加能给面试官留下深刻的印象。
你了解 Redis Cluster 吗？（mianshi.icu）的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
Redis Cluster 是一个对等结构和主从结构的混合架构。Redis Cluster 由多个节点组成，这些节点之间地位是平等的，也就是说它们构成了一个对等结构。

但是从细节上来说，每一个节点都是一个主从集群，也就是说每一个节点都是类似于 Redis Sentinel 模式，并借此来保证高可用。

Redis Cluster 借鉴一致性哈希的思想，利用 CRC16 将 key 分散到 16384 个槽（哈希槽就相当于一致性哈希中的虚拟节点）上面，而后再次将这些槽分配给不同的节点。可以平均分，也可以不是平均分。
但是 Redis Cluster 并不是毫无缺点，最大的问题就是难以处理跨槽的问题。

这最典型的例子就是 pipeline。例如说在 pipeline 里面要处理分散在不同槽上的多个 key，那么pipeline 就会返回错误，这需要客户端进行处理。而有些语言的 Redis 客户端其实没有那么智能。
Redis Cluster 这种对等集群和主从集群的混合模式，在别的中间件里面也能看到类似的设计，甚至于可以说现代的大规模分布式软件的高可用都是通过这种设计来保证的。

------------------------------

618. *Redis 集群会出现脑裂问题吗？, page url: https://www.mianshi.icu/question/detail?id=1171
618-1171-答案：
略难的题，各个层级面试中都可能出现。

需要理解 Redis 集群的架构和故障转移机制。回答时要清晰地解释脑裂问题的概念，重点说明 Redis 集群在某些情况下可能会出现脑裂问题，并要详细解释 Redis 集群如何尽量避免脑裂问题。



脑裂是指在分布式系统中，由于网络分区或节点故障，导致集群被分割成多个独立的子集群。每个子集群都认为自己是主节点，并继续提供服务，从而导致数据不一致和冲突。简单来说，就是“一个身体，多个大脑”。


Redis 集群出现脑裂问题的主要原因包括：



Redis 集群通过主从复制和故障转移机制来实现高可用性。在正常情况下，每个主节点（Master）都有一个或多个从节点（Slave），当主节点发生故障时，从节点会接管主节点的工作。然而，在网络分区的情况下，可能会出现以下情况：
脑裂是分布式系统中的一个重要概念，简单来说，它指的是由于网络分区或节点故障，导致集群被分割成多个独立的子集群。每个子集群都认为自己是主节点，并继续提供服务，这样就会导致数据不一致和冲突。可以形象地理解为“一个身体，多个大脑”。

脑裂的原因主要有两个。首先是网络分区，集群中的节点因为网络故障或者其他原因无法互相通信，结果就会形成多个独立的子集群。其次是主节点故障转移。当主节点发生故障时，Redis 集群会自动进行故障转移，把一个从节点提升为新的主节点。如果在这个过程中，原来的主节点恢复了，并且仍然认为自己是主节点，那么就会出现脑裂。

为了避免 Redis 集群出现脑裂问题，可以采取一些措施。

------------------------------

619. Redis 中的 Big Key 问题是什么？如何解决？, page url: https://www.mianshi.icu/question/detail?id=1172
619-1172-答案：
略难的题，在社招中会遇到，一般在中高级岗位中考察较多。

这个问题是包含两层意思：怎么知道有大 key，知道了怎么解决。

回答这个问题，最好就是能引出一个具体的案例，然后借助这个案例来刷亮点。
Redis 中如何排查大 Key 问题？（mianshi.icu）的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
Big Key 问题不是指 Key 本身很长，而是指Key对应的值很大，比如说一个巨大的 JSON 串，一个巨大的 zset，list 等。大 key 一般会引起数据倾斜、QPS 倾斜等问题。或者说大 Key 有极大的可能是热点 Key，并发极高。

一般来说，大 Key 问题基本上都是通过监控慢查询和 Redis 内存使用量发现的，尤其是慢查询监控。举个例子来说，之前我就排查过一个 Redis 的 list 有很多元素，导致查询、遍历非常慢的问题。

当然，发现大 key 问题并不难，难的是如何解决大 key 问题。
这里还有一些比较关键的问题。
我就曾经解决过一个大 key 问题，是为了解决大数据、高可用、高性能的榜单问题。

一开始我们业务的数据量不是很大，所以直接用了最简单的 Redis zset 来计算榜单。

后面随着业务的发展，计算这个榜单的数据越来越多，并发量越来越高。这个时候，一个 zset 里面要放几百万个数据，存储这个 zset 的 Redis 并发极高，压力极大。并且 zset 中元素数量太多，导致更新的时候越来越慢。

------------------------------

620. *如何用Redis实现分布式锁？, page url: https://www.mianshi.icu/question/detail?id=1175
620-1175-答案：
简单题，在校招或者初中级岗位面试中常见。

需要理解分布式锁的概念、原理和实现方式。回答时要清晰地解释如何使用 Redis 的 SET 命令、Lua脚本和 EXPIRE 命令来实现分布式锁。要顺带解释一下，此过程遇到的问题，例如如何解决死锁、锁超时、锁误删等问题。
前置知识：

使用 Redis 实现分布式锁是一种常见的方案。核心思想是利用 Redis 的原子操作来保证在分布式环境下只有一个客户端能够获得锁。以下是实现分布式锁的关键步骤和注意事项：


注意事项：
如果是自己实现的话，可遵循一下步骤：

首先是加锁操作，这个必须是原子性的。我推荐使用 SET lock_key value NX EX/PX milliseconds 命令，这是一个原子操作，可以避免 SETNX 和 EXPIRE 分开执行可能导致的竞态条件。lock_key 就是锁的键名，value 是一个随机且唯一的客户端标识（Client ID），用于后续释放锁时进行验证。NX（Not eXists）表示仅当键不存在时才设置键的值，这保证了只有一个客户端能成功加锁。EX seconds / PX milliseconds 是设置键的过期时间，单位分别是秒和毫秒，这可以防止死锁，即使持有锁的客户端崩溃，锁也会在一段时间后自动释放。

然后是释放锁操作，这个也必须保证原子性和安全性。释放锁时，必须先验证锁的持有者是不是当前客户端，然后再删除锁。这个过程必须是原子性的，以防止误删其他客户端持有的锁。所以要使用 Lua 脚本，脚本的逻辑是先获取锁的值（Client ID），然后判断锁的值是否与当前客户端的 Client ID 相等，如果相等，就删除锁。

------------------------------

621. *用 Redis 实现分布式锁时遇到了哪些问题？如何解决的？, page url: https://www.mianshi.icu/question/detail?id=1176
621-1176-答案：
简单题，社招可能遇到，校招不太可能，因为校招没经验。

你在回答这个问题的时候，要深入讨论续约中可能出现的问题，以及对应的解决方案。

前置知识：
Redis 实现分布式锁时遇到以下问题：

首先是死锁问题，如果客户端在获取锁后，由于某种原因（例如，程序崩溃）未能正常释放锁，导致锁一直被占用，其他客户端无法获取锁，造成死锁。解决方法很简单，在加锁的同时设置过期时间即可，即使客户端未能正常释放锁，锁也会在过期后自动释放。或者使用 Redlock 算法，提高锁的可靠性，防止死锁。

其次是锁的误删问题，如果客户端 A 获取锁后，由于执行时间过长，导致锁过期自动释放。此时，客户端 B 获取了锁。然后，客户端 A 执行完业务逻辑后，尝试释放锁，但实际上释放的是客户端 B 的锁，造成锁的误删除。要解决这个问题，可以在加锁时，将锁的值设置为一个唯一标识（例如，UUID），在释放锁时，先判断锁的值是否与自己的唯一标识相等，如果相等，则释放锁；否则，不释放锁。此过程要保证原子性，可以使用 Lua 脚本实现。
续约的难点不在于如何续约，而是一些细节问题。

第一个问题是续约失败了怎么办？这时候一般会考虑重试，但是重试多次之后都有可能失败，这时候就只能返回错误给业务方，让业务方决定怎么办。大多数时候业务方应该尽量中断自己的业务，如果不能中断则是要告警。

------------------------------

622. *如何使用 Redis 实现排行榜？, page url: https://www.mianshi.icu/question/detail?id=1177
622-1177-答案：
简单题，校招和初中级岗位面试常见。

需要理解 Redis 有序集合（Sorted Set）的特性和应用场景。回答时要重点说明如何使用有序集合实现排行榜。可以引导到 Redis 有序集合的其他应用场景，例如延迟队列、优先级队列，展示对 Redis 有序集合的全面理解。
前置知识：

Redis 提供了有序集合（ZSet）数据结构，非常适合用于实现排行榜。ZSet 允许我们为每个元素分配一个分数，并根据分数对元素进行排序，这使得我们可以轻松地实现排行榜的功能。


使用 Redis 的 ZSet 来存储排行榜数据，元素的值可以是用户的 ID 或名称，分数可以是用户的得分或排名。通过 ZSet 提供的命令，我们可以方便地进行以下操作：
基本思路是这样的，用 Redis 的 ZSet 来存储排行榜数据，元素的值可以是用户的 ID，分数可以是用户的得分或者排名。

通过 ZSet 提供的一些命令，我们可以很方便地实现排行榜相关操作：

首先是添加或者更新用户的分数，可以使用 ZADD 命令，把用户 ID 作为元素，用户分数作为分数添加到有序集合中。如果用户 ID 已经存在，ZADD 命令会更新这个用户的分数。比如，ZADD leaderboard 100 "user1" 200 "user2" 150 "user3"。

------------------------------

623. *如何使用 Redis 实现布隆过滤器？, page url: https://www.mianshi.icu/question/detail?id=1178
623-1178-答案：
简单题，在校招和初中级岗位面试中可能被问到。

需要理解布隆过滤器的概念、原理和实现方式。回答时要解释清楚如何使用Redis 位图（Bitmap）来实现布隆过滤器。
布隆过滤器（Bloom Filter）是一种空间效率很高的概率型数据结构，用于判断一个元素是否存在于一个集合中。它允许一定的误判率，但不会漏判，非常适合在大数据场景下减少不必要的查询。Redis 本身可以通过位图（Bitmap）来实现布隆过滤器，同时 RedisBloom 模块也提供了原生支持。

布隆过滤器的基本原理：


参数选择：
在 Redis 中可以使用位图操作来实现布隆过滤器，Redis 提供了 BITFIELD 命令，可以方便地操作位数组。可以使用 Redis 的 SETBIT 命令设置位数组中的值，使用 GETBIT 命令检查位数组中的值。具体步骤如下：

初始化的时候，需要选择合适的位数组大小 m 和哈希函数数量 k，然后在 Redis 中创建一个 key，用于存储位数组。

添加元素的时候，使用 k 个哈希函数计算元素的哈希值，再得到 k 个索引，然后使用 SETBIT 命令将 Redis 位图中对应索引的位置设置为 1。

------------------------------

624. *什么是线程安全？如何实现线程安全？, page url: https://www.mianshi.icu/question/detail?id=1180
624-1180-答案：
简单题，在校招和初中级岗位面试中常见。

在回答此问题时，可以引导到你会使用的语言（Python、Java、Go）的并发编程。
线程安全是指在多线程环境下，当多个线程并发访问共享资源时，程序能够保证数据的一致性和正确性，而不会出现数据竞争、死锁等问题。简单来说，线程安全的代码在并发执行时能够保证正确性和一致性。

线程安全问题通常是由于以下原因引起的：


实现线程安全的方法有很多，主要包括以下几种：
线程安全，简单来说，就是在多线程环境下，多个线程同时访问共享资源的时候，程序能够保证数据的一致性和正确性，不会出现数据竞争、死锁之类的问题。也就是说，线程安全的代码在并发执行的时候，能保证正确性和一致性。

线程安全问题通常是由于这几个原因引起的：

首先是共享资源，多个线程访问同一个共享资源，比如全局变量、静态变量、堆内存等等。

------------------------------

625. *什么是分页和分段？它们的区别是什么？, page url: https://www.mianshi.icu/question/detail?id=1181
625-1181-答案：
简单题，在校招里面有比较低的可能碰到，这个问题因为过于古板以至于你在社招里面几乎不会遇到。简单题，在校招和初中级岗位中比较常见。
前置知识：
分页是指操作系统将虚拟内存和物理内存都划分成固定大小的页进行管理，所以分页还需要进一步解决虚拟内存地址到物理内存地址的映射，而这个过程是借助了页表和内存管理单元（MMU）来完成的。

简单来说，每一个进程都有一个页表。当进程访问一个虚拟内存地址的时候，要先找到页表，根据虚拟页号找到物理内存页号。而后在物理内存页的基础上加上页内偏移量，就得到了虚拟内存地址对应的物理内存地址。

分段是是指将程序的逻辑地址空间划分为若干个大小不等的段，每个段是一组逻辑上相关的信息的集合。分段机制允许操作系统以段为单位来管理内存，而不是使用连续的内存块。
页的大小对操作系统性能有比较大的影响。

如果要是页过小，那么更加容易产生内部碎片，比如说 1KB 每页，随便装点东西就接近装满，这个页剩余的内部空间就被浪费了。较小的页也需要更多内存来放页表。比如说你 4KB 每页，有 1000 页，页表就是 1000 条记录。而如果你是 1KB 页表，那么就是 4000 页，页表就是需要 4000 条记录。较小的页也更加容易触发页面替换。
分页可以说是主流的内存管理算法了。比如说 Linux 的内存管理就是使用了一个四级分页机制，每次虚拟地址寻址的时候都是按照这个 PGD，PUD，PMD 和页表的顺序来映射到物理地址。

------------------------------

626. *了解页表么？它的作用是什么？, page url: https://www.mianshi.icu/question/detail?id=1182
626-1182-答案：
简单题，在校招和初中级岗位中比较常见。
前置知识：


页表（Page Table）是操作系统中用于实现虚拟内存管理的一种数据结构，它记录了虚拟地址空间与物理地址空间之间的映射关系。通过页表，操作系统可以将进程的虚拟地址转换为实际的物理地址，从而实现内存的分配和管理。
页表是操作系统中用于实现虚拟内存管理的一种数据结构，它记录了虚拟地址空间与物理地址空间之间的映射关系。通过页表，操作系统可以将进程的虚拟地址转换为实际的物理地址，从而实现内存的分配和管理。

页表主要有这么几个作用：

首先是虚拟地址到物理地址的转换。进程用的是虚拟地址，而实际的物理内存用的是物理地址。页表记录了虚拟地址与物理地址之间的映射关系，操作系统通过查询页表，将虚拟地址转换为物理地址，从而访问实际的内存。

------------------------------

627. *什么是虚拟内存？为什么需要虚拟内存？, page url: https://www.mianshi.icu/question/detail?id=1183
627-1183-答案：
简单题，在校招和初中级岗位里面非常常见。

这个问题其实有一点猥琐，它就是我说的那种反直觉问题。也就是正常你都是知道有虚拟内存，但是比较少去反思为什么要有虚拟内存。所以如果你平时没有准备，就比较容易寄了。

回答这个问题你可以从虚拟内存这种设计的引申到一般性的增加中间层的设计理念上刷亮点。
为什么要引入虚拟内存空间而不是直接使用物理内存？（mianshi.icu）的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
首先，如果系统中只有一个进程，那么物理内存直接分配给它使用就可以了。但是，在多进程的环境中，我们面临一个物理内存共享的问题。简单地平分物理内存给每个进程是不现实的，因为进程的数量和它们的内存需求是动态变化的。

为了解决这个问题，操作系统引入了虚拟内存的概念。虚拟内存为每个进程提供了一个假想的、连续的内存空间，即虚拟地址空间。进程认为自己独享整个内存，而实际上，操作系统会在进程需要时动态地分配物理内存给它。
在计算机系统中，类似的设计理念也被应用于其他领域。例如，虚拟文件系统（VFS）就是这样一个设计。VFS是操作系统文件管理的一个抽象层，它为用户和应用程序提供了一个统一的文件操作接口，而不管底层存储设备的实际类型是什么。这样，不同的文件系统实现（如EXT4、NTFS、FAT32等）可以在操作系统中共存，用户和应用程序不需要关心文件实际上是如何存储的。

------------------------------

628. *什么是虚拟内存？它的作用是什么？, page url: https://www.mianshi.icu/question/detail?id=1184
628-1184-答案：
略难的题，在校招和初中级岗位的面试中比较常见。

回答这个问题你的关键点是点出 MMU、交换区和对应的换入换出这几个概念，这样面试官就会进一步追问。
什么是虚拟内存？（mianshi.icu）的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
虚拟内存是一种内存管理技术，它为每个进程提供了一个虚拟的地址空间，使得进程可以访问比实际物理内存更大的内存区域。操作系统引入这个东西，主要是为了解决多进程日益增长的内存需要和有限的物理内存之间的矛盾。

但是在引入了这种虚拟内存和物理内存之后，要解决的第一个问题就是虚拟内存映射过去物理内存，而这是借助所谓的内存管理单元 MMU 来实现的。简单来说，就是当进程访问一个虚拟内存地址的时候，MMU 会把它转化成一个物理内存地址，而后进程就可以读到对应的物理内存上的数据。

这种机制又会引入另外一个问题，就是虚拟内存远比物理内存大，而且物理内存都是所有的进程共享的。这个时候就会有问题，即物理内存不够用。因此操作系统又引入了交换区和换入换出的概念。

------------------------------

629. *什么是物理地址和逻辑地址？它们的区别是什么？, page url: https://www.mianshi.icu/question/detail?id=1186
629-1186-答案：
简单题，在校招和初中级岗位里面非常常见。
在计算机系统中，物理地址和逻辑地址是两种不同的地址概念，它们描述了内存中数据存储位置的不同视角。理解这两种地址的区别对于理解操作系统的内存管理至关重要。
物理地址是计算机内存中真实存在的地址，它直接对应于内存中的物理存储单元。物理地址也被称为绝对地址或实地址。每个物理地址对应于内存中唯一的物理存储单元，直接与硬件相关，由内存控制器和地址总线使用。

逻辑地址是程序在运行时使用的地址，也称为虚拟地址。每个程序都认为自己拥有一个完整的、连续的内存空间，这个空间就是逻辑地址空间。每个程序都有自己的逻辑地址空间，不同程序的逻辑地址空间是相互独立的。

物理地址和逻辑地址的主要有以下区别：

------------------------------

630. 简述页面置换算法, page url: https://www.mianshi.icu/question/detail?id=1187
630-1187-答案：
简单题，在校招和初中级工程师面试中非常常见。

要想刷亮点的话，你可以简单提及 Linux 使用的页面替换算法。
页面替换算法有哪些？（mianshi.icu）的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
有很多种算法，每种算法都有自己的特色。

第一种是先进先出（FIFO）算法：这是最简单的页面替换算法。它基于“先进先出”的原则，即最早进入内存的页面将首先被替换。这种算法易于实现，但可能不适合实际的工作负载，因为它不考虑页面的使用频率。

第二种是最近最少使用（LRU）算法：LRU算法认为过去一段时间内最少被使用的页面，在未来的使用概率也相对较低。因此，当需要替换页面时，它会选择最长时间未被使用的页面进行替换。
Linux内核使用了多种页面替换算法的组合，主要是基于LRU的变种，同时考虑了文件页面和匿名页面的不同特性。它不是一个固定的算法，而是根据系统负载和内存使用模式动态调整的策略。随着内核版本的更新，页面替换算法也在不断地得到改进和优化。

------------------------------

631. 操作系统中的页面置换算法有哪些？各有什么特点？, page url: https://www.mianshi.icu/question/detail?id=1188
631-1188-答案：
简单题，在校招和初中级工程师面试中非常常见。

要想刷亮点的话，你可以简单提及 Linux 使用的页面替换算法。
页面替换算法有哪些？（mianshi.icu）的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
有很多种算法，每种算法都有自己的特色。

第一种是先进先出（FIFO）算法：这是最简单的页面替换算法。它基于“先进先出”的原则，即最早进入内存的页面将首先被替换。这种算法易于实现，但可能不适合实际的工作负载，因为它不考虑页面的使用频率。

第二种是最近最少使用（LRU）算法：LRU算法认为过去一段时间内最少被使用的页面，在未来的使用概率也相对较低。因此，当需要替换页面时，它会选择最长时间未被使用的页面进行替换。
Linux内核使用了多种页面替换算法的组合，主要是基于LRU的变种，同时考虑了文件页面和匿名页面的不同特性。它不是一个固定的算法，而是根据系统负载和内存使用模式动态调整的策略。随着内核版本的更新，页面替换算法也在不断地得到改进和优化。

------------------------------

632. *如何避免死锁？, page url: https://www.mianshi.icu/question/detail?id=1189
632-1189-答案：
简单题，不过记忆的内容有点多，所以也不太容易答好。在校招和初级工程师面试中，如果要是提及了死锁，那么差不多肯定会问这个问题。
如何处理死锁？（mianshi.icu）的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
可以采取以下几种方式来尽可能避免死锁，想要完全避免死锁是不可能的。

首先是死锁预防。基本思想就是确保死锁发生的四个必要条件中至少有一个不成立。例如破除资源互斥条件：典型例子就是读锁，而读锁是共享的；破除“请求与保持”条件：实行资源预分配策略，进程在运行之前，必须一次性获取所有的资源。缺点：在很多情况下，无法预知进程执行前所需的全部资源，因为进程是动态执行的，同时也会降低资源利用率，导致降低了进程的并发性；破除“不可剥夺”条件：允许系统强行剥夺进程已占有的资源。例如，在进程请求新资源时，如果资源不足，系统可以剥夺该进程已占有的资源分配给其他进程；破除“循环等待”条件：实行资源有序分配策略，对所有资源排序编号，只能按照编号递增的顺序获取。或者为每个资源设置一个超时时间，如果进程在占用资源的时间超过了超时时间，则自动释放该资源。这种方法可以避免进程长时间占用资源，但可能会导致进程的任务失败。。

其次是，在每次资源请求时，系统会检查此次分配是否会导致系统进入不安全状态。如果会，则拒绝此次资源请求；如果不会，则允许分配。这里最有名的就是使用银行家算法来检测资源分配后是否安全。

------------------------------

633. 死锁产生的必要条件是什么？, page url: https://www.mianshi.icu/question/detail?id=1190
633-1190-答案：
简单题，在校招和初级工程师面试中常见，尤其是校招你一家公司几轮面下来，差不多肯定会遇到这个问题，毕竟在实践中时不时遇到一个死锁。

回答这个死锁，最好的面试方式就是列举自己遇到过的死锁问题，分析如何满足必要条件，以及对应的解决方案。
前置知识：
死锁产生的必要条件是：

第一个是互斥条件，即一个资源一次只能被一个进程使用；

第二个是请求和保持条件，即一个进程因请求资源而阻塞时，对已获得资源保持不放；
在实践中，死锁是一个很常见的问题。一般来说主要出现在两个地方。

第一个地方就是并发编程里面。比如说锁使用不当，并发工具使用不当都有可能引起死锁。这一类死锁跟语言特性是强相关的。举个例子来说，之前就遇到过 JAVA 代码没写好引发的死锁问题。

------------------------------

634. *什么是死锁？如何避免死锁？, page url: https://www.mianshi.icu/question/detail?id=1191
634-1191-答案：
简单题，在校招和初级工程师面试中常见，尤其是校招你一家公司几轮面下来，差不多肯定会遇到这个问题，毕竟在实践中时不时遇到一个死锁。

回答这个死锁，最好的面试方式就是列举自己遇到过的死锁问题，以及对应的解决方案。毕竟死锁的八股文谁都会背，但是死锁的问题排查，就不是所有人都会了。
前置知识：
死锁从概念上来说很简单，就是在两个或者多个并发进程中，如果每个进程持有某种资源而又等待其它进程释放它或它们现在保持着的资源，在未改变这种状态之前都不能向前推进，称这一组进程产生了死锁。

而死锁的产生，有四个条件。

第一个是互斥条件，即一个资源一次只能被一个进程使用；

------------------------------

635. 什么是死锁？产生死锁的必要条件是什么？, page url: https://www.mianshi.icu/question/detail?id=1192
635-1192-答案：
简单题，在校招和初级工程师面试中常见，尤其是校招你一家公司几轮面下来，差不多肯定会遇到这个问题，毕竟在实践中时不时遇到一个死锁。

回答这个死锁，最好的面试方式就是列举自己遇到过的死锁问题，以及对应的解决方案。毕竟死锁的八股文谁都会背，但是死锁的问题排查，就不是所有人都会了。
什么是死锁？（mianshi.icu）的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
死锁从概念上来说很简单，就是在两个或者多个并发进程中，如果每个进程持有某种资源而又等待其它进程释放它或它们现在保持着的资源，在未改变这种状态之前都不能向前推进，称这一组进程产生了死锁。

而死锁的产生，有四个条件。

第一个是互斥条件，即一个资源一次只能被一个进程使用；
在实践中，死锁是一个很常见的问题。一般来说主要出现在两个地方。

第一个地方就是并发编程里面。比如说锁使用不当，并发工具使用不当都有可能引起死锁。这一类死锁跟语言特性是强相关的。举个例子来说，之前就遇到过 JAVA 代码没写好引发的死锁问题。

------------------------------

636. 介绍几种常见的进程同步机制, page url: https://www.mianshi.icu/question/detail?id=1193
636-1193-答案：
简单题，在校招中可能遇到，但是社招就很少问了。

其实这个问题没太多的实践意义，因为现在很少接触到多进程编程了，多数都是多线程编程，甚至于直接就是协程编程了。

要在这个问题下装逼，可以通过分布式环境下的进程同步方式来刷亮点。
进程间同步的方式有哪些？（mianshi.icu）的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
进程之间的同步方式有很多。

第一种是共享内存。也就是多个进程共享同一块内存区域，实现高效的数据交换，但是一般要配合别的同步机制来保护共享内存，避免出现并发问题；

第二种是管道。管道可以认为是一种数据结构，可以在进程之间传递数据，多用于父子进程之间；
而在分布式环境下，还有更多的同步方式。

------------------------------

637. 常见的进程调度算法有哪些？优缺点是什么？, page url: https://www.mianshi.icu/question/detail?id=1194
637-1194-答案：
简单题，在校招和初级工程师面试中常见，越往上越不容易遇到。

你在这个问题之下，可以将话题引导过去 Linux 使用的进程调度策略上。
进程调度策略有哪几种？（mianshi.icu）的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
从理论上来说，有很多种调度策略。

第一种是先来先服务，也就是按照到达就绪队列的顺序来调度。优点是简单易实现，缺点就是就绪队列尾部的进程可能会出现饥饿。

第二种是短作业优先。也就是优先调度预计执行时间最短的。优点是可以减少平均等待时间，但是缺点是长作业会饥饿。
大多数操作系统并不会使用单一的调度策略，而是多种策略混合使用。

------------------------------

638. *什么是时间片轮转调度？, page url: https://www.mianshi.icu/question/detail?id=1195
638-1195-答案：
简单题，在校招和初级工程师面试中常见，越往上越不容易遇到。

你在这个问题之下，可以将话题引导过去 Linux 使用的进程调度策略上。
前置知识：



时间片轮转调度（Round Robin Scheduling，简称 RR 调度）是一种最古老、最简单、最公平且使用最广泛的进程调度算法之一。它主要用于分时系统，旨在为所有进程提供公平的 CPU 使用机会。它主要由以下要素组成：





时间片的大小对系统的性能有重要影响：
时间片轮转调度，也就是 RR 调度，是一种非常经典、简单且公平的进程调度算法。它主要用在分时系统，目的是为了给所有进程提供一个公平的 CPU 使用机会。

它的工作原理是这样的：

调度器会从就绪队列中选择队首的进程，并为它分配一个时间片。这个进程开始执行，直到时间片结束，或者它因为某种原因阻塞了。如果进程在时间片结束时还没有完成，它就会被放回就绪队列的队尾，等着下一次调度。调度器会选择就绪队列中的下一个进程，给它分配时间片，继续执行。
大多数操作系统并不会使用单一的调度策略，而是多种策略混合使用。

------------------------------

639. *什么是 inode？它在文件系统中起什么作用？, page url: https://www.mianshi.icu/question/detail?id=1196
639-1196-答案：
简单题，在校招和初中级岗位中可能会遇到。
inode（索引节点）是文件系统中用于存储文件元数据的数据结构。每个文件或目录在文件系统中都有一个唯一的 inode，inode 中包含了与文件相关的各种信息，但不包括文件名和文件内容。

一个 inode 通常包含以下信息：


inode 在文件系统中的作用
inode，或者说索引节点，是文件系统中用来存储文件元数据的数据结构。每个文件或目录在文件系统中都有一个唯一的 inode，这个 inode 包含了与文件相关的各种信息，但不包括文件名和文件内容。

一个 inode 通常包含一些关键信息，比如文件类型（普通文件、目录、符号链接）、权限信息（读、写、执行）、所有者信息）、文件的大小、时间戳等。

inode 在文件系统中的主要有以下作用。

------------------------------

640. *文件的打开和关闭过程是怎样的？, page url: https://www.mianshi.icu/question/detail?id=1197
640-1197-答案：
简单题，在校招和初中级岗位中可能会遇到。

文件的打开过程涉及操作系统内核的一系列操作，主要目的是建立进程与文件之间的连接，以便后续的读写操作。以下是文件打开过程的详细步骤：



文件的关闭过程是为了释放文件相关的资源，断开进程与文件之间的连接。以下是文件关闭过程的详细步骤：
当用户进程想打开文件，就像向操作系统申请通行证，通过 open() 系统调用，提供文件名和打开模式。

内核拿到请求，首先得验证权限，就像门卫一样，检查进程是否有权访问，比如用户ID和组ID是否匹配，权限位是否允许等。 没权限就返回错误，直接拒绝。

权限过了，内核就要在文件系统里找对应的 inode，就像在档案室里找文件。 如果文件不存在，但打开模式允许创建，就创建一个新的 inode。

------------------------------

641. *Select、Poll、Epoll 之间有什么区别？, page url: https://www.mianshi.icu/question/detail?id=1198
641-1198-答案：
简单题，高频题，在校招和初中级岗位里面非常常见。
select、poll 和 epoll 都是用于实现 I/O 多路复用的机制，允许一个进程同时监视多个文件描述符（File Descriptor，FD），并在其中任何一个文件描述符就绪（可读、可写或有错误）时得到通知。它们的主要区别在于实现方式和性能特点。



主要有以下区别：
select、poll 和 epoll 都是用于实现 I/O 多路复用的机制。它们的主要作用是允许一个进程同时监视多个文件描述符，并在其中任何一个文件描述符就绪时得到通知。

首先，select 使用一个 fd_set 结构来表示一组文件描述符。它通过轮询的方式检查每个文件描述符的状态，时间复杂度为O(n)。select 的一个特点是，它使用的是固定大小的位图，通常由 FD_SETSIZE 宏定义，默认值是 1024。每次调用 select 时，整个 fd_set 都需要被复制到内核空间，内核会轮询检查每个文件描述符的状态，并将就绪的文件描述符返回给用户空间。

select 是 POSIX 标准的一部分，具有很好的可移植性，几乎所有操作系统都支持它。一般来说，select 适用于连接数较少的场景，尤其是在对可移植性要求较高的情况下。

------------------------------

642. *什么是面向对象？基本要素有哪些？, page url: https://www.mianshi.icu/question/detail?id=1199
642-1199-答案：
简单题，校招和初级岗位面试中可能出现。

面向对象编程（OOP）是一种编程范式，它将程序中的数据和操作数据的方法组合成一个称为“对象”的实体。面向对象编程的核心思想是将现实世界中的事物抽象成程序中的对象，通过对象之间的交互来完成程序的逻辑。

与面向过程编程（Procedural Programming）不同，面向对象编程更加注重数据的封装、继承和多态，从而提高代码的可重用性、可维护性和可扩展性。


面向对象编程有三个基本要素，通常被称为 OOP 的三大特性：
面向对象编程（OOP）是一种编程范式，它把程序中的数据和操作数据的方法组合成一个叫做“对象”的实体。你可以把 OOP 想象成把现实世界的事物抽象成程序里的对象，然后通过这些对象之间的互动来完成程序的逻辑。

和面向过程编程不一样，OOP 更注重数据的封装、继承和多态，这样可以提高代码的可重用性、可维护性和可扩展性。

OOP 有三个基本要素，也就是大家常说的三大特性：

------------------------------

643. *介绍一下单例模式, page url: https://www.mianshi.icu/question/detail?id=1200
643-1200-答案：
简单题，校招和初中岗位面试中常见。
单例模式（Singleton Pattern）是一种创建型设计模式。它的主要目的是确保一个类只有一个实例，并提供一个全局访问点来获取这个实例。换句话说，单例模式限制了类的实例化次数，保证在整个应用程序中，只有一个该类的对象存在。

单例模式的特点：


单例模式适用于需要全局唯一实例的场景，通常用于需要控制资源的共享和管理，例如：
单例模式（Singleton Pattern）是一种创建型设计模式，主要目的是确保一个类只有一个实例，并提供一个全局访问点来获取这个实例。简单来说，单例模式限制了类的实例化次数，确保在整个应用程序中只有一个该类的对象存在。

单例模式适用于需要全局唯一实例的场景，通常用于控制资源的共享和管理。例如，应用程序的全局配置（像数据库连接信息、系统参数等）需要在整个应用中共享，以确保配置的一致性。还有资源管理，比如数据库连接池、线程池和缓存等，避免资源浪费或冲突。日志记录系统也需要全局唯一，以确保所有日志记录到同一个文件或输出流。此外，像状态管理器、服务注册中心等全局唯一的服务也适合使用单例模式。

单例模式有多种实现方式，常见的包括：

------------------------------

644. *单例模式有哪几种实现方式？如何保证线程安全？, page url: https://www.mianshi.icu/question/detail?id=1201
644-1201-答案：
简单题，校招和初中岗位面试中常见。
前置知识：
单例模式常见的实现方式有，饿汉式、懒汉式、双重校验锁、静态内部类实现方式和枚举实现方式。

Java中通过以下措施来保证线程安全：

首先是使用同步机制，例如在懒汉式实现中，使用 synchronized 关键字来确保在多线程环境下只有一个线程可以创建实例；在双重校验锁实现中，使用 synchronized 关键字和双重检查来减少同步开销。

------------------------------

645. *实现单例模式为什么需要双重校验？, page url: https://www.mianshi.icu/question/detail?id=1202
645-1202-答案：
简单题，校招和初中岗位面试中常见。
前置知识：

实现单例模式时需要双重校验，主要有以下几点原因：

首先是性能优化，减少同步开销。在 Java 中，如果每次调用 getInstance() 方法都使用 synchronized 关键字进行同步，会导致性能下降，因为同步操作会阻塞其他线程。双重校验锁通过在同步代码块外部进行第一次检查，避免了不必要的同步操作。只有在实例未被创建时，才会进入同步代码块。

------------------------------

646. *单例模式有哪些应用场景？, page url: https://www.mianshi.icu/question/detail?id=1203
646-1203-答案：
简单题，校招和初中岗位面试中常见。
前置知识：
单例模式适用于需要全局唯一实例的场景，通常用于控制资源的共享和管理。例如，应用程序的全局配置（像数据库连接信息、系统参数等）需要在整个应用中共享，以确保配置的一致性。还有资源管理，比如数据库连接池、线程池和缓存等，避免资源浪费或冲突。日志记录系统也需要全局唯一，以确保所有日志记录到同一个文件或输出流。此外，像状态管理器、服务注册中心等全局唯一的服务也适合使用单例模式。

------------------------------

647. *你知道工厂模式吗？, page url: https://www.mianshi.icu/question/detail?id=1204
647-1204-答案：
简单题，校招和初中岗位面试中常见。

工厂模式（Factory Pattern）是一种创建型设计模式，它提供了一种创建对象的方式，而无需指定具体的类。工厂模式通过定义一个接口或抽象类来创建对象，但将具体对象的创建延迟到子类或工厂类中。这样可以将对象的创建与使用分离，提高代码的灵活性和可维护性。简单来说，工厂模式将对象的创建过程封装起来，客户端只需要通过工厂来获取对象，而无需关心对象的具体创建细节。

工厂模式的核心思想是：


工厂模式适用于以下场景：
工厂模式（Factory Pattern）是一种创建型设计模式，它提供了一种创建对象的方式，而不需要指定具体的类。

简单来说，工厂模式通过定义一个接口或抽象类来创建对象，但具体对象的创建则延迟到子类或工厂类中。这样做的好处是可以将对象的创建与使用分离，从而提高代码的灵活性和可维护性。客户端只需要通过工厂来获取对象，而不需要关心对象的具体创建细节。

工厂模式适用于以下场景。

------------------------------

648. *工厂模式具体是怎么用的，详细说说？, page url: https://www.mianshi.icu/question/detail?id=1205
648-1205-答案：
简单题，校招和初中岗位面试中常见。
工厂模式的应用场景有哪些？（mianshi.icu）的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
工厂模式其实是简单工厂模式、工厂方法模式和抽象工厂模式的统称，常用于以下场景：

首先是当对象的创建过程比较复杂，需要复杂的初始化逻辑（像依赖注入、配置加载等）时，工厂模式可以很好地封装这些逻辑。

其次是如果需要根据不同的条件创建不同的对象，比如根据配置文件或用户输入，工厂模式也能动态创建不同的产品。

------------------------------

649. *工厂模式的应用场景有哪些？, page url: https://www.mianshi.icu/question/detail?id=1206
649-1206-答案：
简单题，校招和初中岗位面试中常见。
前置知识：
工厂模式其实是简单工厂模式、工厂方法模式和抽象工厂模式的统称，常用于以下场景：

首先是当对象的创建过程比较复杂，需要复杂的初始化逻辑（像依赖注入、配置加载等）时，工厂模式可以很好地封装这些逻辑。

其次是如果需要根据不同的条件创建不同的对象，比如根据配置文件或用户输入，工厂模式也能动态创建不同的产品。

------------------------------

650. *说一下你比较熟悉的设计模式？, page url: https://www.mianshi.icu/question/detail?id=1207
650-1207-答案：
简单题，高频题，在校招和初中级岗位中常见。
你用过哪些设计模式？（mianshi.icu）的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
我熟悉以下几个设计模式：

首先是单例模式。这个模式确保一个类只有一个实例，并提供全局访问点。比如在数据库连接池中，我们希望整个系统只有一个连接池实例，避免重复创建和资源浪费。一个典型的实现是使用懒汉式+双重校验锁，确保在多线程环境下也只创建一个实例。

其次是工厂模式，它有三种具体实现方式，简单工厂、工厂方法和抽象工厂。它的核心是将对象的创建逻辑封装在工厂类中，客户端不需要关心具体实现。举个例子，在日志系统中，我可以根据配置动态创建文件日志或控制台日志。这种方式极大地提高了代码的灵活性和可扩展性。

------------------------------

651. *你用过哪些设计模式？, page url: https://www.mianshi.icu/question/detail?id=1208
651-1208-答案：
简单题，高频题，在校招和初中级岗位中常见。
前置知识：
我用过以下几个设计模式：

首先是单例模式。这个模式确保一个类只有一个实例，并提供全局访问点。比如在数据库连接池中，我们希望整个系统只有一个连接池实例，避免重复创建和资源浪费。一个典型的实现是使用懒汉式+双重校验锁，确保在多线程环境下也只创建一个实例。

其次是工厂模式，它有三种具体实现方式，简单工厂、工厂方法和抽象工厂。它的核心是将对象的创建逻辑封装在工厂类中，客户端不需要关心具体实现。举个例子，在日志系统中，我可以根据配置动态创建文件日志或控制台日志。这种方式极大地提高了代码的灵活性和可扩展性。

------------------------------

652. *你在项目中应用过哪些设计模式？用于解决什么问题？, page url: https://www.mianshi.icu/question/detail?id=1210
652-1210-答案：
简单题，高频题，在校招和初中级岗位中常见。
前置知识：
在实际项目中，我确实应用过多种设计模式来解决各种问题，下面我结合具体的例子来详细说明一下。

首先是单例模式。在电商平台中，我用它来管理全局唯一的订单 ID 生成器。这样做可以确保系统中只有一个订单 ID 生成器实例，避免多个实例导致 ID 冲突，保证订单数据的准确性和一致性，同时也能节约系统资源。

其次是工厂模式。在支付模块中，我用它来根据不同的支付类型（比如支付宝、微信支付、银行卡支付）创建不同的支付客户端。这样可以将支付客户端的创建逻辑与业务代码解耦，方便扩展新的支付方式，提高代码的可维护性和可扩展性，同时也简化了对象创建逻辑。

------------------------------

653. *介绍一下策略模式, page url: https://www.mianshi.icu/question/detail?id=1211
653-1211-答案：
简单题，校招和初级岗位中常见。
策略模式（Strategy Pattern）是一种行为型设计模式，它定义了一系列算法，并将每个算法封装到独立的策略类中，使得它们可以互相替换，让算法的变化独立于使用算法的客户。

核心思想：

适用场景
策略模式（Strategy Pattern）是一种行为型设计模式，它的核心思想是定义一系列算法，然后将每个算法封装到独立的策略类中。这样做的好处是，这些算法可以互相替换，而且算法的变化不会影响到使用算法的客户端。

策略模式在很多场景下都非常有用。比如，当有多种算法可以选择，并且客户端需要根据需要选择不同的算法时，就可以使用策略模式。比如，使用不同的策略类来实现不同的缓存策略（LRU、FIFO、LFU）。另外，当算法需要在运行时灵活切换时，策略模式也能派上用场。比如，支付系统可以使用不同的策略类来实现不同的支付方式（支付宝、微信支付、银行卡支付）。还有，当代码中存在大量的 if-else 语句，可以使用策略模式将不同的分支封装到独立的策略类中，从而简化代码。

策略模式的优点有很多。它符合开闭原则，可以在不修改原有代码的情况下引入新的策略。它将算法的实现与使用算法的客户端解耦，提高了代码的可维护性。它可以在运行时动态地切换算法，而不需要修改客户端代码。最重要的是，它可以消除大量的条件语句，使代码更加简洁。

------------------------------

654. 进程、线程、协程各自的使用场景是什么？, page url: https://www.mianshi.icu/question/detail?id=1213
654-1213-答案：
基础题，在校招和初级工程师面试中比较常见。

要在这个问题之下刷出亮点，最好就是讲清楚沿着进程、线程和协程三者的演进趋势。
进程、线程、协程的区别是什么？它们各自的使用场景是什么？（mianshi.icu）的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
进程是操作系统进行资源分配和调度的基本单位，每个进程都有独立的内存空间和系统资源，这意味着进程间的资源是隔离的。进程适用于资源隔离性要求高的场景，比如 Web 服务器可以使用多进程来处理不同的客户端请求，这样即使某个进程崩溃，也不会影响到其他进程，保证了服务器的稳定性。对于 CPU 密集型任务，像科学计算、大规模数据处理、视频编码等，由于某些编程语言（比如 Python）的全局解释器锁（GIL）限制了多线程的并行执行，因此使用多进程可以更好地利用多核 CPU 的优势。另外，对于需要稳定性的系统服务，比如操作系统中的一些关键服务，像日志服务、监控服务等，通常使用多进程来实现，这样可以保证一个服务的崩溃不会影响到其他服务。

线程是进程中的一个执行单元，共享进程的内存空间和资源。线程之间的切换开销比进程小。线程适用于 I/O 密集型任务，比如网络爬虫、文件读写、数据库操作等，这些任务需要频繁地进行 I/O 操作，多线程可以并发地执行这些 I/O 操作，提高程序的效率。对于需要共享数据的任务，多线程可以方便地共享进程的内存空间，因此适用于需要共享数据的任务，比如一个多线程的图像处理程序，多个线程可以同时访问和修改同一张图片。GUI 应用程序通常也需要在单独的线程中处理耗时的操作，这样可以避免阻塞主线程，保证用户界面的响应性。

协程是一种用户态的轻量级线程，可以在单线程中实现并发。协程的切换由程序员控制，避免了线程切换的开销。协程适用于高并发、I/O 密集型任务，比如异步 Web 服务器、网络游戏服务器等，这些服务器需要处理大量的并发连接，协程可以高效地处理这些连接，提高服务器的吞吐量。对于异步编程，比如异步爬虫、异步任务队列等，这些任务需要异步地执行多个操作，协程可以简化异步编程的复杂性，提高代码的可读性和可维护性。另外，像游戏中的角色 AI、网络协议的解析等，这些任务可以抽象成状态机，协程可以方便地实现状态机的切换和管理。
整体上来说，这三者体现了一个趋势：就是在当下任务越来越重，并且计算机性能也越来越好的时候，我们需要更细粒度、更轻量级的机制。

比如说进程到线程，我们就有了多线程编程，这样可以在共享别的资源的时候，独立调度不同的线程，充分利用 CPU 的计算能力。

------------------------------

655. 进程、线程、协程的区别是什么？它们各自的使用场景是什么？, page url: https://www.mianshi.icu/question/detail?id=1214
655-1214-答案：
基础题，在校招和初级工程师面试中比较常见。

要在这个问题之下刷出亮点，最好就是讲清楚沿着进程、线程和协程三者的演进趋势。
前置知识：

进程、线程和协程是实现并发和并行的三种不同方式，它们各自有不同的特点和适用场景。
进程是操作系统进行资源分配和调度的基本单位，每个进程都有独立的内存空间和系统资源，这意味着进程间的资源是隔离的。进程适用于资源隔离性要求高的场景，比如 Web 服务器可以使用多进程来处理不同的客户端请求，这样即使某个进程崩溃，也不会影响到其他进程，保证了服务器的稳定性。对于 CPU 密集型任务，像科学计算、大规模数据处理、视频编码等，由于某些编程语言（比如 Python）的全局解释器锁（GIL）限制了多线程的并行执行，因此使用多进程可以更好地利用多核 CPU 的优势。另外，对于需要稳定性的系统服务，比如操作系统中的一些关键服务，像日志服务、监控服务等，通常使用多进程来实现，这样可以保证一个服务的崩溃不会影响到其他服务。

线程是进程中的一个执行单元，共享进程的内存空间和资源。线程之间的切换开销比进程小。线程适用于 I/O 密集型任务，比如网络爬虫、文件读写、数据库操作等，这些任务需要频繁地进行 I/O 操作，多线程可以并发地执行这些 I/O 操作，提高程序的效率。对于需要共享数据的任务，多线程可以方便地共享进程的内存空间，因此适用于需要共享数据的任务，比如一个多线程的图像处理程序，多个线程可以同时访问和修改同一张图片。GUI 应用程序通常也需要在单独的线程中处理耗时的操作，这样可以避免阻塞主线程，保证用户界面的响应性。

协程是一种用户态的轻量级线程，可以在单线程中实现并发。协程的切换由程序员控制，避免了线程切换的开销。协程适用于高并发、I/O 密集型任务，比如异步 Web 服务器、网络游戏服务器等，这些服务器需要处理大量的并发连接，协程可以高效地处理这些连接，提高服务器的吞吐量。对于异步编程，比如异步爬虫、异步任务队列等，这些任务需要异步地执行多个操作，协程可以简化异步编程的复杂性，提高代码的可读性和可维护性。另外，像游戏中的角色 AI、网络协议的解析等，这些任务可以抽象成状态机，协程可以方便地实现状态机的切换和管理。
整体上来说，这三者体现了一个趋势：就是在当下任务越来越重，并且计算机性能也越来越好的时候，我们需要更细粒度、更轻量级的机制。

比如说进程到线程，我们就有了多线程编程，这样可以在共享别的资源的时候，独立调度不同的线程，充分利用 CPU 的计算能力。

------------------------------

656. *什么是协程？, page url: https://www.mianshi.icu/question/detail?id=1216
656-1216-答案：
简单题，高频题，在校招的时候尤其常见，初级工程师也有可能遇到，中级工程师以上就不常见了。
协程（Coroutine）：协程是轻量级的⽤户态线程，由协程的运行时在用户态调度。协程的创建和销毁⽐线程更为轻量，可以很容易地创建⼤量的协程。
协程是一种轻量级的用户态线程，由协程的运行时在用户态调度。协程的创建和销毁比线程更轻量，所以可以很容易地创建大量的协程。最为典型的就是 Go 的协程；

协程在高并发和 I/O 密集型任务中非常有用。比如，异步 Web 服务器、网络游戏服务器等，这些服务器需要处理大量的并发连接。协程可以高效地处理这些连接，提高服务器的吞吐量。

------------------------------

657. *什么是简单工厂模式？, page url: https://www.mianshi.icu/question/detail?id=1217
657-1217-答案：
简单题，校招和初级岗位面试中常见。
简单工厂模式（Simple Factory Pattern），也称为静态工厂方法模式，属于创建型设计模式。它提供一个统一的入口（通常是一个静态方法），根据客户端提供的参数来决定创建哪种具体类的实例。简单工厂模式的核心在于将对象的创建逻辑封装在一个工厂类中，客户端只需要传入相应的参数，而无需关心对象的具体创建过程。

核心思想：

适用场景：
简单工厂模式，也称为静态工厂方法模式，是一种创建型设计模式。可以其理解为一个"万能创建器"。想象一下，我们有一个工厂类，它就像一个生产线的总调度，根据你传入的参数，直接决定生产什么产品。比如说，如果你传入"汽车"，它就生产汽车；传入"自行车"，它就生产自行车。

------------------------------

658. *什么是工厂方法模式？, page url: https://www.mianshi.icu/question/detail?id=1218
658-1218-答案：
简单题，校招和初级岗位面试中常见。
前置知识：

工厂方法模式（Factory Method Pattern）是一种创建型设计模式，它定义一个用于创建对象的接口，但允许子类决定实例化哪个类。工厂方法模式将类的实例化推迟到子类中进行，从而使得代码更加灵活和可扩展。

核心思想：
工厂方法模式是一种创建型设计模式，可以说是对简单工厂模式的一种优雅升级。它不再是一个“万能创建器”，而是为每种产品都定制了专属的工厂。就像是每种产品都有自己的生产线，想要新增产品时，只需要添加新的工厂类，不需要修改已有的代码（遵守了开闭原则），因此它也更加灵活、扩展性也更强。举个例子，汽车工厂只负责生产汽车，自行车工厂只负责生产自行车，互不干扰。

------------------------------

659. *什么是抽象工厂模式？, page url: https://www.mianshi.icu/question/detail?id=1219
659-1219-答案：
简单题，校招和初级岗位面试中常见。
前置知识：

抽象工厂模式（Abstract Factory Pattern）是一种创建型设计模式，它提供一个创建一系列相关或相互依赖对象的接口，而无需指定它们的具体类。抽象工厂模式的核心在于提供一个接口，用于创建一组相关或依赖的对象，而不需要指定具体的实现类。

核心思想：
抽象工厂模式是一种创建型设计模式，它比简单工厂模式和工厂方法模式更加复杂和强大。这个模式的核心在于它不仅仅是生产单一产品，而是能够生产一整个产品家族。

------------------------------

660. *简单工厂模式、工厂方法模式、抽象工厂模式的区别是什么？, page url: https://www.mianshi.icu/question/detail?id=1220
660-1220-答案：
简单题，校招和初级岗位面试中常见。
前置知识：



简单工厂模式：简单易用，但不符合开闭原则。
工厂方法模式：符合开闭原则，但类的数量增加。
我将从扩展性、灵活性、复杂性和使用场景四个维度，详细阐述这三种工厂模式的区别。

简单工厂模式可以理解为一个"万能创建器"。想象一下，我们有一个工厂类，它就像一个生产线的总调度，根据你传入的参数，直接决定生产什么产品。比如说，如果你传入"汽车"，它就生产汽车；传入"自行车"，它就生产自行车。这种模式非常直接，但问题是每次新增产品类型，我们都得修改这个工厂类的代码，这违背了开闭原则。因此其灵活性、扩展性最弱，但复杂性较低，代码实现简单直接。适合那些产品类型少、创建逻辑简单的场景。

工厂方法模式则更加灵活、扩展性也更强。它不再是一个万能工厂，而是为每种产品都定制了专属的工厂。就像是每种产品都有自己的生产线，想要新增产品时，只需要添加新的工厂类，不需要修改已有的代码。这样就很好地遵守了开闭原则。举个例子，汽车工厂只负责生产汽车，自行车工厂只负责生产自行车，互不干扰。在复杂性上，它比简单工厂稍微复杂一些，因为需要为每种产品定义对应的工厂类。但这种复杂性带来的是更好的扩展性和解耦。适用场景更广，特别是当你预期未来会不断添加新的产品类型时。

------------------------------

661. *工厂方法模式和抽象工厂模式的区别是什么？, page url: https://www.mianshi.icu/question/detail?id=1221
661-1221-答案：
简单题，校招和初级岗位面试中常见。
前置知识：
我将从扩展性、灵活性、复杂性和使用场景四个维度，详细阐述二者的区别。

工厂方法模式可以说是对简单工厂模式的一种优雅升级。它不再是一个“万能创建器”，而是为每种产品都定制了专属的工厂。就像是每种产品都有自己的生产线，想要新增产品时，只需要添加新的工厂类，不需要修改已有的代码（遵守了开闭原则），因此它也更加灵活、扩展性也更强。举个例子，汽车工厂只负责生产汽车，自行车工厂只负责生产自行车，互不干扰。当然，它也局限性。它相比简单工厂模式，代码实现更加复杂。因为每种产品都需要一个对应的工厂类，随着产品类型的增加，类的数量也会线性增长。但这种复杂性是有价值的，它带来了更好的解耦和扩展性。当你预期系统会不断添加新的产品类型，并希望保持代码的可维护性和扩展性，工厂方法模式就是一个不错的选择。

------------------------------

662. *简单工厂模式和工厂方法模式的区别是什么？, page url: https://www.mianshi.icu/question/detail?id=1222
662-1222-答案：
简单题，校招和初级岗位面试中常见。
前置知识：
我将从扩展性、灵活性、复杂性和使用场景四个维度，详细阐述二者的区别。

简单工厂模式可以理解为一个"万能创建器"。想象一下，我们有一个工厂类，它就像一个生产线的总调度，根据你传入的参数，直接决定生产什么产品。比如说，如果你传入"汽车"，它就生产汽车；传入"自行车"，它就生产自行车。这种模式非常直接，但问题是每次新增产品类型，我们都得修改这个工厂类的代码，这违背了开闭原则。因此其灵活性、扩展性最弱，但复杂性较低，代码实现简单直接。适合那些产品类型少、创建逻辑简单的场景。

------------------------------

663. *什么是建造者模式？, page url: https://www.mianshi.icu/question/detail?id=1223
663-1223-答案：
略难的题，一般在校招和初中级岗位面试中可能遇到。
建造者模式（Builder Pattern）是一种创建型设计模式，它允许你分步骤创建复杂对象。建造者模式将对象的构建过程与其表示分离，使得同样的构建过程可以创建不同的表示。建造者模式允许客户端通过指定不同的建造者来创建不同类型的对象，而无需关心对象的具体构建过程。

核心思想：


适用场景：
建造者模式（Builder Pattern）是一种创建型设计模式，它允许我们分步骤创建复杂对象。简单来说，这种模式可以将一个复杂对象的构建过程分解成多个步骤，同时还能让我们在不同的步骤中灵活地创建对象。

建造者模式主要由几个关键部分组成。抽象建造者定义了构建对象的抽象方法，比如设置属性、添加组件等。具体建造者则实现这些抽象方法，负责构建具体的产品对象。指挥者负责协调建造者，按照特定的顺序执行构建步骤。产品就是最终被构建的复杂对象。客户端则负责创建指挥者和建造者，并调用指挥者来构建对象。

举个实际的例子，假设我们要创建一个复杂的计算机对象。抽象建造者可能定义了设置CPU、内存、硬盘等方法。具体的建造者可能是台式机建造者和笔记本建造者，它们会根据自己的特点实现这些方法。指挥者则负责按照一定的顺序调用这些方法，最终创建出一台完整的计算机。

------------------------------

664. *工厂模式和建造者模式的区别是什么？, page url: https://www.mianshi.icu/question/detail?id=1224
664-1224-答案：
前置知识：


建造者模式与工厂模式的相同点：


建造者模式与工厂模式的区别
建造者模式和工厂模式之间主要有以下区别：

首先，在关注点上，工厂模式关注的是对象的创建方式，也就是说，它是如何通过工厂类或工厂方法来创建对象的。而建造者模式则更关注对象的构建过程，具体来说，就是如何分步骤构建一个复杂的对象。

其次，在构建方式上，工厂模式通常通过一个工厂类或工厂方法直接创建对象，客户端不需要关心对象的构建过程。而建造者模式则是通过指挥者来控制建造者，分步骤构建对象，客户端需要了解指挥者和建造者的接口。

------------------------------

665. *什么是模版模式？有哪些应用场景？, page url: https://www.mianshi.icu/question/detail?id=1225
665-1225-答案：
简单题，在校招和初级岗位中常见。
模板模式（Template Method Pattern）是一种行为型设计模式，它定义了一个算法的框架，并将某些步骤的具体实现延迟到子类中，即允许子类在不改变算法整体结构的情况下重新定义算法的某些步骤。

核心思想：

适用场景
模板模式（Template Method Pattern）是一种行为型设计模式，它的主要作用是定义一个算法的框架，然后把一些具体步骤的实现延迟到子类中。简单来说，就是父类定义好算法的骨架，子类负责实现特定的步骤，但不能改变算法的整体结构。

模板模式在很多场景下都非常有用。比如，Web 框架处理 HTTP 请求的流程，像 Spring MVC 中的 DispatcherServlet，就是一个典型的模板模式应用。单元测试框架，比如 JUnit 中的 TestCase，它的测试用例执行流程也使用了模板模式。还有数据库访问，像 JDBC 中的 JdbcTemplate等，都用到了模板模式。

模板模式的优点很明显。它可以提高代码的复用性，因为算法的通用部分都放在抽象类中，子类只需要实现特定的步骤，减少了重复代码。同时，它也具有良好的扩展性，可以通过增加新的具体子类，重写算法的某些步骤来扩展算法，而不需要改变算法的整体结构。最重要的是，模板方法模式通过抽象类控制算法的结构，子类只能实现特定的步骤，不能改变算法的整体流程，保证了算法的稳定性。

------------------------------

666. *策略模式和模版方法模式的区别是什么？, page url: https://www.mianshi.icu/question/detail?id=1227
666-1227-答案：
简单题，校招和初级岗位面试中常见。
前置知识：
策略模式和模板方法模式都是行为型设计模式，它们都旨在解决代码复用和扩展性的问题，但它们的应用场景和侧重点有所不同。

首先，在关注点上，策略模式关注的是算法的替换。它定义了一系列算法，并将每个算法封装到独立的策略类中，使得它们可以互相替换，让算法的变化独立于使用算法的客户端。客户端可以根据需要选择不同的策略。而模板方法模式关注的是算法的结构。它定义一个操作中的算法的骨架，而将一些步骤延迟到子类中。模板方法使得子类可以不改变一个算法的结构即可重定义该算法的某些特定步骤。

在适用场景上，策略模式适用于需要动态切换算法的场景，例如支付方式选择、排序算法选择等。当有多种算法可以选择，并且客户端需要根据需要选择不同的算法时，策略模式是一个很好的选择。模板方法模式适用于算法的步骤固定，但某些步骤的实现可能不同的场景，例如数据导出流程、测试框架用例执行流程等。当完成某一个过程的步骤是固定的，但是每一步的具体实现可能不同时，模板方法模式是一个很好的选择。

------------------------------

667. *什么是行锁？, page url: https://www.mianshi.icu/question/detail?id=1228
667-1228-答案：
简单题，校招和初中级岗位面试中常见。

行锁是数据库中一种细粒度的锁，用于控制对数据库表中单行记录的并发访问。它锁定的是表中的特定行，而不是整个表。它的主要目的是在多用户环境中，确保数据的一致性和完整性，同时允许多个事务并发地访问同一表中的不同记录。当一个事务获取了某一行数据的行锁后，其他事务在修改或删除该行数据时会被阻塞，直到该事务释放锁。

行锁的特点：


行锁的作用：
行锁是数据库中一种细粒度的锁，它用于控制对数据库表中单行记录的并发访问。行锁锁定的是表中的特定行，而不是整个表。

行锁主要有两方面作用。一是保证数据一致性，当一个事务获取了某一行数据的行锁后，其他事务在修改或删除该行数据时会被阻塞，直到该事务释放锁，防止并发事务修改同一行数据，从而保证数据的一致性。二是提高并发性，允许不同的事务同时访问同一张表的不同行，从而提高了并发性。

行锁通常由数据库管理系统（DBMS）在执行数据操作时自动管理。具体的实现方式可能因数据库系统而异，以 MySQL 为例，在使用 InnoDB 存储引擎时，MySQL 会自动为每个行级操作加锁。可以使用 SELECT ... FOR UPDATE 来显式地请求行锁，也就是排他锁。

------------------------------

668. *什么是页锁？, page url: https://www.mianshi.icu/question/detail?id=1229
668-1229-答案：
简单题，校招和初中级岗位中常见。


页锁是数据库管理系统中一种锁的粒度，它锁定的是数据页（Page），而不是具体的行。数据页是数据库存储数据的基本单位，一个页通常包含多行记录，页的大小通常为 4KB 或 8KB。当一个事务需要访问或修改某个数据页中的数据时，数据库系统会对整个数据页加锁。

页锁的特点


页锁的优缺点：
页锁是数据库管理系统中一种锁的粒度，它锁定的是数据页（Page），而不是具体的行。数据页是数据库存储数据的基本单位，一个页通常包含多行记录，页的大小通常是 4KB 或 8KB。当一个事务需要访问或修改某个数据页中的数据时，数据库系统会对整个数据页加锁。

页锁的优点是，它的管理开销较小，相对于行锁，页锁需要管理的锁数量较少，降低了锁管理的开销。它的并发性能较高，相对于表锁，页锁锁定的范围较小，提高了并发性能。而且，页锁的实现相对简单，相对于行锁，页锁的实现复杂度较低。页锁在性能和资源控制之间达到了一个平衡，比表锁并发性能好，比行锁开销小，适合中等并发场景。缺点是，它的并发性能较低，相对于行锁，页锁的并发性能较低，因为多个事务可能需要访问同一个数据页。它的锁定范围较大，相对于行锁，页锁锁定的范围较大，可能导致不必要的锁冲突。页锁的锁定粒度不够精细，可能锁定不需要的记录，并发性能不如行锁。而且，页锁的管理相对复杂，需要额外的锁管理机制，增加了系统复杂度。

------------------------------

669. *什么是表锁？, page url: https://www.mianshi.icu/question/detail?id=1230
669-1230-答案：
简单题，在校招和初中级岗位面试中常见。
表锁是数据库管理系统（DBMS）中粒度最粗的锁机制。它直接锁定整个数据表，而非表中的特定行或页。当一个事务获得表锁时，其他事务对该表的所有访问（读或写）都会受到限制，直到锁被释放。

表锁的优缺点：
适用场景：
表锁是数据库管理系统（DBMS）中粒度最粗的锁机制。它直接锁定整个数据表，而不是表中的特定行或者数据页。当一个事务获得表锁时，其他事务对该表的所有访问（无论是读还是写）都会受到限制，直到锁被释放。

表锁的优点是，它的实现简单，锁管理逻辑相对简单，易于实现。它的管理开销低，由于锁数量少（每个表最多一个共享锁和一个排他锁），锁管理器的开销较小。它的资源消耗少，锁本身占用的内存资源较少。在特定情况下，表锁可以做到无死锁，如果所有事务都只获取表锁，且按照相同顺序获取，则不会发生死锁（但实际应用中很少这样使用）。缺点是，表锁的并发性能极低，只要有一个事务持有表锁，其他所有事务对该表的访问都会被阻塞，严重限制并发。它的锁定范围过大，即使事务只需要修改表中的一行数据，也会锁定整个表，导致大量不必要的阻塞。在一般情况下，表锁容易发生死锁，如果事务需要获取多个表的锁，且获取顺序不一致，很容易发生死锁。

------------------------------

670. *什么是乐观锁？, page url: https://www.mianshi.icu/question/detail?id=1231
670-1231-答案：
简单题，校招和初中级岗位面试中常见。

乐观锁是一种并发控制策略，它秉持“乐观”的态度，假设事务间数据竞争不常发生。因此，它不在读取数据时立即加锁，而是在提交更新前检查数据是否被其他事务修改过。若未被修改，则提交；若已修改（即发生冲突），则通常回滚事务并重试。简单来说，就是通过"检测-冲突-重试"实现无锁并发。

核心思想：

优缺点：
乐观锁是一种并发控制策略，它假设事务间的数据竞争不常发生。因此，它在读取数据时不会立即加锁，而是在提交更新前检查数据是否被其他事务修改过。如果没有被修改，就提交；如果已经被修改了（也就是发生了冲突），通常会回滚事务并重试。简单来说，乐观锁就是通过“检测-冲突-重试”来实现无锁并发。

实现乐观锁通常有两种方式，一种是版本号机制，另一种是时间戳机制。

乐观锁优点很明显，由于无锁读取，大大提升了读操作的并发性能。同时，它避免了锁的开销和死锁风险。而且，实现起来相对简单，只需要在数据表中增加一个额外的字段，并在更新时添加校验逻辑即可。

------------------------------

671. *什么是悲观锁？, page url: https://www.mianshi.icu/question/detail?id=1232
671-1232-答案：
简单题，校招和初中级岗位面试中常见。

悲观锁（Pessimistic Locking）是一种并发控制策略，它对数据冲突持“悲观”态度，认为事务间的数据竞争 总是 会发生。因此，悲观锁在事务开始之前就对所需数据资源加锁，阻止其他事务的并发访问（读取或修改），直到当前事务完成并释放锁。

核心思想：

优缺点：
悲观锁是一种并发控制策略，它对数据冲突持“悲观”态度，认为事务间的数据竞争总是会发生。因此，悲观锁在事务开始之前就对所需的数据资源加锁，阻止其他事务的并发访问（读取或修改），直到当前事务完成并释放锁。

悲观锁的优点是，它能严格防止并发冲突，保证数据在事务中的一致性，提供强数据一致性。而且，实现起来比较简单，因为数据库层面直接支持。在高冲突场景下，比如写操作频繁、冲突概率高的场景，悲观锁能有效避免数据不一致。当然，它也有局限性。由于锁的独占性，导致并发受限，吞吐量会降低，并发性能较低。同时，还存在死锁风险，如果多个事务互相等待对方持有的锁，就可能导致死锁，这需要通过锁顺序、超时等机制来预防。锁的管理（获取、释放、监控）也会消耗一定的系统资源。

悲观锁比较适用于写多读少、数据竞争激烈、系统并发量不高以及对数据一致性要求极高的场景，比如银行转账、金融交易等。

------------------------------

672. *行锁、页锁、表锁的区别是什么？, page url: https://www.mianshi.icu/question/detail?id=1233
672-1233-答案：
简单题，校招和初中级岗位面试中常见。
前置知识：


在数据库管理系统（DBMS）中，为了控制并发访问并保证数据一致性，引入了锁机制。行锁、页锁和表锁是三种不同粒度的锁，它们的主要区别体现在以下几个方面：
在数据库管理系统（DBMS）中，为了控制并发访问并保证数据一致性，引入了锁机制。行锁、页锁和表锁是三种不同粒度的锁，它们的主要区别体现在以下几个方面：

首先是锁定范围，也就是锁的粒度。行锁锁定的是数据表中的单行记录，粒度最细，只锁定需要操作的特定行。页锁锁定的是数据表中的一个数据页，一个数据页通常包含多行记录，页的大小通常为 4KB、8KB 或 16KB，具体大小取决于 DBMS。页锁的粒度适中，它会锁定一个数据页内的所有行。表锁锁定的是整个数据表，粒度最粗，它会锁定表中的所有行和所有数据页。

其次是并发性能。行锁的并发性能最高，因为它只锁定单行，允许多个事务同时访问不同行的数据，最大程度地减少了锁冲突。页锁的并发性能适中，它允许不同事务访问不同数据页，但同一数据页内的所有行会被锁定，锁冲突的可能性高于行锁，低于表锁。表锁的并发性能最低，只要有一个事务持有表锁，其他所有事务对该表的访问都会被阻塞。

------------------------------

673. *乐观锁的应用场景有哪些？, page url: https://www.mianshi.icu/question/detail?id=1234
673-1234-答案：
简单题，校招和初级岗位面试中常见。
前置知识：

乐观锁的应用场景如下：


总结
乐观锁的应用场景主要有以下几个方面：

首先是读多写少型的应用。像社交网络中的点赞、评论、分享等操作，读请求远高于写请求，这时使用乐观锁可以确保互动数据的原子性更新，避免频繁的锁竞争，从而减少数据库的锁开销，优化并发性能，降低延迟。

其次是低冲突更新场景。比如电商秒杀，大量用户抢购少量商品，成功者很少。乐观锁可以控制库存扣减，失败的事务快速回滚，减少锁的持有时间，并防止超卖，保证数据一致性。

------------------------------

674. *使用悲观锁时需要注意些什么？, page url: https://www.mianshi.icu/question/detail?id=1235
674-1235-答案：
简单题，校招和初中级岗位面试中常见。
前置知识：

悲观锁虽然能有效保证数据强一致性，但使用不当可能导致性能问题、死锁等风险。以下是使用悲观锁时需要重点关注的方面：
使用悲观锁时需要注意以下几个方面：

首先是死锁的预防与处理。可以采取固定锁的顺序、避免长事务、减少锁的数量、设置锁超时等方式。

其次是锁粒度的控制。锁的粒度如果过大，比如表锁，会严重降低并发性，导致大量事务被阻塞。锁的粒度如果过小，比如行锁过多，会增加锁管理的开销，可能影响性能。通常建议尽量使用行锁，因为行锁只锁定受影响的数据行，可以最大程度地提高并发。SELECT ... FOR UPDATE 语句的 WHERE 条件要精确，只锁定真正需要修改的行。

------------------------------

675. *悲观锁的应用场景有哪些？, page url: https://www.mianshi.icu/question/detail?id=1236
675-1236-答案：
简单题，校招和初中级岗位面试中常见。
前置知识：
悲观锁是一种强调强一致性的并发控制机制，它在事务开始之前就对数据加锁，确保数据在整个事务期间的独占性。这种“先锁后用”的策略适用于以下场景：

首先是写多读少的场景，也就是写操作（更新、删除）远比读操作频繁。例如，电商系统的秒杀活动

其次是高冲突场景，即多个事务同时修改同一数据的概率较高。例如，银行账户转账操作。

------------------------------

676. *使用乐观锁时需要注意些什么？, page url: https://www.mianshi.icu/question/detail?id=1237
676-1237-答案：
简单题，校招和初中级岗位面试中常见。
前置知识：

使用乐观锁时需要注意以下关键点，以确保其有效性和可靠性：


总结
使用乐观锁时需要以下几点：

首先是版本控制机制。乐观锁依赖于版本号或时间戳机制，这些机制的正确性直接影响到锁的有效性。我们需要在数据表中添加一个 version 或 timestamp 字段，确保这个字段的类型和大小足够应对高频更新。每次更新数据时，都要同时更新版本字段，防止版本控制失效。更新操作必须是原子性的，同时，还要注意防止版本号溢出，可以使用循环递增或较大数据类型。

然后是冲突检测与重试机制。乐观锁在提交时检测冲突，如果检测到冲突，事务需要回滚并重试。需要实现一个智能的重试策略，比如指数退避，避免立即重试造成的冲突加剧。同时，要设置合理的最大重试次数，防止无限重试导致系统资源耗尽。重试间隔也要设置得当，以减少冲突概率。最重要的是，要确保重试操作是幂等的，避免重复提交导致数据错误。

------------------------------

677. *什么是意向锁？, page url: https://www.mianshi.icu/question/detail?id=1239
677-1239-答案：
简单题，校招和初中级岗位面试中常见。
前置知识：


意向锁是数据库管理系统（DBMS）中一种特殊的 表级锁，用于优化多粒度锁定（Multiple Granularity Locking）环境下的并发控制。它主要服务于行锁和表锁并存的场景，提高锁管理的效率。

当数据库系统同时支持行锁（细粒度）和表锁（粗粒度）时，如果一个事务持有某些行的行锁，另一个事务尝试获取整个表的表锁，就需要一种机制来快速判断是否存在冲突，而无需扫描整个表的所有行锁。意向锁通过在表级别标记事务对表中行的锁定意图，来解决这个问题。
意向锁是数据库管理系统（DBMS）中一种特殊的表级锁，它主要用于优化多粒度锁定环境下的并发控制。简单来说，就是当数据库系统同时支持行锁（细粒度）和表锁（粗粒度）时，意向锁可以提高锁管理的效率。

意向锁的主要作用是提高锁冲突检测效率。当一个事务要获取表锁时，只需要检查表上是否存在冲突的意向锁，而无需扫描所有行锁。它还支持多粒度锁定，允许数据库系统同时使用行锁和表锁，实现更灵活的并发控制。同时，通过意向锁，可以更容易地管理锁的层次结构（表级锁和行级锁）。

以MySQL为例，在MySQL 的InnoDB 存储引擎中，意向锁是自动管理的，用户无需显式请求或释放意向锁。当事务需要获取行锁时，InnoDB 会自动先获取相应的意向锁。可以通过 SHOW ENGINE INNODB STATUS; 命令查看当前 InnoDB 引擎的锁状态信息，其中可能包含意向锁的信息。

------------------------------

678. *什么是共享锁？, page url: https://www.mianshi.icu/question/detail?id=1240
678-1240-答案：
简单题，校招和初中级岗位面试中常见。
共享锁（Shared Lock，简称 S 锁）是数据库并发控制中的一种基本锁类型，用于实现多个事务对同一数据的并发读取。它允许多个事务同时读取数据，但阻止任何事务修改数据，从而在保证读一致性（防止读取过程中数据被修改）的同时提高并发性能。

锁的兼容性：


加锁方式：
共享锁（Shared Lock，简称 S 锁）是数据库并发控制中的一种基本锁类型，主要用于实现多个事务对同一数据的并发读取。它允许多个事务同时读取数据，但阻止任何事务修改数据，这样既保证了读一致性，防止读取过程中数据被修改，同时又提高了并发性能。

共享锁适用于以下场景：首先是读多写少的场景，例如数据查询、报表生成等，这些操作读取频繁但修改较少。其次，共享锁允许多个事务同时读取同一数据，提高并发性能。最后，共享锁在读取期间可以防止数据被修改，保证了读取的数据一致性，防止了脏读和不可重复读等问题。

------------------------------

679. *什么是排他锁？, page url: https://www.mianshi.icu/question/detail?id=1241
679-1241-答案：
简单题，校招和初中级岗位面试中常见。
排他锁（X 锁）是一种独占式的数据库锁机制，一旦某个事务对数据加了排他锁，其他事务无法再获取任何类型的锁（包括共享锁和排他锁）来读写同一份数据，直到持有排他锁的事务释放锁。它在保证写操作安全、避免并发修改冲突方面发挥了至关重要的作用。排他锁有以下几大核心要点：


应用场景：

注意事项
排他锁，也就是 X 锁。排他锁是一种独占式的数据库锁机制，一旦某个事务对数据加了排他锁，其他任何事务就无法在这份数据上再加任何类型的锁，包括共享锁和排他锁，直到当前事务释放锁。这样一来，可以保证写操作的安全性，有效避免并发修改冲突，我觉得这在实际工作中非常关键。

不同的数据库实现也会有所差异。例如，在 MySQL 中，InnoDB 引擎默认使用行级锁，一般我们会通过 SELECT ... FOR UPDATE 加锁；而 MyISAM 则使用表级锁，可能一次操作就会阻塞整表的读写。PostgreSQL、Oracle 等数据库提供的类似语法和机制也在业务中有应用，但他们的内部实现还是略有不同的。

排他锁适用于数据修改操作。比如说在库存扣减或者用户余额更新这种高一致性要求的场景中，必须确保在更新期间数据不会被其他事务读到或修改。另外，在一些涉及关键业务操作，比如电子支付、资金转账这类操作时，要求事务的原子性和隔离性非常高，这时候排他锁就能确保其他事务不能同时对同一数据进行修改，从而确保数据的准确性和安全性。

------------------------------

680. *共享锁的应用场景有哪些？, page url: https://www.mianshi.icu/question/detail?id=1242
680-1242-答案：
简单题，校招和初中级岗位面试中常见。
前置知识：

共享锁（Shared Lock，简称 S 锁）的核心价值在于平衡读取并发与数据一致性。它允许多个事务同时读取同一数据，但阻止任何事务修改数据。因此，共享锁主要应用于以下几类场景：


总结
共享锁主要用于读多写少，并且需要保证读取期间数据一致性或者需要防止脏读或不可重复读的场景。在多版本并发控制（MVCC）中，在可重复读（Repeatable Read）隔离级别下，数据库可能会隐式地使用共享锁来防止其他事务修改已经读取的数据，从而避免不可重复读。在读写分离架构中的读库，因为读库主要处理大量的只读查询，共享锁可以允许很高的并发。而写库则处理排他锁操作，读写分离可以降低主库的压力。

------------------------------

681. *排他锁的应用场景有哪些？, page url: https://www.mianshi.icu/question/detail?id=1243
681-1243-答案：
简单题，校招和初中级岗位面试中常见。
前置知识：


排他锁主要用于那些必须保证数据修改的独占性、原子性的场景，防止并发操作导致数据不一致、更新丢失等问题。以下是几个典型应用场景：


注意事项：
排他锁主要用于那些必须保证数据修改的独占性和原子性的场景，防止并发操作导致数据不一致、更新丢失等问题。例如，在插入或更新数据时，需要保证某些字段的唯一性，比如用户名、邮箱。核心需求是防止并发插入导致重复数据。实现思路是，开始一个事务，然后用 SELECT 1 FROM users WHERE username = 'new_user' FOR UPDATE; 检查用户名是否已存在，这里加了排他锁。如果用户名不存在，就插入新用户，否则就回滚事务。排他锁的价值在于，FOR UPDATE 锁定了潜在冲突行，确保了唯一性检查的准确性，避免了并发插入可能导致的重复数据。

------------------------------

682. *使用共享锁时需要注意些什么？, page url: https://www.mianshi.icu/question/detail?id=1245
682-1245-答案：
简单题，校招和初中级岗位面试中常见。
前置知识：

使用共享锁（S 锁）可以提高读取操作的并发性，但需要仔细考虑以下几个关键点，以避免潜在问题并确保系统的稳定性和性能：


总结
在使用共享锁（S 锁）时，为了提高读取操作的并发性，同时又要确保系统的稳定性和性能，需要注意以下几个方面：

首先是锁的兼容性与冲突管理。基本原则是，共享锁之间是兼容的，也就是说多个事务可以同时持有同一资源的共享锁，进行并发读取。但是，共享锁与排他锁是互斥的，这意味着写操作（需要排他锁）必须等待所有共享锁释放。所以，需要注意避免长时间持有共享锁，因为长时间持有会阻塞写操作，降低系统的吞吐量。

其次是死锁风险与预防。一个典型的死锁场景是，事务 A 持有资源 1 的共享锁，同时等待资源 2 的共享锁；而事务 B 持有资源 2 的共享锁，同时等待资源 1 的共享锁。另一种情况是，事务 A 和 B 都持有共享锁，并且都尝试将共享锁升级为排他锁。为了预防死锁，可以采取固定顺序、避免锁升级、设置锁等待超时、死锁检测等措施。

------------------------------

683. *使用排他锁时需要注意些什么？, page url: https://www.mianshi.icu/question/detail?id=1246
683-1246-答案：
简单题，校招和初中级岗位面试中常见。
前置知识：

使用排他锁时需要以下几点：


总结
在使用排他锁时，有几个关键点需要特别注意：

首先是缩短持锁时间。因为排他锁会阻塞其他事务的读写操作，如果长时间持有锁，会导致严重的并发性能下降。所以，只在必要的时候才加锁，并且在完成关键的更新操作后，尽快提交或者回滚事务。同时，应该将耗时或者非数据库的操作，比如外部 API 调用、复杂的计算，移到事务的外部，避免锁被长时间占用。

其次是死锁风险与预防。一个典型的死锁场景是，两个事务相互持有对方需要的资源的锁，导致持续等待，无法继续执行。为了预防死锁，可以采取固定加锁顺序、设置锁超时、一次性锁定等措施。

------------------------------

684. *共享锁和排他锁的区别是什么？, page url: https://www.mianshi.icu/question/detail?id=1247
684-1247-答案：
简单题，校招和初中级岗位面试中常见。
前置知识：

共享锁和排他锁是数据库用于并发控制的两种基本锁类型，它们在多个方面存在显著差异：
共享锁和排他锁主要有以下区别：

首先是核心目的不同。共享锁（S 锁）允许多个事务同时读取同一数据，主要目的是保证读取数据的一致性，防止读取到未提交的脏数据（Dirty Read）。排他锁（X 锁）则是确保同一时间只有一个事务能够对数据进行写入（修改、删除或插入），主要目的是保证数据修改的独占性，防止并发写入导致的数据不一致。

其次是锁的兼容性不同。共享锁（S 锁）与其他共享锁（S 锁）兼容，也就是说，多个事务可以同时持有同一数据的共享锁，实现并发读取。但是，共享锁与排他锁（X 锁）互斥，如果一个事务持有某数据的共享锁，其他事务就无法获取该数据的排他锁，必须等待共享锁释放。排他锁（X 锁）与任何锁都互斥，一个事务持有排他锁时，其他事务无法获取该数据的任何锁（包括共享锁和排他锁），必须等待排他锁释放。

------------------------------

685. 什么是死锁？怎么解决？, page url: https://www.mianshi.icu/question/detail?id=1248
685-1248-答案：
简单题，校招和初中级岗位面试中常见。
死锁（Deadlock）是指多个事务或进程之间相互等待对方持有的资源而无法继续执行，最终造成所有参与方都陷入无限等待的一种僵局状态。在数据库并发控制中，死锁通常发生在不同事务对同一数据资源进行加锁操作但顺序不一致的情况下。

死锁需要同时满足以下四个条件：


导致死锁的场景：
死锁是指多个事务或者进程之间相互等待对方持有的资源，而导致无法继续执行，最终造成所有参与方都陷入无限等待的一种僵局状态。在数据库并发控制中，死锁通常发生在不同事务对同一数据资源进行加锁操作，但是顺序不一致的情况下。

可以通过以下几个方面来降低死锁发生的概率，并且减少影响范围：

首先是事前预防，包括固定加锁顺序、减少锁持有时间、合理设置锁粒度和事务隔离级别等来破坏死锁必备的四个条件（互斥条件、持有并等待、不可剥夺和循环等待）。

------------------------------

686. *常见的数据库死锁场景有哪些？如何应对？, page url: https://www.mianshi.icu/question/detail?id=1249
686-1249-答案：
简单题，校招和初中级岗位面试中常见。
前置知识：

数据库死锁本质在于多个事务同时竞争数据资源且出现循环等待。一旦陷入死锁，所有相关事务都将无法继续。常见数据库死锁场景如下：


除了针对特定场景的策略外，还有一些通用的应对死锁的策略：
数据库死锁的本质是多个事务同时竞争数据资源，并且出现了循环等待。一旦陷入死锁，所有相关的事务都无法继续执行。常见的数据库死锁场景有以下几种：

首先是交叉更新导致的死锁，这是一种经典的资源抢占情况。两个或者多个事务各自持有部分资源，通常是排他锁，同时又在请求对方已经持有的资源，形成了循环等待。 应对这种死锁的策略有：固定加锁顺序、设置锁超时和一次性获取所有资源等。

其次是共享锁升级为排他锁导致的死锁。多个事务先以共享锁（S 锁）读取同一数据，随后几乎同时尝试将共享锁升级为排他锁（X 锁）进行修改，因为 S 锁与 X 锁是互斥的，彼此等待对方释放 S 锁，形成死锁。应对这种死锁的策略是，如果最终需要修改数据，直接使用 SELECT ... FOR UPDATE 获取排他锁，避免先共享锁再升级排他锁，减少锁竞争。如果需要修改多行数据，尽量一次性锁定所有需要修改的行，减少锁竞争。

------------------------------

687. *导致数据库死锁的原因有哪些？如何解决？, page url: https://www.mianshi.icu/question/detail?id=1250
687-1250-答案：
简单题，校招和初中级岗位面试中常见。
常见的数据库死锁场景有哪些？如何应对？（mianshi.icu）的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
数据库死锁的本质是多个事务同时竞争数据资源，并且出现了循环等待。一旦陷入死锁，所有相关的事务都无法继续执行。常见的数据库死锁场景有以下几种：

首先是交叉更新导致的死锁，这是一种经典的资源抢占情况。两个或者多个事务各自持有部分资源，通常是排他锁，同时又在请求对方已经持有的资源，形成了循环等待。 应对这种死锁的策略有：固定加锁顺序、设置锁超时和一次性获取所有资源等。

其次是共享锁升级为排他锁导致的死锁。多个事务先以共享锁（S 锁）读取同一数据，随后几乎同时尝试将共享锁升级为排他锁（X 锁）进行修改，因为 S 锁与 X 锁是互斥的，彼此等待对方释放 S 锁，形成死锁。应对这种死锁的策略是，如果最终需要修改数据，直接使用 SELECT ... FOR UPDATE 获取排他锁，避免先共享锁再升级排他锁，减少锁竞争。如果需要修改多行数据，尽量一次性锁定所有需要修改的行，减少锁竞争。

------------------------------

688. *什么是间隙锁？, page url: https://www.mianshi.icu/question/detail?id=1251
688-1251-答案：
简单题，校招和初中级岗位面试中常见。
间隙锁是 InnoDB 在可重复读（REPEATABLE READ）隔离级别下，为防止幻读（Phantom Read）而引入的一种特殊锁机制。它锁定的对象不是具体的某条记录本身，而是记录之间的“间隙”（如索引值 10 和 15 之间的区间），从而阻止其他事务在该区间内插入新数据。

工作原理：

假设表 products 中已有索引 ID=5、10、15：
间隙锁是 InnoDB 存储引擎在可重复读（REPEATABLE READ）这个隔离级别下，为了防止幻读（Phantom Read）而引入的一种特殊的锁机制。它锁定的对象不是具体的某一条记录，而是记录之间的“间隙”，比如说索引值 10 和 15 之间的这个区间，从而阻止其他事务在这个区间内插入新的数据。

------------------------------

689. *什么是临键锁？, page url: https://www.mianshi.icu/question/detail?id=1252
689-1252-答案：
简单题，校招和初中级岗位面试中常见。
临键锁（Next-Key Lock）是由“记录锁（Record Lock）+ 间隙锁（Gap Lock）”组合而成的一种锁机制，主要用于在 MySQL InnoDB 的可重复读（REPEATABLE READ）隔离级别下防止“幻读”（Phantom Read）。当对某条索引记录加锁时，InnoDB 不仅锁住该记录本身，也会锁定前后的“间隙”，确保在事务进行期间，其他事务无法在这个区间内插入新数据从而导致读到“幻影”。

工作原理:

记录锁，锁定索引本身，使该记录无法被其他事务修改或删除。间隙锁，锁定索引记录之前或之后的间隙，阻止其他事务在这个间隙内插入新记录。临键锁则是将上述两种锁合并，对当前索引记录及其相邻区间形成“(a, b]”或“[a, b)”这样的闭合-开区间进行锁定。这样一来，既能防止对已存在记录的修改，又能阻止在相邻间隙内插入新数据。举个例子，假设表里已有索引值 5、10、15：如果在索引值为10的记录上加临键锁，则实际会锁定 (5,10]；在事务提交前，其他事务无法在区间 (5,10) 内插入新记录，也无法修改或删除10这条记录本身。
临键锁（Next-Key Lock）是由“记录锁（Record Lock）”加上“间隙锁（Gap Lock）”组合而成的一种锁机制，主要用在 MySQL InnoDB 的可重复读（REPEATABLE READ）隔离级别下，目的是防止“幻读”（Phantom Read）。当对某一条索引记录加锁时，InnoDB 不仅会锁住这条记录本身，还会锁定它前后的“间隙”，确保在事务进行期间，其他事务无法在这个区间内插入新的数据，从而避免产生幻读。但同时，它也会扩大锁的范围，并且带来一定的并发性能开销。在实际使用中，需要结合业务的需求，精细化地设计索引、控制事务的粒度，并且正确地选择事务隔离级别，从而在数据的一致性和并发的效率之间取得一个平衡。

------------------------------

690. *如何优化数据库？, page url: https://www.mianshi.icu/question/detail?id=1253
690-1253-答案：
简单题，校招和初中级岗位面试中常见。
你通常从哪些方面来优化数据库？（mianshi.icu）的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
通常从以下几个方面来对数据库进行优化：

首先是整体的架构设计。比如可以考虑读写分离，把主库专门用于写操作，从库处理读操作，这样可以分担数据库的压力。还可以考虑分库分表。水平拆分，就是按照用户 ID 或者时间等维度，把数据分散到不同的数据库或者表里面，解决大表查询以及超大数据量存储的问题。或者引入缓存层。在应用层集成 Redis、Memcached 等缓存，缓存热点数据，减少数据库的访问频率。对于静态资源或者大流量的访问点，尽量使用 CDN、负载均衡等手段。

其次是数据库的表结构。比如字段的设计要精简。选择合适的数据类型，比如用 INT 来代替 VARCHAR 存储数值字段，减少存储空间和索引的开销。尽量避免使用 NULL 列，或者设定默认值，以减少索引和统计信息的复杂度。范式和反范式要结合起来。采用第三范式可以减少冗余，保证数据的一致性。如果读多写少，为了提升查询的性能，可以在关键点做少量的冗余，避免过多的 JOIN 操作。

------------------------------

691. *你通常从哪些方面来优化数据库？, page url: https://www.mianshi.icu/question/detail?id=1254
691-1254-答案：
简单题，校招和初中级岗位面试中常见。
可以从以下几个方面来优化数据库：
通常从以下几个方面来对数据库进行优化：

首先是整体的架构设计。比如可以考虑读写分离，把主库专门用于写操作，从库处理读操作，这样可以分担数据库的压力。还可以考虑分库分表。水平拆分，就是按照用户 ID 或者时间等维度，把数据分散到不同的数据库或者表里面，解决大表查询以及超大数据量存储的问题。或者引入缓存层。在应用层集成 Redis、Memcached 等缓存，缓存热点数据，减少数据库的访问频率。对于静态资源或者大流量的访问点，尽量使用 CDN、负载均衡等手段。

其次是数据库的表结构。比如字段的设计要精简。选择合适的数据类型，比如用 INT 来代替 VARCHAR 存储数值字段，减少存储空间和索引的开销。尽量避免使用 NULL 列，或者设定默认值，以减少索引和统计信息的复杂度。范式化和反范式化要结合起来。采用三大范式可以减少冗余，保证数据的一致性。如果读多写少，为了提升查询的性能，可以在关键点做少量的冗余，避免过多的 JOIN 操作。

------------------------------

692. *你优化SQL的大致流程是怎样的？, page url: https://www.mianshi.icu/question/detail?id=1255
692-1255-答案：
简单题，校招和初中级岗位面试中常见。
如何优化SQL？（mianshi.icu）的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
SQL 优化的过程，通常可以分为五个主要的阶段：“发现问题”、“分析瓶颈”、“优化实施”、“验证效果”和“持续迭代”。

首先是“发现问题”，可以通过分析慢查询日志、实时监控（ SHOW PROCESSLIST /Prometheus、Grafana ）或者业务层的反馈来发现问题。

接下来是“分析瓶颈”，可以通过分析慢查询日志、查看SQ 语句的执行计划（EXPLAIN）和SQL Profiling（MySQL中使用SHOW PROFILE 或者 SHOW PROFILES）查看 SQL 语句在各个阶段的详细耗时。如果出现死锁或者大量的锁等待，可以通过 SHOW ENGINE INNODB STATUS 发现问题。

------------------------------

693. *如何优化SQL？, page url: https://www.mianshi.icu/question/detail?id=1256
693-1256-答案：
简单题，校招和初中级岗位面试中常见。
SQL 优化通常可以分为「发现问题 → 分析瓶颈 → 优化实施 → 验证效果→ 持续迭代」这五大阶段。整体流程大概如下：
SQL 优化的过程，通常可以分为五个主要的阶段：“发现问题”、“分析瓶颈”、“优化实施”、“验证效果”和“持续迭代”。

首先是“发现问题”，可以通过分析慢查询日志、实时监控（ SHOW PROCESSLIST /Prometheus、Grafana ）或者业务层的反馈来发现问题。

接下来是“分析瓶颈”，可以通过分析慢查询日志、查看SQL语句的执行计划（EXPLAIN）和SQL Profiling（MySQL中使用SHOW PROFILE 或者 SHOW PROFILES）查看 SQL 语句在各个阶段的详细耗时。如果出现死锁或者大量的锁等待，可以通过 SHOW ENGINE INNODB STATUS 发现问题。

------------------------------

694. *Kafka为什么性能高？, page url: https://www.mianshi.icu/question/detail?id=1257
694-1257-答案：
略难的题，高频面试题。Kafka 的设计者使用了非常多的技巧来提高 Kafka 的性能，而这些性能优化手段互相之间也没太多逻辑上的联系，所以你记忆起来会比较困难。

但是从面试的角度来说，这个问题又是一个非常好的打开话题，引导面试方向的问题。所以你在回答这个问题的时候，就要尽量提及你记得的所有的能够提高 Kafka 性能的点，而后等待面试官进一步追问。

Kakfa 是如何做到高性能的？（mianshi.icu）的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
Kafka 使用了非常的技巧来优化性能，主要有：
当然，这些性能优化手段也并不仅仅是只有 Kafka 才会使用。严格来说，大部分的跟网络 IO，磁盘 IO 有关的中间件都会使用这些技术。

例如说在 MySQL 中，也大量利用了顺序写的特性。比如说我们熟悉的 MVCC 机制使用的 undo log 和 redo log，都是一种 WAL，也就是顺序写的日志。再比如说在 Redis 使用 AOF 也是充分利用了顺序写的高性能。

------------------------------

695. *介绍一下Kafka的零拷贝技术, page url: https://www.mianshi.icu/question/detail?id=1258
695-1258-答案：
基础题。如果你是科班出身，那么你在学习操作系统的时候几乎肯定会学习到这个概念。

要想回答好这个问题，一定要讲清楚零拷贝的原理，而后引申到零拷贝在不同中间件中的应用。
前置知识：
零拷贝是一种优化数据传输的技术，核心思想是尽量避免数据在内存之间的拷贝，并且做到 CPU 完全不参与拷贝，这也就是零拷贝的零的含义。

（用一个例子来解释）举个例子来说，如果要从文件里面读取数据并且发送到网络，那么在没有零拷贝技术的时候过程是：

那么这个过程的效率是比较低的。那么零拷贝技术就是规避了 2 和 3 的两次拷贝，简化成了：

------------------------------

696. *用过消息队列吗？用于解决什么问题？, page url: https://www.mianshi.icu/question/detail?id=1259
696-1259-答案：
简答题，校招和初级岗位面试中常见。
消息队列是构建分布式系统、解决特定场景问题的关键组件，主要用来解决以下问题：


不同 MQ 的选型考量：


吞吐量需求：日志、大数据场景优先Kafka。
是的，我在多个项目中都用过消息队列，主要用来解决下面几个问题：

首先是应用解耦。在传统的紧耦合架构中，如果系统 A 直接调用系统 B 的接口，一旦 B 出现故障或者变更，A 也会受到影响。引入消息队列作为“中间层”，A 只负责将消息发送到消息队列（MQ），B 从 MQ 订阅并消费。这样一来，A 和 B 之间不再直接依赖，各自可以独立地发展和维护。举个例子，订单系统在订单创建之后，只需要向 MQ 发送一条“订单已创建”的消息，而库存、物流、积分等下游系统各自订阅并处理这条消息，不需要订单系统与它们逐一进行同步调用。

其次是异步处理。有些业务流程耗时比较长，比如发送邮件、生成报表，如果同步执行，会导致用户请求的响应时间过长，影响用户体验。将耗时操作封装成消息发送到 MQ，主流程快速返回，由消费者异步处理这些消息。比如，用户注册成功后，主流程只需要将“发送欢迎邮件”消息放入 MQ，然后立即响应用户；邮件服务作为消费者，异步执行邮件发送，不会阻塞主流程。

------------------------------

697. *Kafka、RabbitMQ、RocketMQ之间如何选型？, page url: https://www.mianshi.icu/question/detail?id=1260
697-1260-答案：
简答题，校招和初级岗位面试中常见。
前置知识：
在选择不同的消息队列（MQ）时，需要考虑以下几个方面：

首先是吞吐量的极限。Kafka 可以达到百万级 TPS（每秒事务数），这得益于它的批处理和零拷贝机制。RabbitMQ 的吞吐量在万级 TPS，因为它主要基于内存队列。RocketMQ 的吞吐量在十万级 TPS，这是因为它采用了分布式架构优化。

其次是消息架构模型。Kafka 采用发布-订阅模式，基于主题（Topic）和分区（Partition）。RabbitMQ 提供了多种灵活的路由方式，比如直连、主题、扇出等，通过交换机（Exchange）和队列（Queue）的绑定来实现。RocketMQ 也是发布-订阅模式，但它还支持消息标签过滤。

------------------------------

698. *Kafka 的核心组件有哪些？它们的作用分别是什么？, page url: https://www.mianshi.icu/question/detail?id=1261
698-1261-答案：
简答题，校招和初级岗位面试中常见。
Kafka 作为一个高吞吐、分布式的消息系统，其核心组件的协同工作至关重要。以下是 Kafka 的主要组件及其作用的详细介绍：


组件协作流程：



总结
Kafka 是一个高吞吐、分布式的消息系统，它的核心组件及各自的作用如下：

首先是 Broker（中间人）。Broker 是 Kafka 集群中的单个服务器节点，每个 Broker 都是一个独立的 Kafka 实例。Broker 主要负责消息的存取，它会将接收到的消息持久化存储到磁盘上，并且使用顺序写入和零拷贝技术来优化 I/O 性能。

然后是 Controller。Controller 是 Kafka 集群中一个特殊的 Broker，它负责管理整个集群的元数据。Controller 会监控 Broker 的状态，管理分区和副本的状态，执行分区重新分配，以及处理 Broker 的上线和下线。

------------------------------

699. *Kafka的基本架构是什么？, page url: https://www.mianshi.icu/question/detail?id=1262
699-1262-答案：
简答题，校招和初级岗位面试中常见。
Kafka 的核心组件有哪些？它们的作用分别是什么？（mianshi.icu）的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
Kafka 是一个高吞吐、分布式的消息系统，它的基本架构如下：

首先是 Broker（中间人）集群。每个 Broker 都是单个服务器节点即一个独立的 Kafka 实例。Broker 主要负责消息的存取，它会将接收到的消息持久化存储到磁盘上，并且使用顺序写入和零拷贝技术来优化 I/O 性能。

然后是 Controller。Controller 是 Kafka 集群中一个特殊的 Broker，它负责管理整个集群的元数据。Controller 会监控 Broker 的状态，管理分区和副本的状态，执行分区重新分配，以及处理 Broker 的上线和下线。

------------------------------

700. *介绍一下Kafka中的主题、分区、生产者和消费者, page url: https://www.mianshi.icu/question/detail?id=1263
700-1263-答案：
简答题，校招和初级岗位面试中常见。
前置知识：
Topic（主题）是消息的逻辑分类，用于区分不同类型的消息。一个 Topic 可以有多个分区（Partition），这样可以实现消息的并行处理和水平扩展。

Partition（分区）是 Topic 的物理划分，每个分区是一个有序的、不可变的消息日志。分区的主要作用是实现消息的并行处理和水平扩展，它是 Kafka 并行处理的基本单位。分区内的消息按照写入的顺序严格有序，每个消息在分区内有一个唯一的偏移量（Offset），用于标识消息的位置。

------------------------------

701. *Kafka 的作用是什么？, page url: https://www.mianshi.icu/question/detail?id=1264
701-1264-答案：
简答题，校招和初级岗位面试中常见。
用过消息队列吗？用于解决什么问题？（mianshi.icu）的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
Kafka 主要有以下作用：

首先是应用解耦。在传统的紧耦合架构中，如果系统 A 直接调用系统 B 的接口，一旦 B 出现故障或者变更，A 也会受到影响。引入Kafka作为“中间层”，A 只负责将消息发送到Kafka，B 从 Kafka 订阅并消费。这样一来，A 和 B 之间不再直接依赖，各自可以独立地发展和维护。举个例子，订单系统在订单创建之后，只需要向 Kafka 发送一条“订单已创建”的消息，而库存、物流、积分等下游系统各自订阅并处理这条消息，不需要订单系统与它们逐一进行同步调用。

其次是异步处理。有些业务流程耗时比较长，比如发送邮件、生成报表，如果同步执行，会导致用户请求的响应时间过长，影响用户体验。将耗时操作封装成消息发送到 Kafka，主流程快速返回，由消费者异步处理这些消息。比如，用户注册成功后，主流程只需要将“发送欢迎邮件”消息放入 Kafka，然后立即响应用户；邮件服务作为消费者，异步执行邮件发送，不会阻塞主流程。

------------------------------

702. *Kafka如何实现主从同步？, page url: https://www.mianshi.icu/question/detail?id=1265
702-1265-答案：
简答题，校招和初中级岗位面试中常见。
前置知识：

Kafka 通过多副本机制和 ISR（In-Sync Replicas）来实现主从同步，这是确保数据高可用性和可靠性的关键。

Kafka 的副本机制：
Kafka 通过多副本机制和 ISR（In-Sync Replicas）来实现主从同步，这是确保数据高可用性和可靠性的关键。

先说一下 Kafka 的副本机制。Kafka中副本有两种类型，Leader 副本和 Follower 副本。

每个分区（Partition）有且仅有一个 Leader 副本，它负责处理所有来自客户端的读写请求，维护着分区数据的最新状态，同时维护 ISR （同步副本集合）集合，跟踪哪些副本与自己保持同步。

------------------------------

703. *Kafka的分区策略有哪些？, page url: https://www.mianshi.icu/question/detail?id=1266
703-1266-答案：
简答题，校招和初中级岗位面试中常见。
Kafka 的分区策略决定了生产者将消息发送到哪个分区。选择合适的分区策略对于消息的顺序性、负载均衡、以及整体系统性能至关重要。Kafka 提供了以下几种分区策略：



选择合适的分区策略需要综合考虑以下几个方面：
Kafka 提供了以下几种分区策略：

首先是轮询策略（Round-Robin）。这是最常用的默认策略，尤其是在消息没有指定 Key 的情况下。消息会依次、轮流地分配到 Topic 的所有可用分区。比如，第一条消息发送到分区 0，第二条发送到分区 1，以此类推，循环往复。这种策略的优点是简单高效，能够实现良好的负载均衡。缺点是无法保证具有相同 Key 的消息被发送到同一个分区，也就是说，无法保证 Key 级别的消息顺序性。

然后是按键哈希策略（Key-Based Hashing）。当消息指定了 Key 时，通常会使用这种策略。Kafka 会对消息的 Key 进行哈希计算（一般使用 MurmurHash2 算法）。然后，根据哈希值与分区总数取模，来确定目标分区。这样可以保证具有相同 Key 的消息总是被发送到同一个分区。这种策略的优点是保证了 Key 级别的消息顺序性，也就是说，同一 Key 的消息在分区内是有序的。缺点是，如果 Key 的分布不均匀（例如，某些 Key 对应的消息量远大于其他 Key），可能会导致数据倾斜，也就是某些分区的数据量过大。

------------------------------

704. *Kafka如何保证消息的有序性？, page url: https://www.mianshi.icu/question/detail?id=1267
704-1267-答案：
简答题，校招和初级岗位面试中常见。
Kafka 主要通过分区级别的顺序性来保证消息的有序性。虽然 Kafka 本身不提供跨分区的全局有序性，但可以通过一些策略和配置来实现特定需求下的有序性。


生产者端保证顺序的关键配置：


消费者端保证顺序的关键：
Kafka 主要通过分区级别的顺序性来保证消息的有序性。虽然 Kafka 本身不提供跨分区的全局有序性，但可以通过一些策略和配置来实现特定需求下的有序性。

先说分区内的有序性，Kafka 保证单个分区内的消息严格按照它们被写入的顺序进行消费。每个分区都是一个有序的、不可变的消息序列，消息会被分配一个连续递增的偏移量（Offset）。Leader 副本接收消息并将其追加到本地日志文件，消费者严格按照偏移量的顺序来消费消息。

Kafka 是不保证跨分区的消息顺序的，主要是因为不同的分区可能分布在不同的 Broker 上，多个分区可能有不同的处理延迟，以及消费者可能会并行地处理多个分区的消息。

------------------------------

705. *Kafka 是如何保证消息顺序的？, page url: https://www.mianshi.icu/question/detail?id=1268
705-1268-答案：
简单的题，高频考题。之所以说这个是简单的题，是因为问得太多以至于差不多人人都会了。
Kafka如何保证消息的有序性？（mianshi.icu）的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
Kafka 主要通过分区级别的顺序性来保证消息的顺序性。虽然 Kafka 本身不提供跨分区的全局有序性，但可以通过一些策略和配置来实现特定需求下的顺序性。

先说分区内的顺序性，Kafka 保证单个分区内的消息严格按照它们被写入的顺序进行消费。每个分区都是一个有序的、不可变的消息序列，消息会被分配一个连续递增的偏移量（Offset）。Leader 副本接收消息并将其追加到本地日志文件，消费者严格按照偏移量的顺序来消费消息。

Kafka 是不保证跨分区的消息顺序的，主要是因为不同的分区可能分布在不同的 Broker 上，多个分区可能有不同的处理延迟，以及消费者可能会并行地处理多个分区的消息。

------------------------------

706. *Kafka 是如何实现消息的高吞吐量的？, page url: https://www.mianshi.icu/question/detail?id=1269
706-1269-答案：
简答题，校招和初中级岗位面试中常见。
Kafka 通过多维度协同优化实现高吞吐，主要包含以下几个方面：
Kafka 能够实现这么高的吞吐量，主要是因为它采用了多维度协同优化的方式。

首先，Kafka 利用分布式并行架构，把每个 Topic 划分成多个分区，这样就能实现并行处理。无论是生产者写入数据或是消费者拉取数据，都能在多个实例上同时进行，而且集群还能自动均衡负载，让分区均匀分布到不同节点上。

其次，在存储层方面，Kafka 采取了顺序磁盘 I/O 的方式，利用日志追加写入模式，把原本的随机写转成顺序写，从而实现单磁盘高吞吐。另外，每个分区被拆分成固定大小的日志段（比如 1GB），只有活跃段是可写的，而旧段保持只读状态，再加上操作系统的页缓存预加载，这都进一步提高了读写效率。此外，Kafka 还使用了零拷贝技术，通过 sendfile() 调用，将数据直接从磁盘经过内核缓冲区传输到网卡，跳过用户空间的拷贝，这样不仅 降低了CPU 消耗，也提升了网络吞吐。

------------------------------

707. *Kafka如何处理重复消息？, page url: https://www.mianshi.icu/question/detail?id=1270
707-1270-答案：
简答题，校招和初中级岗位面试中常见。
Kafka 对重复消息问题采取了端到端的多层防护措施，将生产者、Broker 与消费者各自的去重机制有机结合，从而实现了“仅处理一次”的效果。具体来说，可归纳为以下几个方面：
Kafka 解决重复消息问题是从生产者、Broker 到消费者这几个环节都构建了一道防线，确保业务系统最终只处理一次消息。

在生产者这一端，Kafka 从 0.11 版本就引入了幂等生产者。简单来说，每条消息在发送时都会附带上唯一的 ProducerID 和递增序列号，Broker 就可以利用这三个因素（ProducerID、分区、序列号）判断消息是否重复。如果是重复的，Broker 会自动丢弃。对于一些需要跨分区或者跨 Topic 的场景，Kafka 还提供了事务性生产者，能将多条消息写在同一个事务里，要么一起提交，要么全部撤销。这样不仅保证了消息的一致性，还能把消费者偏移量的提交和消息写入做到原子操作，从根本上降低由于重试带来的重复风险。

在 Broker 端，Kafka 会根据生产者发送的唯一序列号进行过滤，确保即便是生产者因为网络问题触发了重试，重复的消息也不会被重复写入。而在事务场景中，内部的事务协调器会跟踪事务的状态，只允许那些已经提交的事务消息被消费者读取，这样一来就能有效阻止重复消息的扩散。

------------------------------

708. *如何处理Kafka中消息堆积？, page url: https://www.mianshi.icu/question/detail?id=1271
708-1271-答案：
略难的题，高频题目。

大部分人就只会回答异步消费什么的，但是这个方案属实有点烂大街了，毫无竞争力。你需要的做的是先区分是临时积压还是永久性积压。

回答这个问题的关键有两个，一个是要把你能记得的各种方案都讲一下；另外一个关键点是你要把话题引导过去顺序消息的消息积压解决方案上。
Kafka 中消息积压了怎么办？（mianshi.icu）的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
解决消息积压的方案有很多。

首先，如果要是消费者的数量还没达到分区的数量，那么就可以直接增加消费者。这也是最简单，不需要任何改动的方法。

其次，如果要是消费者数量已经达到最大值了，但是消息依旧积压，那么这个时候可以考虑增加分区。如果增加分区不可行的话，那么就创建一个更多分区的新 Topic，把生产者流量切过去，再部署更多消费者。
降级的这个思路，还有一些变种做法。
还有一些涉及比较大的改造的解决方案。

第一种是在消费者这端批量消费，批量提交。比如说一次性拉 100 条消息，将数据合并在一起批量处理；

第二种是消费者这端异步处理，批量提交。一般没有批量处理接口的时候，就可以考虑使用这种策略；

------------------------------

709. *Kafka如何保证消息的可靠性传输？, page url: https://www.mianshi.icu/question/detail?id=1272
709-1272-答案：
简答题，校招和初级岗位面试中常见。
Kafka 保证消息的可靠性传输依赖于生产者、Broker 和消费者各个环节的协同机制，通过合理配置和业务层处理来构建一条完整的安全传输链。
Kafka 能确保消息的可靠性传输主要是依靠生产者、Broker 和消费者之间的协同机制，再加上合理的配置和业务处理，构建了一条完整的安全传输链。

在生产者这一端，Kafka 提供了确认机制，也就是 acks 参数，比如不等待 Broker 确认、只等待 Leader 将消息写入日志后确认和等待所有同步副本写入成功后才确认。除此之外，在遇到网络波动或 Broker 暂时故障的时候，生产者还会通过重试机制，重新发送消息。为了防止重试导致重复写入，Kafka 还引入了幂等生产者，每条消息都会附带唯一的 ProducerID 和递增的序列号，从而 Broker 能判断并丢弃重复消息。如果遇到跨分区或跨 Topic 的场景，还可以配置事务性生产者，实现多条消息的原子写入。

转到 Broker 这一端，Kafka 利用副本复制机制来保证数据的持久性。每个 Topic 的分区都有一个 Leader 和多个 Follower，通过 ISR（In-Sync Replicas）集合来确保只有跟 Leader 保持同步的副本才能确认写入。当生产者选择了 acks=all 的情况下，只有所有 ISR 副本都写入成功后才会返回确认。

------------------------------

710. *Kafka是如何保证消息不丢失的？, page url: https://www.mianshi.icu/question/detail?id=1273
710-1273-答案：
简答题，校招和初级岗位面试中常见。
Kafka如何保证消息的可靠性传输？（mianshi.icu）的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
Kafka 能确保消息不丢失主要是依靠生产者、Broker 和消费者之间的协同机制，再加上合理的配置和业务处理，构建了一条完整的安全传输链。

在生产者这一端，Kafka 提供了确认机制，也就是 acks 参数，比如不等待 Broker 确认、只等待 Leader 将消息写入日志后确认和等待所有同步副本写入成功后才确认。除此之外，在遇到网络波动或 Broker 暂时故障的时候，生产者还会通过重试机制，重新发送消息。为了防止重试导致重复写入，Kafka 还引入了幂等生产者，每条消息都会附带唯一的 ProducerID 和递增的序列号，从而 Broker 能判断并丢弃重复消息。如果遇到跨分区或跨 Topic 的场景，还可以配置事务性生产者，实现多条消息的原子写入。

转到 Broker 这一端，Kafka 利用副本复制机制来保证数据的持久性。每个 Topic 的分区都有一个 Leader 和多个 Follower，通过 ISR（In-Sync Replicas）集合来确保只有跟 Leader 保持同步的副本才能确认写入。当生产者选择了 acks=all 的情况下，只有所有 ISR 副本都写入成功后才会返回确认。

------------------------------

711. *Kafka中的ISR（In-Sync Replica）指的是什么？, page url: https://www.mianshi.icu/question/detail?id=1274
711-1274-答案：
简答题，校招和初中级岗位面试中常见。

在 Kafka 中，ISR（In-Sync Replicas，同步副本集合）是指与 Leader 副本保持数据同步的 Follower 副本集合，这个集合也包括 Leader 副本本身。默认情况下，只有 ISR 中的副本才有资格参与 Leader 选举。ISR 是 Kafka 副本机制的核心组成部分，对于保障数据一致性、实现高可用性以及系统容错至关重要。

ISR 的动态维护：


ISR 的作用：
在 Kafka 中，ISR（In-Sync Replicas，同步副本集合）指的是与 Leader 副本保持数据同步的 Follower 副本的集合，这个集合也包括 Leader 副本本身。默认情况下，只有 ISR 中的副本才有资格参与 Leader 选举。

ISR 是动态维护的，维护过程大致如下：

新创建的 Follower 副本，或者从故障中恢复的 Follower 副本，会从 Leader 副本拉取数据。当 Follower 副本的 LEO（Log End Offset，日志末端偏移量）追上了 Leader 副本的 LEO，也就是说，两者之间的差距小于 replica.lag.time.max.ms 这个参数的值（默认是 30 秒）时，这个 Follower 副本就会被加入到 ISR 中。

------------------------------

712. *Kakfa中的消费者组是什么？它如何实现消息的负载均衡？, page url: https://www.mianshi.icu/question/detail?id=1275
712-1275-答案：
简答题，校招和初中级岗位面试中常见。
前置知识：


Kafka 中的消费者组是指由共享相同 group.id 的多个消费者实例构成的逻辑整体。消费者组不仅负责实现消息的并行消费，还通过分区分配与动态再均衡机制实现了高效的负载均衡。负载均衡实现原理如下：
Kafka 的消费者组可以说是实现消息并行消费、负载均衡和高可用性的核心机制。消费者组就是由共享相同 group.id 的一个或多个消费者实例组成，组内这些消费者会协同订阅一个或多个主题的消息，但关键是，同一时刻，一个分区只能被组内的一个消费者消费。每个消费者组内的分区进度都只保留一份，并且是由 Kafka 内部的 __consumer_offsets 主题统一管理的。因此，同一个主题可以有很多独立的消费者组，每个组的消费进度都是独立的、互不干扰的，这样就实现了类似广播的效果。

具体来说，它的负载均衡主要通过分区分配机制和再均衡机制来实现：

分区分配机制是指Kafka 通过内置的分区分配器，把主题里的各个分区分配给消费者组内的实例。这里的关键在于，要保证同一个分区在同一时刻只能被一个消费者处理，这样才能保证消息的顺序性。分配的时候，Kafka 会遵循一些基本规则：如果消费者的数量和分区的数量一样，那么每个消费者正好消费一个分区；如果消费者的数量比分区少，那么有些消费者就会被分配到多个分区；反过来，如果消费者的数量比分区多，那么就会有一些消费者闲置。Kafka 提供了好几种分区分配策略，比如 Range、RoundRobin 和 Sticky，目的是尽可能地让负载分配得均匀。

------------------------------

713. *Kakfa 中的消费者组是什么？它的作用是什么？, page url: https://www.mianshi.icu/question/detail?id=1276
713-1276-答案：
简答题，校招和初级岗位面试中常见。

Kafka 中的消费者组是实现消息并行消费、负载均衡和高可用的核心机制。消费者组由共享相同 group.id 的一个或多个消费者实例构成。组内的消费者协同订阅一个或多个主题的消息，但同一时刻，一个分区只能由组内的一个消费者消费。因此，同一主题可以存在多个独立的消费者组，每个组内的消费进度是相互独立且互不干扰的，每个消费者组对消息的处理互为独立，实现了类似广播的效果。

消费者组内的每个分区消费进度只保留一份，并由 Kafka 内部的 __consumer_offsets 主题统一管理。当消费者启动时，会向组协调器（Group Coordinator）发送加入请求，随后协调器根据所选的分配策略（如 Range、RoundRobin 或 Sticky）自动将待消费的分区分配给各个消费者。如果消费者数量或者主题分区数发生变化，消费者组会触发再均衡机制，即在暂停消费、提交当前偏移量后重新分配分区，最终恢复消费。这种动态调整机制确保了系统资源的高效利用、负载均衡以及容错性，即使部分消费者发生故障或退出，也能由其他消费者接替分区消费，保证消息处理的连续性。

主要作用：
Kafka 的消费者组可以说是实现消息并行消费、负载均衡和高可用性的核心机制。消费者组就是由共享相同 group.id 的一个或多个消费者实例组成，组内这些消费者会协同订阅一个或多个主题的消息，但关键是，同一时刻，一个分区只能被组内的一个消费者消费。每个消费者组内的分区进度都只保留一份，并且是由 Kafka 内部的 __consumer_offsets 主题统一管理的。因此，同一个主题可以有很多独立的消费者组，每个组的消费进度都是独立的、互不干扰的，这样就实现了类似广播的效果。

消费者组的主要作用，体现在这几个方面：

首先是负载均衡和并行消费。通过消费者组，可以把主题中多个分区自动分配给组内的各个消费者，比如一个有 6 个分区的主题，如果消费者组里有 3 个消费者，每个消费者就会平均分摊 2 个分区。这样做不仅可以充分利用硬件资源，还能提高整个系统的吞吐量。而且，只要分区数量足够，还可以通过增加消费者实例实现水平扩展，从而大大提升消息处理能力。

------------------------------

714. *Kafka中主题（Topic）与分区（Partition）的区别和联系, page url: https://www.mianshi.icu/question/detail?id=1278
714-1278-答案：
简答题，校招和初级岗位面试中常见。
前置知识：

Kafka 中的主题（Topic）和分区（Partition）是消息存储和分发的两个核心概念，它们相互配合实现了高吞吐、高可用和扩展性。
主题其实是 Kafka 中的一个逻辑概念，主要用来将相似类型的消息归类在一起。生产者会把消息发布到特定的主题，然后消费者通过订阅这些主题来获取消息。

分区则是主题在物理层面的拆分单元。每个主题可以有一个或多个分区，而每个分区内部其实就是一个有序的消息队列，消息会按照顺序写入和读取。

------------------------------

715. *Kafka中分区的作用是什么？, page url: https://www.mianshi.icu/question/detail?id=1279
715-1279-答案：
简答题，校招和初级岗位面试中常见。
前置知识：
简单来说，引入分区就是为了分而治之，提高性能。

在 Kafka 里面，Topic 代表的是特定业务，或者说特定的业务场景。假定没有分区，或者说一个 Topic 只有一个分区，那么一个 Topic 就只能存放在一个 Kafka 服务器上。
所以说在考虑创建一个 Topic 的时候，分区数量是一个很重要的权衡点。分区数量过少，容易出现性能瓶颈，撑不住那么高的并发，又或者出现消息积压。当然分区数量也不是越多越好的，因为分区数量过多也会影响到 Kafka 的性能。
当然，类似的设计在别的中间件里面也能见到。比如说同样是消息队列，Kafka 里面叫做分区，在别的消息队列中间件里面就叫做队列，总之都是有类似的结构的。

------------------------------

716. *MySQL支持哪些索引？, page url: https://www.mianshi.icu/question/detail?id=1280
716-1280-答案：
简答题，校招和初中级岗位面试中常见。
MySQL的索引类型有哪些？（mianshi.icu）的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
MySQL数据库支持多种索引类型，以满足不同的查询场景和性能需求：

首先，从逻辑功能来看，常见的有主键索引、唯一索引、普通索引、联合索引、覆盖索引、前缀索引、全文索引、空间索引等。

------------------------------

717. *MySQL的索引类型有哪些？, page url: https://www.mianshi.icu/question/detail?id=1281
717-1281-答案：
简答题，校招和初中级岗位面试中常见。
数据库中索引的种类有哪些？（mianshi.icu）的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
MySQL数据库提供多种索引类型，以满足不同的查询场景和性能需求：

首先，从逻辑功能来看，常见的有主键索引、唯一索引、普通索引、联合索引、覆盖索引、前缀索引、全文索引、空间索引等。

------------------------------

718. *你了解MySQL索引吗？, page url: https://www.mianshi.icu/question/detail?id=1282
718-1282-答案：
简答题，校招和初中级岗位面试中常见。
前置知识：
（总体介绍）MySQL索引是一种数据结构，用于快速查找数据库表中的数据。它类似于书籍的目录，可以帮助我们快速找到需要的信息。

索引的主要作用是加速查找，当执行查询操作时，如果没有索引，就会涉及到全表扫描，每一行都得判断一遍；而有了索引后，MySQL只需要扫描索引里相关的数据，就能直接跳转到符合条件的记录。

索引之所以可以加速查找是因为索引数据结构的有序性可以把查找的时间复杂度降低到 O(log n) 或更低，并且索引文件通常比整个表要小很多，所以所需的磁盘 I/O 操作也大大减少，有时候如果查询的所有字段都已经包含在索引里面，那就可以实现索引覆盖或覆盖索引优化，直接从索引中获取想要的数据，完全避免回表操作。

------------------------------

719. *MySQL的InnoDB引擎支持哪些索引？是如何实现的？, page url: https://www.mianshi.icu/question/detail?id=1283
719-1283-答案：
简答题，校招和初中级岗位面试中常见。
前置知识：

MySQL 的 InnoDB 存储引擎支持多种索引类型，以满足不同的查询和性能需求。这些索引类型主要包括 B+Tree 索引、自适应哈希索引、全文索引和空间索引。
MySQL 的 InnoDB 存储引擎提供了好几种索引类型，主要是为了应对不同的查询需求和性能优化。其中最核心、最常用的就是 B+ 树索引，它也是 InnoDB 默认的索引类型。

B+ 树索引是基于B+树实现的，B+树其实是一种平衡多路搜索树，所有数据都存在叶子节点，非叶子节点只存键值和指针。叶子节点之间用双向链表连起来，所以范围查询效率很高。对于聚簇索引，也就是主键索引，数据行是按主键值顺序存储在叶子节点上的，每张表只能有一个聚簇索引。而非聚簇索引，也就是二级索引，叶子节点存的是主键值，查询时通常需要通过主键值再回表去查完整数据。从实现的角度来说，主键索引、唯一索引、普通索引、联合索引和前缀索引，都是用B+树实现的。

除了 B+ 树索引，InnoDB 还有自适应哈希索引、全文索引和空间索引。

------------------------------

720. *MySQL中聚簇索引和非聚簇索引的区别？, page url: https://www.mianshi.icu/question/detail?id=1284
720-1284-答案：
简答题，校招和初中级岗位面试中常见。
前置知识：
在MySQL中不同的存储引擎对索引的支持是不同的，只有InnoDB存储引擎支持聚簇索引（主键）和非聚簇索引（二级索引）。而MyISAM,、Memory、Archive等存储引擎都不支持聚簇索引，它们的索引都是非聚簇索引。

在InnoDB 存储引擎中聚簇索引和非聚簇索引主要有以下区别：

首先，在存储结构方面，聚簇索引最大的特点是数据和索引是紧密结合在一起的。InnoDB 表的数据是按照主键顺序直接存储在 B+ 树的叶子节点里的，这意味着索引的叶子节点就包含了整行数据。如果表定义了主键，那主键就自动成为聚簇索引。要是没有显式定义主键，InnoDB 会优先选择第一个非空唯一索引，如果连这个也没有，InnoDB 会自己生成一个隐藏的 6 字节 ROW_ID 作为聚簇索引。而且，每张表只能有一个聚簇索引，毕竟数据行只能按一种物理顺序存储。

------------------------------

721. *MySQL InnoDB 引擎中的聚簇索引和非聚簇索引有什么区别？, page url: https://www.mianshi.icu/question/detail?id=1285
721-1285-答案：
简答题，校招和初中级岗位面试中常见。
前置知识：
在MySQL 的 InnoDB 存储引擎中聚簇索引和非聚簇索引主要有以下区别：

首先，在存储结构方面，聚簇索引最大的特点是数据和索引是紧密结合在一起的。InnoDB 表的数据是按照主键顺序直接存储在 B+ 树的叶子节点里的，这意味着索引的叶子节点就包含了整行数据。如果表定义了主键，那主键就自动成为聚簇索引。要是没有显式定义主键，InnoDB 会优先选择第一个非空唯一索引，如果连这个也没有，InnoDB 会自己生成一个隐藏的 6 字节 ROW_ID 作为聚簇索引。而且，每张表只能有一个聚簇索引，毕竟数据行只能按一种物理顺序存储。

而非聚簇索引，也叫二级索引，它的索引和数据是分开的。非聚簇索引的 B+ 树叶子节点存的是索引列的值和对应的主键值，而不是整行数据。一张表可以创建多个非聚簇索引，比如普通索引、唯一索引、联合索引等等。

------------------------------

722. *在 MySQL 中建索引时需要注意哪些事项？, page url: https://www.mianshi.icu/question/detail?id=1286
722-1286-答案：
简答题，校招和初中级岗位面试中常见。

在 MySQL 中，索引是优化查询性能的关键手段。然而，索引并非越多越好，不合理的索引设计反而可能导致性能下降。因此，创建索引时需综合考虑查询需求、数据分布、存储成本以及维护开销等多方面因素。
MySQL 中的索引对于优化查询性能至关重要，但索引并不是越多越好。不合理的索引设计反而可能让性能更差。所以，创建索引时，会综合考虑以下几个方面：

首先，明确查询场景，优先给 WHERE、JOIN、ORDER BY、GROUP BY 子句里经常用到的列创建索引。尽可能建立联合索引，这样可以减少索引数量。并且使联合索引满足最左匹配原则，能够成为覆盖索引更好。对于不常查询的列，避免创建索引，减少不必要的存储和维护开销。

其次，评估列的选择性，也就是区分度。选择性高的列，比如用户 ID、订单号，更适合创建索引，因为它们能更有效地过滤数据。

------------------------------

723. *MySQL中使用索引一定有效吗？如何排查索引效果？, page url: https://www.mianshi.icu/question/detail?id=1287
723-1287-答案：
简答题，校招和初中级岗位面试中常见。
前置知识：
在 MySQL中使用索引不一定有效，常见的导致索引失效的情况有：查询条件跟索引列类型不匹配时产生隐式转换，或者在 WHERE 子句里对索引列使用函数或运算，甚至用不等于操作符，都会让优化器很难利用现有索引。另外像在字符串匹配里，如果写成 LIKE '%关键字' 这种前面带通配符的方式，也同样无法利用索引的前缀匹配性能。对于联合索引，必须遵循最左前缀原则，否则只要缺了最左边那一列，剩余部分的索引就部分失效。

在判断索引效果的时候，可以通过几种有效的方法来分析和验证。

首先，最直接的方法就是使用EXPLAIN命令查看执行计划。只需要在查询语句前面加上EXPLAIN或者EXPLAIN FORMAT=JSON，就能看到查询的执行流程。需要特别关注几个关键字段：type、key、possible_keys、key_len、rows和Extra等字段。

------------------------------

724. *MySQL中怎么看一条查询语句有没有走索引?, page url: https://www.mianshi.icu/question/detail?id=1288
724-1288-答案：
简答题，校招和初中级岗位面试中常见。
前置知识：
在MySQL中判断一条查询语句是否走索引，可以通过几种有效的方法来分析和验证。

首先，最直接的方法就是使用EXPLAIN命令查看执行计划。只需要在查询语句前面加上EXPLAIN或者EXPLAIN FORMAT=JSON，就能看到查询的执行流程。需要特别关注几个关键字段：type、key、possible_keys、key_len、rows和Extra等字段。

其次是开启慢查询日志，可以捕获那些没有使用索引的耗时查询。或者使用Performance Schema，它可以对最近执行的语句、扫描行数和索引使用情况进行监控和分析。

------------------------------

725. *MySQL中覆盖索引是什么？, page url: https://www.mianshi.icu/question/detail?id=1289
725-1289-答案：
简答题，校招和初级岗位面试中常见。
什么是覆盖索引？（mianshi.icu）的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
（基本定义）如果一个索引的列包含某个查询所需的全部的列，那么这个索引就是覆盖索引。
同时，我们说覆盖索引效果好的一个前提条件是：索引能装进去内存里面。如果索引本身也要从磁盘里面加载，或者触发了内存 swap，那么覆盖索引有一定的效果，但是效果就没有预期中的好了。

------------------------------

726. *MySQL中索引下推指的是什么？, page url: https://www.mianshi.icu/question/detail?id=1290
726-1290-答案：
简答题，校招和初级岗位面试中常见。
什么是索引下推？（mianshi.icu）的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
（基本定义）索引下推是指尽量使用索引列的值来筛选数据。

（举个例子）例如说有一个索引（A，B，C），并且有一个查询条件 WHERE A = 'abc' AND B LIKE ' %xxxx%'。那么在没有索引下推之前，只能用 A = 'abc' 来筛选，而后会查询出命中的行的数据，再用这里面的 B 的列的值来判定 B LIKE 是否成立。
也不仅仅是 LIKE 查询。

------------------------------

727. *MySQL中的回表指的是什么？, page url: https://www.mianshi.icu/question/detail?id=1291
727-1291-答案：
简答题，校招和初中级岗位面试中常见。
MySQL 中的回表，是指在使用非聚簇索引（例如二级索引）进行查询时，由于索引中不包含所有需要的列数据，必须根据索引中记录的主键值，再回到聚簇索引（数据表）中查找完整行数据的过程。这种操作虽然能够保证查询结果的完整性，但会引入额外的 I/O 开销，可能会对查询性能产生影响。

产生原因：


执行流程：
在 MySQL 中，所谓的回表，简单来说就是当使用非聚簇索引（比如二级索引）查询时，如果索引里没有包含所有需要的列数据，那就得根据索引中记录的主键值，再回到聚簇索引（也就是数据表）里去找到完整的那一行数据。这个过程虽然能保证查到的数据是完整的，但会增加额外的 I/O 操作，可能会影响查询性能。

------------------------------

728. *MySQL中索引的最左前缀匹配原则指的是什么？, page url: https://www.mianshi.icu/question/detail?id=1292
728-1292-答案：
简答题，校招和初级岗位面试中常见。
什么是最左匹配原则？(mianshi.icu）的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
所谓的最左匹配原则，用一句话来说，就是一个索引的全部列，从左往右，等值继续，其它中断。
之所以是最左匹配原则，其实可以通过一个简单的模型就解释清楚，即MySQL数据库在使用联合索引的时候，看上去就像是一个多重循环。例如说（A，B，C）三个列上的索引，就相当于一个最外层是 A，中间是 B，最里面是 C 的三重循环。

那么等值继续的意思就是，如果当层循环的值确定了，你就可以进一步利用下一层循环。而如果当层循环是一个范围查询，那么你就无法利用内存循环了。

------------------------------

729. *MySQL中的索引数量是否越多越好？, page url: https://www.mianshi.icu/question/detail?id=1293
729-1293-答案：
简答题，校招和初中级岗位面试中常见。

这是一个典型的陷阱式问法。因为在常规思维里面，我们都是认为索引是好的。那么有些面试官就不按套路出牌，会反问你索引的缺陷。因此索引是不是越多越好，本质上就是考察索引的开销和缺陷。
索引是不是越多越好？（mianshi.icu）的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
很显然，索引并不是越多越好的。主要考虑两个点：

------------------------------

730. *MySQL的锁机制有哪些？行锁和表锁的区别是什么？, page url: https://www.mianshi.icu/question/detail?id=1294
730-1294-答案：
简答题，校招和初级岗位面试中常见。
你了解 MYSQL 的锁吗？（mianshi.icu）的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
MYSQL 的锁有很多种说法：
在 MYSQL 中，什么时候加锁，加什么锁都不是我们能够控制的。比如说我们经常使用的 SELECT FOR UPDATE 的写法，这算是强制要求数据库加锁，但是从这个语句上我们其实是看不出来 MYSQL 究竟加什么锁。
不过我从个人角度来说，我是非常不建议在数据库中直接使用 SELECT FOR UPDATE 之类的这种悲观锁，而是更加倾向于使用乐观锁。我之前在优化数据库查询的性能，就通过使用乐观锁来优化了很多 SELECT FOR UPDATE 的这种写法，一方面提高了性能，一方面也规避了死锁等问题。

------------------------------

731. *MySQL中有哪些锁类型？, page url: https://www.mianshi.icu/question/detail?id=1295
731-1295-答案：
简答题，校招和初级岗位面试中常见。
你了解 MYSQL 的锁吗？（mianshi.icu）的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
MYSQL 的锁有很多种说法：
在 MYSQL 中，什么时候加锁，加什么锁都不是我们能够控制的。比如说我们经常使用的 SELECT FOR UPDATE 的写法，这算是强制要求数据库加锁，但是从这个语句上我们其实是看不出来 MYSQL 究竟加什么锁。
不过我从个人角度来说，我是非常不建议在数据库中直接使用 SELECT FOR UPDATE 之类的这种悲观锁，而是更加倾向于使用乐观锁。我之前在优化数据库查询的性能，就通过使用乐观锁来优化了很多 SELECT FOR UPDATE 的这种写法，一方面提高了性能，一方面也规避了死锁等问题。

------------------------------

732. *MySQL的乐观锁和悲观锁的区别是什么？, page url: https://www.mianshi.icu/question/detail?id=1296
732-1296-答案：
简答题，校招和初中级岗位面试中常见。
乐观锁和悲观锁的区别是什么？（mianshi.icu）的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
MySQL的乐观锁和悲观锁的区别主要体现在以下几个方面：

首先是对冲突的假设不同。乐观锁假设冲突很少发生，它认为大部分情况下数据不会被其他事务修改。而悲观锁则相反，它假设冲突总是发生，认为数据在任何时候都可能被其他事务修改。

其次是加锁的时机和方式不同。乐观锁实际上并不加锁，它是在数据提交更新时，通过版本号（Version）或时间戳（Timestamp）等机制来检查数据是否发生了冲突。如果检测到冲突，操作就会失败，通常需要用户或程序进行重试。悲观锁则是在事务开始之前就对数据加锁，确保在整个事务过程中，其他事务无法修改该数据，直到当前事务提交或回滚后释放锁。

------------------------------

733. *MySQL中如何解决死锁？, page url: https://www.mianshi.icu/question/detail?id=1297
733-1297-答案：
简答题，校招和初中级岗位面试中常见。
什么是死锁？怎么解决？（mianshi.icu）的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
死锁是指多个事务或者进程之间相互等待对方持有的资源，而导致无法继续执行，最终造成所有参与方都陷入无限等待的一种僵局状态。在MySQL数据库并发控制中，死锁通常发生在不同事务对同一数据资源进行加锁操作，但是顺序不一致的情况下。

可以通过以下几个方面来降低死锁发生的概率，并且减少影响范围：

首先是事前预防，包括固定加锁顺序、减少锁持有时间、合理设置锁粒度和事务隔离级别等来破坏死锁必备的四个条件（互斥条件、持有并等待、不可剥夺和循环等待）。

------------------------------

734. *MySQL 的事务隔离级别有哪些？分别解决了哪些问题？, page url: https://www.mianshi.icu/question/detail?id=1298
734-1298-答案：
基础高频题。基本上在讨论数据库或者数据库事务的时候，大概率会问这个问题。

你在回答的时候，一方面要回答出来隔离级别的基本定义，一方面也要回答出来不同隔离级别可能引发的问题。而后你有几个选择：一个选择是将话题引导到 MySQL 实现隔离级别的底层机制上；一个选择是将话题延伸到快照读上；最后一个选择是进一步讨论分布式事务中的隔离级别问题。
什么是隔离级别？（mianshi.icu）的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
MySQL数据库的隔离级别分成四种：

首先是读未提交（Read Uncommitted），这是最低的隔离级别，允许一个事务读取另一个事务尚未提交的数据。这种情况下可能会出现脏读，即读取到其他事务可能最终会回滚的数据。

其次是读已提交（Read Committed），在这个隔离级别下，一个事务只能读取已经被其他事务提交的数据。它避免了脏读，但可能会出现不可重复读，即一个事务在其运行期间多次读取相同记录时，可能会发现记录已被其他已提交的事务修改。
此外，在MySQL数据库中还有一种快照读的概念。它是指同一个事务内部，读到的数据永远是一样的，也就是在可重复读的基础上进一步解决了幻读问题。MySQL在可重复读隔离级别下，利用 MVCC 中的 Read View 机制，实现了快照读的效果。
在分布式事务里面，倒是很少讨论隔离级别这个问题。现在的热门的 TCC 之类的分布式事务，几乎完全没有考虑到任何隔离级别的问题。

------------------------------

735. *MySQL中默认的事务隔离级别是什么？与其他事务隔离级别的区别是什么？, page url: https://www.mianshi.icu/question/detail?id=1299
735-1299-答案：
简答题，校招和初中级岗位面试中常见。
什么是隔离级别？（mianshi.icu）的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
MySQL中默认的事务隔离级别是可重复读（Repeatable Read）。它确保在一个事务中，相同的查询会返回相同的结果集，即使其他事务对数据进行了修改。

与其他事务隔离级别的区别是：

首先，读未提交（Read Uncommitted）隔离级别允许一个事务读取另一个事务尚未提交的数据。这种情况下可能会出现脏读，即读取到其他事务可能最终会回滚的数据。与其相比，可重复读（Repeatable Read）避免了脏读，但性能相差较多。
此外在数据库中还有一种快照读的概念。它是指同一个事务内部，读到的数据永远是一样的，也就是在可重复读的基础上进一步解决了幻读问题。在 MySQL 中的可重复读隔离级别下，利用 MVCC 中的 Read View 机制，实现了快照读的效果。
在分布式事务里面，倒是很少讨论隔离级别这个问题。现在的热门的 TCC 之类的分布式事务，几乎完全没有考虑到任何隔离级别的问题。

------------------------------

736. *乐观锁和悲观锁的区别是什么？, page url: https://www.mianshi.icu/question/detail?id=1301
736-1301-答案：
简单题，校招和初中级岗位面试中常见。
前置知识：

乐观锁和悲观锁的区别主要体现在以下几个方面：
乐观锁和悲观锁的区别主要体现在以下几个方面：

首先是对冲突的假设不同。乐观锁假设冲突很少发生，它认为大部分情况下数据不会被其他事务修改。而悲观锁则相反，它假设冲突总是发生，认为数据在任何时候都可能被其他事务修改。

其次是加锁的时机和方式不同。乐观锁实际上并不加锁，它是在数据提交更新时，通过版本号（Version）或时间戳（Timestamp）等机制来检查数据是否发生了冲突。如果检测到冲突，操作就会失败，通常需要用户或程序进行重试。悲观锁则是在事务开始之前就对数据加锁，确保在整个事务过程中，其他事务无法修改该数据，直到当前事务提交或回滚后释放锁。

------------------------------

737. *MySQL支持哪些存储引擎？它们之间有什么区别？, page url: https://www.mianshi.icu/question/detail?id=1302
737-1302-答案：
简答题，校招和初级岗位面试中常见。
MySQL 支持多种存储引擎，不同引擎针对不同的业务需求和场景做了优化，下面归纳几个常见的存储引擎及其主要区别：
MySQL支持以下存储引擎：

首先是 InnoDB。它是 MySQL 的默认引擎，它不仅支持 ACID 事务，还能提供行级锁和外键约束，使用聚簇索引存储数据，并支持 MVCC 实现非锁定读。它的优点是能够支持事务操作、锁的粒度比较细，数据一致性也非常高，同时还具有良好的崩溃恢复机制。不过，正因为支持事务，数据存储的开销会比那些不支持事务的引擎略大，在某些写操作场景下可能性能不会那么理想。但总体来说，对于电商、金融这些对写入高并发、事务一致性要求高的核心业务场景，InnoDB 是非常理想的选择。

其次是 MyISAM，这个引擎虽然不支持事务和外键约束，但它通过表级锁来实现并发控制，而且数据和索引是分开存储的，支持全文索引，这对于文本检索来讲是个优势。它的优点在于读多写少的场景下查询性能很优，也能节省存储空间，但因为不支持事务，数据一致性较低，而且在高并发写操作时由于表级锁可能会有性能瓶颈，再加上出故障后需要手动修复，所以它更多的是用在日志、报表或者数据仓库等主要以读为主的应用中。

------------------------------

738. *MySQL的InnoDB存储引擎和MyISAM存储引擎有什么区别, page url: https://www.mianshi.icu/question/detail?id=1303
738-1303-答案：
简答题，校招和初级岗位面试中常见。
MySQL支持哪些存储引擎？它们之间有什么区别？（mianshi.icu）的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
InnoDB 是 MySQL 的默认引擎，它不仅支持 ACID 事务，还能提供行级锁和外键约束，使用聚簇索引存储数据，并支持 MVCC 实现非锁定读。它的优点是能够支持事务操作、锁的粒度比较细，数据一致性也非常高，同时还具有良好的崩溃恢复机制。不过，正因为支持事务，数据存储的开销会比那些不支持事务的引擎略大，在某些写操作场景下可能性能不会那么理想。但总体来说，对于电商、金融这些对事务一致性要求高的核心业务场景，InnoDB 是非常理想的选择。

------------------------------

739. *MySQL的InnoDB存储引擎为什么使用B+树实现索引而不用其他B族树？, page url: https://www.mianshi.icu/question/detail?id=1304
739-1304-答案：
简答题，校招和初级岗位面试中常见。这是一个高频面试题目。回答这个问题的误区是无脑背 B+ 树的特点，而没有针对索引这种场景进行分析。并且，如果要想刷出亮点，就不能将话题局限在 B+ 树上，而是要进一步考虑其它的数据结构，并且在面试高端岗位的时候能给出非常具体的例子。
为什么主流索引结构都是使用 B+ 树？（mianshi.icu）的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
（基本回答）因为 B+ 树适合作为索引，尤其是关系型数据库的索引。核心原因有：
B+ 树的内部节点不存放数据这一个特性其实非常重要，因为我们在平时分析数据库查询性能的时候，都会默认索引放在内存里面。而如果索引无法放进去内存里面，也就是即便查询索引本身也会触发磁盘 IO，性能极差。
B+ 树的这些特性还有一些比较有意思的传言。比如说业界有很多流传的说法，说表最好不要超过 500 万行，1000 万行，或者 2000 万行，就是为了控制 B+ 树的高度。我个人认为这些说法都过于僵化了，什么时候分库，什么时候分表又不是只看数据量。

------------------------------

740. *MySQL中的B+树索引是如何工作的？, page url: https://www.mianshi.icu/question/detail?id=1305
740-1305-答案：
简答题，校招和初级岗位面试中常见。
索引的底层原理是什么？如何运作的？（mianshi.icu）的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
B+ 树是多叉树，它由根节点、很多层内部节点和叶子节点组成。它会限制每个节点的键值数量在一个范围内，保证所有叶子节点的深度都一样，这样树就比较平衡。内部节点并不存储数据，只存储了索引键的数据，因此更加紧凑。同样的大小的磁盘或者内存，能够装下更多的内部节点。叶子节点通常存储着指向实际数据行的指针，或者记录的物理地址，而且所有的叶子节点都通过双向链表连在一起，非常适合范围查询，性也非常稳定。

查找的时候，从根节点开始，根据要查找的键值选择相应的子节点。然后在内部节点里依次比较键值，沿着指针往下走，最终到达叶子节点。在叶子节点里找到目标数据行或者指针，然后就可以访问实际的数据了。如果是范围查询的话，找到范围的起始点后，沿着叶子节点之间的双向链表向后（或向前）遍历，直到遇到范围查询的结束键值或链表末尾。在遍历过程中，访问每个叶子节点中符合范围条件的数据行即可。

------------------------------

741. *MySQL的SELECT语句中各子句的执行顺序？, page url: https://www.mianshi.icu/question/detail?id=1306
741-1306-答案：
简答题，校招和初级岗位面试中常见。
MySQL 中 SELECT 语句的子句执行顺序与书写顺序并不完全一致。理解其逻辑执行顺序对于编写高效 SQL、优化查询性能至关重要。
在 MySQL 中，虽然编写 SELECT 语句的顺序是从 SELECT 到 FROM，再经过 WHERE、GROUP BY、HAVING、ORDER BY 最后到 LIMIT，但它的逻辑执行顺序其实并不完全一致。

首先，MySQL 会先从 FROM 以及 JOIN 开始，确定数据的来源，这时会加载表数据，处理子查询，并且把多表连接的数据先生成一个中间结果集。

接下来，在 JOIN 中的 ON 子句就会起作用，用来筛选满足连接条件的行，不论是 INNER JOIN 还是 LEFT JOIN，都根据具体情况决定保留哪些记录。

------------------------------

742. *详细描述一条 SQL 语句在 MySQL 中的执行过程, page url: https://www.mianshi.icu/question/detail?id=1307
742-1307-答案：
简答题，校招和初级岗位面试中常见。
一条 SQL 语句在 MySQL 中的执行过程，可以概括为以下几个主要阶段：连接管理、解析与预处理、查询优化、查询执行、存储引擎交互，以及最终的结果返回。
一条 SQL 语句在 MySQL 中从开始到结束主要分为以下几个阶段：

首先，当客户端，比如应用程序或者命令行工具，通过 TCP/IP 或 Unix Socket 连接到 MySQL 服务器时，服务器会为每个连接分配一个线程，并先对用户进行身份验证和权限检查，确定用户是否有足够的权限去执行某个查询。

接下来是解析与预处理阶段。这里 MySQL 会先检查查询缓存（不过在 MySQL 8.0 之后这个功能已经移除），然后把 SQL 语句进行语法解析，分解成各种关键字、表名、列名、条件等等，并且检查这些语法是否正确。紧接着就是语义解析，会验证语句中提到的表和列是否存在，以及用户是否有权访问它们。预处理阶段则展开视图、处理别名，甚至替换预编译语句中的参数。

------------------------------

743. *MySQL 中 count(*)、count(1) 和 count(列名) 有什么区别？, page url: https://www.mianshi.icu/question/detail?id=1308
743-1308-答案：
简答题，校招和初级岗位面试中常见。

在 MySQL 中，COUNT(*)、COUNT(1) 和 COUNT(列名) 都用于统计行数，具体区别如下：


实践建议：
在 MySQL 中，COUNT(*)、COUNT(1) 和 COUNT(列名) 都用来统计行数，但它们之间还是有一些区别的。

首先是COUNT(*)，COUNT(*) 会统计表中的所有行，包括那些所有列的值都是 NULL 的行。这里的 * 并不会扩展成所有的列，而是直接对行进行计数。在 InnoDB 引擎下，COUNT(*) 通常会遍历最小的可用二级索引。如果不存在二级索引，它就会遍历聚集索引（也就是主键索引）。它不会去读取实际的数据行，只是统计索引记录的数量。而在 MyISAM 引擎下，如果没有任何 WHERE 条件，COUNT(*) 会直接读取表元数据里存储的总行数，这个速度非常快。如果有 WHERE 条件，COUNT(*) 的速度取决于是否能使用索引，如果 WHERE 条件无法使用索引，MySQL 必须执行全表扫描，速度通常较慢；如果 WHERE 条件可以使用索引，并且是覆盖索引，那么 COUNT(*) 可以只扫描索引，速度会快很多。

再来是 COUNT(1)，COUNT(1) 和 COUNT(*) 其实是完全一样的，也是统计所有行，包括 NULL 值的行。这里的 1 只是一个常量值，表示对每一行计数时都加 1。MySQL 优化器通常会把 COUNT(1) 优化成 COUNT(*)。所以，COUNT(1) 和 COUNT(*) 的执行效率几乎没有差别。

------------------------------

744. *MySQL 中的数据排序是怎么实现的？, page url: https://www.mianshi.icu/question/detail?id=1309
744-1309-答案：
简答题，校招和初级岗位面试中常见。
MySQL 中的数据排序（通过 ORDER BY 子句实现）主要依赖两种方式：索引排序 和 文件排序（Filesort）。MySQL 查询优化器会根据查询条件、索引情况、数据量以及配置参数，自动选择最优的排序方式。

示例：

优化策略：
MySQL 中的数据排序通过 ORDER BY 子句实现。使用 ORDER BY 子句时，MySQL 主要依赖两种排序方式：一种是通过索引排序，另一种是文件排序，也叫 Filesort。查询优化器会根据查询条件、索引情况、数据量以及一些配置参数自动选择最合适的排序方式。

索引排序，这种方式利用了 B+ 树索引本身的有序性，可以直接按照索引的顺序读取数据，而不需要额外进行排序操作。前提条件是查询的列需要被索引完全覆盖，也就是说查询只需要访问索引而不必回表，同时 ORDER BY 中的字段要和索引的字段顺序一致，并且排序方向也得和索引定义保持一致。在涉及多表关联的情况下，排序字段也必须属于驱动表，并满足这些条件。使用索引排序的好处在于能够提高查询性能，减少 CPU 消耗和 I/O 操作，使得响应速度更快，因为可以快速返回部分结果，比如在分页查询时。不过，索引排序也受限于索引本身，不是所有情况都能利用，而且如果查询的列没有被索引完全覆盖，还可能需要回表，这会增加 I/O 负担。此外，维护索引的有序性对于写操作如 INSERT、UPDATE 和 DELETE 也会有一些额外开销。

------------------------------

745. *什么是 MySQL 的主从同步机制？它是如何实现的？, page url: https://www.mianshi.icu/question/detail?id=1310
745-1310-答案：
简答题，校招和初级岗位面试中常见。
前置知识：

MySQL 的主从同步（Replication）是一种数据复制技术，通过将主数据库（Master）的数据变更实时或近实时地同步到一个或多个从数据库（Slave），实现数据冗余、读写分离和高可用性。下面从原理、实现方式、配置流程和应用场景四个方面进行详细阐述：
MySQL 的主从同步是一种非常重要的数据复制技术。它把主数据库上的数据变更实时或者近实时地同步到一个或多个从数据库上，从而实现数据冗余、读写分离和高可用性。

这套机制主要依赖于 MySQL 的二进制日志，也就是 binlog。当主库执行写操作的时候，比如进行 INSERT、UPDATE 或 DELETE 操作，所有的数据变化都会被记录到这个 binlog 日志中。这个日志实际上是一种逻辑日志，记录了所有对数据做出的修改。

------------------------------

746. *MySQL 的 Change Buffer 是什么？它有什么作用？, page url: https://www.mianshi.icu/question/detail?id=1311
746-1311-答案：
简答题，校招和初级岗位面试中常见。
通常情况下，对非唯一二级索引的修改，若索引页不在内存中，传统方式需先从磁盘读取，修改后再写回，产生大量随机 I/O。Change Buffer是 InnoDB 缓冲池（Buffer Pool）的一部分，用于缓存对非唯一二级索引的修改操作（INSERT、UPDATE、DELETE）。当索引页不在缓冲池时，InnoDB 将修改操作暂存在 Change Buffer，而非立即读盘修改。其核心作用是：


工作原理：


优缺点：
在 MySQL 的 InnoDB 存储引擎中，当对非唯一二级索引进行修改时，如果目标索引页不在内存中，传统的处理方式需要先从磁盘读取索引页到内存，修改后再写回磁盘，这会产生大量的随机 I/O 操作。而 Change Buffer 的引入，正是为了解决这个问题。它是 InnoDB 缓冲池（Buffer Pool）的一部分，专门用来缓存对非唯一二级索引的修改操作，比如 INSERT、UPDATE 和 DELETE。如果目标索引页不在缓冲池中，InnoDB 会把这些修改操作暂时存储在 Change Buffer 中，而不是立即去读磁盘修改。

------------------------------

747. *MySQL中左连接是什么？与右连接有什么区别？, page url: https://www.mianshi.icu/question/detail?id=1312
747-1312-答案：
简答题，校招和初级岗位面试中常见。
在 MySQL 中，左连接（LEFT JOIN） 和 右连接（RIGHT JOIN） 是两种常见的表连接方式，用于从多个表中获取数据。它们的主要区别在于结果集中保留的记录范围不同。


左连接会返回左表中的所有记录，即使右表中没有匹配的记录。如果右表中没有匹配的记录，结果集中对应的右表字段会用 NULL 填充。

语法如下：
在 MySQL 中，左连接（LEFT JOIN） 和 右连接（RIGHT JOIN） 是两种常见的表连接方式，用于从多个表中获取数据。

其中，左连接（LEFT JOIN），返回左表的所有记录，即使右表中没有匹配项。如果右表中没有匹配项，右表字段用 NULL 填充。

------------------------------

748. *请详细描述在MySQL B+树索引中查询数据的全过程, page url: https://www.mianshi.icu/question/detail?id=1313
748-1313-答案：
简答题，校招和初级岗位面试中常见。
MySQL中的B+树索引是如何工作的？（mianshi.icu）的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
B+ 树是多叉树，它由根节点、很多层内部节点和叶子节点组成。它会限制每个节点的键值数量在一个范围内，保证所有叶子节点的深度都一样，这样树就比较平衡。内部节点并不存储数据，只存储了索引键的数据，因此更加紧凑。同样的大小的磁盘或者内存，能够装下更多的内部节点。叶子节点通常存储着指向实际数据行的指针，或者记录的物理地址，而且所有的叶子节点都通过双向链表连在一起，非常适合范围查询，性也非常稳定。

------------------------------

749. *联合索引 (a, b, c)，where b = xxx and c = xxx and a = xxx 会走索引吗？, page url: https://www.mianshi.icu/question/detail?id=1314
749-1314-答案：
简答题，校招和初级岗位面试中常见。
前置知识：
查询 WHERE b = xxx AND c = xxx AND a = xxx 会走索引。

------------------------------

750. *为什么 MySQL选择使用B+树作为索引结构？, page url: https://www.mianshi.icu/question/detail?id=1315
750-1315-答案：
简答题，校招和初级岗位面试中常见。这是一个高频面试题目。回答这个问题的误区是无脑背 B+ 树的特点，而没有针对索引这种场景进行分析。并且，如果要想刷出亮点，就不能将话题局限在 B+ 树上，而是要进一步考虑其它的数据结构，并且在面试高端岗位的时候能给出非常具体的例子。
为什么主流索引结构都是使用 B+ 树？（mianshi.icu）的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
（基本回答）因为 B+ 树适合作为索引，尤其是关系型数据库的索引。核心原因有：
B+ 树的内部节点不存放数据这一个特性其实非常重要，因为我们在平时分析数据库查询性能的时候，都会默认索引放在内存里面。而如果索引无法放进去内存里面，也就是即便查询索引本身也会触发磁盘 IO，性能极差。
B+ 树的这些特性还有一些比较有意思的传言。比如说业界有很多流传的说法，说表最好不要超过 500 万行，1000 万行，或者 2000 万行，就是为了控制 B+ 树的高度。我个人认为这些说法都过于僵化了，什么时候分库，什么时候分表又不是只看数据量。

------------------------------

751. *MySQL中B+树索引、B树索引和哈希索引的区别？, page url: https://www.mianshi.icu/question/detail?id=1316
751-1316-答案：
简答题，校招和初级岗位面试中常见。
前置知识：
B+ 树索引、B 树索引和哈希索引主要有以下区别：

首先是数据结构。B+ 树是一种多路平衡查找树，它的特点是所有数据都只存在叶子节点，内部节点只存索引键，不存数据。而且叶子节点之间还用双向链表连起来，像一个有序的链表。B 树呢，也是多路平衡查找树，但它跟 B+ 树不一样的地方在于，B 树的内部节点和叶子节点都能存数据。也就是说，数据可能在树的任何一层。哈希索引就完全不同了，它是基于哈希表实现的，通过哈希函数计算键的哈希值，直接把数据存在哈希表里对应哈希值的位置。

其次是数据的存储方式。B+ 树只在叶子节点存数据，内部节点只存键。B 树是内部节点和叶子节点都存数据。哈希索引呢，数据直接存在哈希表里，根据键的哈希值来确定存哪儿。

------------------------------

752. *MySQL索引为什么用B+树？, page url: https://www.mianshi.icu/question/detail?id=1317
752-1317-答案：
简答题，校招和初级岗位面试中常见。这是一个高频面试题目。回答这个问题的误区是无脑背 B+ 树的特点，而没有针对索引这种场景进行分析。并且，如果要想刷出亮点，就不能将话题局限在 B+ 树上，而是要进一步考虑其它的数据结构，并且在面试高端岗位的时候能给出非常具体的例子。
为什么主流索引结构都是使用 B+ 树？（mianshi.icu）的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
（基本回答）因为 B+ 树适合作为索引，尤其是关系型数据库的索引。核心原因有：
B+ 树的内部节点不存放数据这一个特性其实非常重要，因为我们在平时分析数据库查询性能的时候，都会默认索引放在内存里面。而如果索引无法放进去内存里面，也就是即便查询索引本身也会触发磁盘 IO，性能极差。
B+ 树的这些特性还有一些比较有意思的传言。比如说业界有很多流传的说法，说表最好不要超过 500 万行，1000 万行，或者 2000 万行，就是为了控制 B+ 树的高度。我个人认为这些说法都过于僵化了，什么时候分库，什么时候分表又不是只看数据量。

------------------------------

753. *数据库中，存一样的数据B树索引和B+树索引哪个索引树的高度高？, page url: https://www.mianshi.icu/question/detail?id=1318
753-1318-答案：
简答题，校招和初级岗位面试中常见。
前置知识：
在数据库中，存储相同的数据量时，B 树索引的高度通常会高于 B+ 树索引，即B+树更“矮胖” 。这是由 B 树和 B+ 树的结构差异决定的。

------------------------------

754. *除了树什么样的结构可以做索引？哈希表可以么？, page url: https://www.mianshi.icu/question/detail?id=1319
754-1319-答案：
简答题，校招和初级岗位面试中常见。
前置知识：
除了树还有以下几种数据结构可以作为索引，比如哈希或散列或映射（哈希索引、全文索引）、位图（位图索引）等。

哈希表是可以用作索引的。哈希索引的基本机制是，把键值输入到一个哈希函数里，计算出一个哈希值，然后根据这个哈希值找到哈希表里对应的位置。如果不同的键值算出了同一个哈希值，也就是发生了冲突，可以用链表法或者开放寻址法来解决。

查找的时候，先计算查询键值的哈希值，然后根据哈希值直接找到哈希表里的槽位。如果存在冲突，就按照冲突解决的策略继续查找，直到找到目标行。

------------------------------

755. *在MySQL中，bin log是什么？有什么作用？, page url: https://www.mianshi.icu/question/detail?id=1320
755-1320-答案：
简答题，校招和初级岗位面试中常见。
MySQL 中的二进制日志（binlog）是一种以二进制格式记录数据库所有数据变更操作的日志文件，是 MySQL 服务器层面的日志并且不依赖于具体的存储引擎。它不仅记录了诸如 INSERT、UPDATE、DELETE 这类对数据进行修改的 DML 操作，还会记录 CREATE、ALTER、DROP 等 DDL 操作，但不会包括只读查询（如 SELECT）。binlog 文件通常保存在 MySQL 数据目录中，并且按照一定规则进行切换，比如当文件达到设定大小后自动滚动到新的日志文件。

核心作用:


工作机制与配置：
MySQL 的二进制日志，即 binlog，是一种以二进制格式记录所有数据变更操作的日志文件。它处于 MySQL 服务器层面，不依赖于具体的存储引擎。binlog 不仅记录像 INSERT、UPDATE、DELETE 这样的数据修改操作，还会记录 CREATE、ALTER、DROP 等结构变更操作，但像 SELECT 这类只读查询则不记录。一般来说，binlog 文件会保存在 MySQL 数据目录下，并且系统会按照设定好的规则对日志文件进行切换，比如当文件达到一定大小后，会自动滚动到新的日志文件。

binlog 的核心作用主要体现在几个方面。

首先，在主从复制中，binlog 就是最关键的组件，主库通过 binlog 记录所有数据变更，从库的 I/O 线程会实时抓取这些日志，再由 SQL 线程进行重放，从而实现数据同步，保证主从数据的一致性。

------------------------------

756. *什么是 MVCC？它是如何实现并发控制的？, page url: https://www.mianshi.icu/question/detail?id=1321
756-1321-答案：
略难的题，高频面试题。难点主要在于 MVCC 本身内容很多，很难全部记住，而且从 MVCC 衍生出来的内容也太多，问法多样，初学者很容易栽在这里。

这个问题一般是深入面 MVCC 的开场白，所以你在回答的时候只需要把关键点点出来，留下引导就可以了，具体细节可以等面试官追问的时候再说。
你了解 MySQL 的 MVCC 吗？(mianshi.icu）的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
MVCC，全称是多版本并发控制，是 MySQL InnoDB 中用来实现事务，以及事务隔离级别的核心机制。

它的关键点有两个：存储不同版本的数据以及如何控制事务读取哪个版本的事务。并且以下方式实现并发控制：

就存储来说，MVCC 使用了版本链。每一条数据都有两个额外的列，一个是事务 ID，也可以看做是版本号；一个是回滚指针，MVCC 利用回滚指针将数据不同的版本串联在一起，并且将这个版本链存储到了 undo log 日志中，用于事务回滚和 MVCC 的版本控制。

------------------------------

757. *MySQL中的MVCC指的是什么？, page url: https://www.mianshi.icu/question/detail?id=1322
757-1322-答案：
略难的题，高频面试题。难点主要在于 MVCC 本身内容很多，很难全部记住，而且从 MVCC 衍生出来的内容也太多，问法多样，初学者很容易栽在这里。

这个问题一般是深入面 MVCC 的开场白，所以你在回答的时候只需要把关键点点出来，留下引导就可以了，具体细节可以等面试官追问的时候再说。
你了解 MySQL 的 MVCC 吗？(mianshi.icu）的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
MVCC，全称是多版本并发控制，是 MySQL InnoDB 中用来实现事务，以及事务隔离级别的核心机制。

它的关键点有两个：存储不同版本的数据以及如何控制事务读取哪个版本的事务。

就存储来说，MVCC 使用了版本链。每一条数据都有两个额外的列，一个是事务 ID，也可以看做是版本号；一个是回滚指针，MVCC 利用回滚指针将数据不同的版本串联在一起，并且将这个版本链存储到了 undo log 日志中。

------------------------------

758. *MVCC机制怎么实现的？实现了什么事务隔离级别？, page url: https://www.mianshi.icu/question/detail?id=1323
758-1323-答案：
略难的题，高频面试题。难点主要在于 MVCC 本身内容很多，很难全部记住，而且从 MVCC 衍生出来的内容也太多，问法多样，初学者很容易栽在这里。

这个问题一般是深入面 MVCC 的开场白，所以你在回答的时候只需要把关键点点出来，留下引导就可以了，具体细节可以等面试官追问的时候再说。
你了解 MySQL 的 MVCC 吗？(mianshi.icu）的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
MVCC，全称是多版本并发控制，是 MySQL InnoDB 中用来实现事务，以及事务隔离级别的核心机制。读已提交
和可重复读，这两个事务隔离级别就是通过MVCC来实现的。

它的关键点有两个：存储不同版本的数据以及如何控制事务读取哪个版本的事务。

------------------------------

759. *MySQL是如何实现事务的？, page url: https://www.mianshi.icu/question/detail?id=1324
759-1324-答案：
简答题，校招和初级岗位面试中常见。
MySQL 的事务实现主要依赖于 InnoDB 存储引擎，它通过一系列机制来满足 ACID（原子性、一致性、隔离性、持久性）要求，确保数据操作既高效又安全。
MySQL 的事务主要是由 InnoDB 存储引擎实现的，它通过一系列机制来满足 ACID 要求，保证数据操作既高效又安全，也就是常说的原子性、一致性、隔离性和持久性。简单来说，事务就是一组操作，要么都执行成功，要么全部回滚，当中间有任何问题时，任何变化都不会对外可见。

MySQL 主要依靠几种关键技术的协同工作来实现事务。

首先是 Undo Log，也就是回滚日志。在事务执行修改操作之前，MySQL 会先记录数据修改前的旧值，这样如果需要回滚，就可以根据 Undo Log 快速还原到原始数据。另外，Undo Log 还为 MVCC 提供了支持，让事务能够读取到历史版本的数据快照，从而实现一致性读取。

------------------------------

760. *MySQL中长事务指的是什么？可能会导致哪些问题？, page url: https://www.mianshi.icu/question/detail?id=1325
760-1325-答案：
简答题，校招和初级岗位面试中常见。
在 MySQL 中，长事务通常指的是那些执行时间过长、未及时提交或回滚的事务。这类事务并没有一个严格的时间界限，通常在实际使用场景中，如果事务持续时间明显超过常规事务（比如说几分钟以上甚至更长），或者在事务内部执行了大量的数据操作与复杂逻辑，就可以认为它是“长事务”。

长事务的成因：


长事务带来的问题：
在 MySQL 中所谓的长事务，通常是指那些执行时间特别长、没有及时提交或回滚的事务。这种事务没有一个严格的时间标准，但如果一个事务持续时间明显超出正常事务的范围，比如说几分钟甚至更长，或者在事务操作中包含了大量数据变更和复杂的逻辑，那么就可以认为它属于长事务。

长事务可能由多种原因引起，比如由于业务操作太复杂，在一个事务里处理太多的 INSERT、UPDATE 或 DELETE 操作，或是嵌入了复杂的查询和计算逻辑；有时也可能是因为编程时犯了错误，没有在合适的时候显式提交或回滚事务；此外，外部因素也可能造成长事务，比如网络延迟或者跨节点分布式事务中的远程调用时间过长。

长事务导致的问题主要体现以下几个方面。

------------------------------

761. *如何分析一条 SQL 的执行计划？, page url: https://www.mianshi.icu/question/detail?id=1327
761-1327-答案：
简答题，校招和初级岗位面试中常见。
在 MySQL 中，分析 SQL 执行计划的核心工具是 EXPLAIN 命令。通过分析各个字段可深入理解查询的底层执行逻辑，定位性能瓶颈并进行针对性优化。具体方法是在 SQL 语句前加上 EXPLAIN，即可查看执行计划信息。如果想获得更详细的信息，可使用 EXPLAIN FORMAT=JSON。在 MySQL 8.0+ 版本中，还可对 DML 语句（INSERT/UPDATE/DELETE）使用 EXPLAIN 查看计划。

示例如下：

关键字段与含义：
在 MySQL 中只需要在 SQL 语句前加上 EXPLAIN 关键字就能查看执行计划的详细信息。通过它，可以深入理解查询的底层执行逻辑，找出性能瓶颈，然后进行针对性优化。如果需要更详细的信息，我还会用 EXPLAIN FORMAT=JSON 来获取 JSON 格式的输出，这对复杂查询的分析特别有帮助。

在实际工作中，分析执行计划通常会按照以下步骤进行：

首先检查  type 字段，它决定了访问数据的方式。比如看到 const、range 或 ref 这样的值会比较安心，说明用到了主键或索引；但如果出现 ALL 就说明是全表扫描，这时候就要考虑加索引或者索引失效的问题了。

------------------------------

762. *MySQL 中如何进行 SQL 调优？, page url: https://www.mianshi.icu/question/detail?id=1328
762-1328-答案：
简答题，校招和初级岗位面试中常见。
如何优化SQL？（mianshi.icu）的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
SQL 调优的过程，通常可以分为五个主要的阶段：“发现问题”、“分析瓶颈”、“优化实施”、“验证效果”和“持续迭代”。

首先是“发现问题”，可以通过分析慢查询日志、实时监控（ SHOW PROCESSLIST /Prometheus、Grafana ）或者业务层的反馈来发现问题。

接下来是“分析瓶颈”，可以通过分析慢查询日志、查看SQL语句的执行计划（EXPLAIN）和SQL Profiling（SHOW PROFILE 或者 SHOW PROFILES）查看 SQL 语句在各个阶段的详细耗时。如果出现死锁或者大量的锁等待，可以通过 SHOW ENGINE INNODB STATUS 发现问题。

------------------------------

763. *如何优化MySQL查询性能？请列举几种方法。, page url: https://www.mianshi.icu/question/detail?id=1329
763-1329-答案：
简答题，校招和初级岗位面试中常见。
常用的MySQL优化策略有哪些？（mianshi.icu）的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
优化MySQL查询性能有以下几种方式：

首先是调整整体的架构设计。比如可以考虑MySQL读写分离，把MySQL主库专门用于写操作，从库处理读操作，这样可以分担数据库的压力。或者引入缓存层。在应用层集成 Redis、Memcached 等缓存，缓存热点数据，减少数据库的访问频率。对于静态资源或者大流量的访问点，尽量使用 CDN、负载均衡等手段。

其次是调整MySQL表结构。比如字段的设计要精简。选择合适的数据类型，比如用 INT 来代替 VARCHAR 存储数值字段，减少存储空间和索引的开销。尽量避免使用 NULL 列，或者设定默认值，以减少索引和统计信息的复杂度。此外，范式化和反范式化要结合起来。采用三大范式可以减少冗余，保证数据的一致性。如果读多写少，为了提升查询的性能，可以在关键点做少量的冗余，避免过多的 JOIN 操作。

------------------------------

764. *常用的MySQL优化策略有哪些？, page url: https://www.mianshi.icu/question/detail?id=1330
764-1330-答案：
简答题，校招和初中级岗位面试中常见。
你通常从哪些方面来优化数据库？（mianshi.icu）的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
通常从以下几个方面来对MySQL数据库进行优化：

首先是整体的架构设计。比如可以考虑MySQL读写分离，把MySQL主库专门用于写操作，从库处理读操作，这样可以分担数据库的压力。还可以考虑MySQL分库分表。水平拆分，就是按照用户 ID 或者时间等维度，把数据分散到不同的数据库或者表里面，解决大表查询以及超大数据量存储的问题。或者引入缓存层。在应用层集成 Redis、Memcached 等缓存，缓存热点数据，减少数据库的访问频率。对于静态资源或者大流量的访问点，尽量使用 CDN、负载均衡等手段。

其次是MySQL表结构。比如字段的设计要精简。选择合适的数据类型，比如用 INT 来代替 VARCHAR 存储数值字段，减少存储空间和索引的开销。尽量避免使用 NULL 列，或者设定默认值，以减少索引和统计信息的复杂度。范式化和反范式化要结合起来。采用三大范式可以减少冗余，保证数据的一致性。如果读多写少，为了提升查询的性能，可以在关键点做少量的冗余，避免过多的 JOIN 操作。

------------------------------

765. *MySQL优化做过哪些？, page url: https://www.mianshi.icu/question/detail?id=1331
765-1331-答案：
简答题，校招和初中级岗位面试中常见。
常用的MySQL优化策略有哪些？（mianshi.icu）的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
做过以下优化：

首先是MySQL表结构。为字段选择合适的数据类型，比如用 INT 来代替 VARCHAR 存储数值字段，减少存储空间和索引的开销。尽量避免使用 NULL 列，或者设定默认值，以减少索引和统计信息的复杂度。

其次是范式化和反范式化相结合。先采用三大范式减少冗余，保证数据的一致性。后期如果读多写少，为了提升查询的性能，在关键点做少量的冗余（反范式化），并避免过多的 JOIN 操作。

------------------------------

766. *什么是慢查询？如何优化慢查询？, page url: https://www.mianshi.icu/question/detail?id=1332
766-1332-答案：
简答题，校招和初中级岗位面试中常见。
前置知识：

在 MySQL 中，慢查询指的是执行时间超过预设阈值的 SQL 查询。通常，这个阈值由 long_query_time 参数定义（默认可能为 10 秒，但在实际优化中会调低，比如设置为 1 秒），凡执行时间超过该值的查询都会被记录到慢查询日志中。慢查询往往是性能瓶颈的体现，它可能消耗大量的 CPU、内存和 I/O 资源，最终影响整体系统响应和并发能力。

通常采用以下方式诊断和优化慢查询：
慢查询指的是执行时间超过预设阈值的查询，而这个阈值通常是由 long_query_time 参数决定的。默认可能是 10 秒，但在实际优化中，会调低，比如设置成 1 秒，这样一旦查询超过这个时限，就会被记录到慢查询日志里。还可以通过调整参数，记录那些没有利用索引的查询，有助于进一步定位问题。

为了诊断慢查询，通常需要先开启慢查询日志确保慢查询被记录到慢查询日志中。在分析日志时，可以直接查看慢查询日志文件，也可以用 mysqldumpslow 或 Percona Toolkit 里的 pt-query-digest 这样的工具去归纳和统计耗时查询，从而找出问题的根源。接下来结合 EXPLAIN 命令，查看查询的具体执行计划，主要关注type、key、rows、Extra等字段，分析导致慢查询的原因。

优化慢查询，可以从以下几个方面入手：

------------------------------

767. *遇到过慢查询么？如何解决?, page url: https://www.mianshi.icu/question/detail?id=1333
767-1333-答案：
简答题，校招和初中级岗位面试中常见。
什么是慢查询？如何优化慢查询？（mianshi.icu）的另一种问法，题目分析不再赘述，回答时只需要修改一些衔接语即可。
慢查询指的是执行时间超过预设阈值的查询，而这个阈值通常是由 long_query_time 参数决定的。默认可能是 10 秒，但在实际优化中，会调低，比如设置成 1 秒，这样一旦查询超过这个时限，就会被记录到慢查询日志里。还可以通过调整参数，记录那些没有利用索引的查询，有助于进一步定位问题。

为了诊断慢查询，通常需要先开启慢查询日志确保慢查询被记录到慢查询日志中。在分析日志时，可以直接查看慢查询日志文件，也可以用 mysqldumpslow 或 Percona Toolkit 里的 pt-query-digest 这样的工具去归纳和统计耗时查询，从而找出问题的根源。接下来结合 EXPLAIN 命令，查看查询的具体执行计划，主要关注type、key、rows、Extra等字段，分析导致慢查询的原因。

解决慢查询，可以从以下几个方面入手：

------------------------------

768. *慢查询日志是什么？如何排查？, page url: https://www.mianshi.icu/question/detail?id=1334
768-1334-答案：
简答题，校招和初中级岗位面试中常见。
慢查询日志是 MySQL 提供的一项日志功能，用于记录那些执行时间超过指定阈值（通常由参数 long_query_time 设置，如 1 秒或 10 秒）的 SQL 查询语句。它是定位和优化数据库性能瓶颈的关键工具，可以帮助我们发现执行效率低的 SQL 语句。
MySQL 里的慢查询日志就是用来记录那些执行时间超过设定阈值的 SQL 查询。这个日志对于定位和优化数据库性能问题非常关键，通过它能找出那些执行效率比较低的 SQL 语句。

排查慢查询的第一步是找到慢查询，可以直接查看慢查询日志文件，用 cat、tail、less 这些命令都行。也可以使用MySQL 自带的 mysqldumpslow 工具或者Percona Toolkit 里的 pt-query-digest 工具等。

找到具体的慢查询后，用 EXPLAIN 命令来分析它的执行计划。重点关注几个字段：type 表示访问类型，如果是 ALL，表示全表扫描，通常需要优化；key 表示实际使用的索引，如果是 NULL，表示没有使用索引；rows 表示预估的扫描行数，越小越好；Extra 字段里如果出现 Using filesort 或者 Using temporary，可能表示存在性能问题。

------------------------------

